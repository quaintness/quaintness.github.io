# Machine Learning

## Overview

Programming  Environment: **Octave**

1. **Supervised Learning**

   Dataset offers the correct answer.

   - Regression

     To predict a continuous valued output.

   - Classification

     To predict  a discrete valued output.

     - Support Vector Machine

       Allow a computer to deal with an infinite number of features.

2. **Unsupervised Learning**

   Given a dataset and find some structure in the data.

   - Clustering Algorithm

     e.g. Organize computing clusters, Social network analysis, Market segmentation, Astronomical data analysis

   - Cocktail Party Algorithm

     Find structures in the data and separate out them

## Linear Regression

Terms

- Training set

  - $m$ --- # training examples 

  - $x$ --- "input" variable / features

  - $y$ --- "output" variable / features

  - $(x,y)$ --- one training example

  - $(x^{(i)},y^{(i)})$ --- $i_{th}$ training example

  - $h$ --- hypothesis

    **<u>Univariate Liner Regression / Linear Regression with one variable</u>**
    $$
    h_\Theta(x) = \Theta _0 + \Theta _1 x
    $$
    

**Cost Function** $J(\Theta _0,\Theta _1)$ --- *Overall Objective Function*

Goal： minimize $J(\Theta _0,\Theta _1)$

**Square Error Function**
$$
\displaylines {J(\Theta _0,\Theta _1) = \frac{1}{2m} {\displaystyle \sum_{i=1}^m (h_\theta (x^{(i)})-(y^{(i)}))^2 }
\\ \space \underset {\Theta _0, \Theta _1}  {minimize} \space J(\Theta _0,\Theta _1) }
$$

Contour Plot  is  a better way to visualize $J(\Theta _0,\Theta _1)$ .

![image-20220316112050960](https://gitee.com/violets/typora--images/raw/main/imgs/202203161120162.png)

#### Multivariate linear regression

- **Feature scaling**: Get every feature into approximately a $-1 \le x _i \le 1$ range.

  Make sure features are on a similar scale, then gradient descent can converge more quickly.

  <img src="https://gitee.com/violets/typora--images/raw/main/imgs/202203190931206.png" alt="image-20220319093132010" style="zoom: 50%;" />

- **Mean normalization**: Replace $x_i$ with $x_i-\mu_i$ to make features have approximately zero mean. (Do not apply to $x_0 = 1$)

  <img src="https://gitee.com/violets/typora--images/raw/main/imgs/202203190946277.png" alt="image-20220319094608197" style="zoom:50%;" />

- "Debugging": How to make sure gradient descent is working correctly.

  Declare convergence if $J (\theta)$ decreases by less than $\epsilon$ in one iteration, but choose a proper threshold $\epsilon$ is hard.

- How to choose **Learning rate $\alpha$**

  - If $\alpha$ is too small: slow convergence
  - If $\alpha$ is too large: <u>$J(\theta)$ may not decrease on every iteration; may not converge;</u> slow convergence.

  Try a range of value for $\alpha$, every value is roughly 3 times bigger than its previous value. *<u>(e.g. $...0.001, 0.003, 0.01,0.03...$)</u>*, **Find a value too small, and a value that is too large, pick the largest possible value between them.**

## Gradient Descent

Have some function $J (\theta _0, \theta _1)$ *<u>can be $\theta _0, \theta _1 ... \theta _n$</u>*

Want $\underset {\theta _0, \theta _1}  {minimize} \space J(\theta _0,\theta _1)$

**Outline:**

- Start with some $\theta _0, \theta _1$
- Keep changing $\theta _0, \theta _1$ to reduce $J (\theta _0, \theta _1)$ until we hopefully end up at a minimum.

**<u>Subtlety:</u>**

1. All $\theta _j$ should be updated simultaneously.

2. As we approach a local minimum, gradient descent will automatically take smaller steps. So, <u>no need to decrease $\alpha$ over time.</u>

   <img src="C:%5CUsers%5CLENOVO%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20220318105203244.png" alt="image-20220318105203244" style="zoom:50%;" />

#### “Batch” Gradient Descent

Each step of gradient descent uses all the training examples.

## Normal Equation

Method to solve for $\theta$ analytically
$$
\theta = (X^T X)^{-1} X^T y
$$
$X$ --- design matrix

<img src="C:%5CUsers%5CLENOVO%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20220319141338956.png" alt="image-20220319141338956" style="zoom:50%;" />

***Comparison between Gradient descent & Normal equation***

<img src="https://gitee.com/violets/typora--images/raw/main/imgs/202203191358853.png" alt="image-20220319135851696" style="zoom:50%;" />

# CS 229

## Lecture 5 GDA & Naive Bayes

**Two examples**

1. Continuous value features (e.g. tumor classification) —— GDA
2. Discrete features (e.g. email spam, NLP)

### Gaussian Discriminant Analysis

**Multi-variate Gaussian**



## Lecture 6  Support Vector Machine

***

**Notation :**

![image-20220401105602016](https://gitee.com/violets/typora--images/raw/main/imgs/202204011056105.png)

![image-20220401105943402](https://gitee.com/violets/typora--images/raw/main/imgs/202204011059523.png)

Labels $y \in \{-1, +1\}$ 

***

### Optimal Margin Classifier

![image-20220401142258348](https://gitee.com/violets/typora--images/raw/main/imgs/202204011422483.png)

#### Functional Margin

$\Gamma \gamma$

![image-20220401103431429](https://gitee.com/violets/typora--images/raw/main/imgs/202204011034634.png)

If classifier has a large functional margin, two "if" statements listed above are TRUE.

![image-20220401111129080](https://gitee.com/violets/typora--images/raw/main/imgs/202204011111184.png)

$\Gamma _{i} ^{hat} \gt\gt 0$

![image-20220401111143855](https://gitee.com/violets/typora--images/raw/main/imgs/202204011111941.png)

- Easy to cheat and increase the functional margin --- (**Solution: Normalize the length of your parameters (or any length you want, doesn't change the classification)**)

![image-20220401111636388](https://gitee.com/violets/typora--images/raw/main/imgs/202204011116439.png)

#### Geometric Margin [1:12:30]

![image-20220401112245672](https://gitee.com/violets/typora--images/raw/main/imgs/202204011122770.png)

![image-20220401140353877](https://gitee.com/violets/typora--images/raw/main/imgs/202204011403957.png)

![image-20220401140630383](https://gitee.com/violets/typora--images/raw/main/imgs/202204011406485.png)

## Lecture 7 Kernels

### **$L _1$ norm soft margin SVM**

![image-20220403134635522](https://gitee.com/violets/typora--images/raw/main/imgs/202204031346717.png)



<u>For some kernel, we can use dynamic programming ***Knuth-Morris-Pratt*** to make $\phi (x)^T \phi (z) = K(x,z)$</u>

## Lecture 8 Data Splits, Models & Cross-validation

### Bias and Variance

![image-20220403185227848](https://gitee.com/violets/typora--images/raw/main/imgs/202204031852011.png)

### Regularization  $\lambda/2 \|\Theta\| ^2$  

regularization is the most effective way to prevent over-fitting. 

![image-20220403193711027](https://gitee.com/violets/typora--images/raw/main/imgs/202204031937102.png)

The optimization objective of the support vector machine was to minimize $\|w\|^2$, this turns out to maximize the geometric margin SVM

In order to make sure the $\lambda$ on the same scale, a common pre-processing step we're using learning algorithms is normalizing different sized features to a similar scale. [<u>the normalization also makes gradient descent run faster</u>]

**Another way to think about Regularization**

![image-20220403203326691](https://gitee.com/violets/typora--images/raw/main/imgs/202204032033797.png)

![image-20220403203441394](https://gitee.com/violets/typora--images/raw/main/imgs/202204032034503.png)

***Statistics world <u>Frequentist VS Bayesian</u>***

![image-20220403204437001](https://gitee.com/violets/typora--images/raw/main/imgs/202204032044118.png)



**Regularization and choose polynomial degree** 

![image-20220403205429341](https://gitee.com/violets/typora--images/raw/main/imgs/202204032054466.png)



### Cross Validation

**Different mechanistic procedures to find the optimum point**

1. Split your dataset into $S _{train}, S _{dev}, S _{test}$

   <u>$S_{dev}$ also called **Cross Validation Set**</u>

2. Train each model i (option for degree of polynomial) on $S _{train}$, get some hypothesis $h_i$

3. Measure error on $S _{dev}$. Pick model with lowest error on $S _{dev}$

   ***Don't evaluate algorithms on training set.*** *<u>Cause over-fit, because more complex algorithm will always do better on the training set.</u>*

4. *[Optional]* Evaluate the algorithm on a separate test set ($S _{test}$) and report that error.

**How do you decide how much data should go into  $S _{train}, S _{dev}, S _{test}$ ?**

|    Common Weight on Dataset    | $S _{train}$ | $S _{dev}$ | $S _{test}$ |
| :----------------------------: | :----------: | :--------: | :---------: |
| Small dataset(without dev set) |     70%      |     -      |     30%     |
|  Small dataset(with dev set)   |     60%      |    20%     |     20%     |
|         Large dataset          |     90%      |     5%     |     5%      |

- Choose $S _{train}, S _{dev}$ to be big enough 
- If you want to tease out very small differences(e.g. 0.001%), you may want a large $S_{test}$ .
- If you want to compare algorithms with large accuracy differences(e.g. 1% vs 2%),  small-size $S_{test}$ is enough.

**Do not make ANY decisions about your model using the test set.**

#### (Simple) Hold-out Cross Validation

==**When you have a large dataset, Cross Validation can be used to choose**==

1. ==the model of polynomial==
2. ==the regularization parameter $\lambda$ or $C$ or $\tau$==

#### K-fold Cross Validation

- Makes more efficient of the data
- Computationally expensive

==**When you have a small dataset, without too much data waste**==

$K = 10$ is typical

==[Use when $m=50$(roughly) or less]== When $K = m$ , this method called **Leave-one-out Cross Validation**

### Feature Selection

*<u>A special case for model selection</u>*

If you have a lot of features, one way to reduce over-fitting is to try to find a small subset of most useful features for your task.

#### Forward Search

Keep iterating until adding more features now hurts performance, then pick whichever feature subset allows you to have the best possible performance of dev set.

#### Backward Search

## Lecture 9 Approx/Estimation Error & ERM

**Assumptions**

1. Data distribution D

   ![image-20220407190938033](https://gitee.com/violets/typora--images/raw/main/imgs/202204071909127.png)

2. Independent samples

   ![image-20220406201138648](https://gitee.com/violets/typora--images/raw/main/imgs/202204062011584.png)

**Bias and Variance**

Bias and Variance correspond to first and second moment of the sampling distribution.

*There's no correlation between bias and variance.*

- *Parameter View*

![image-20220407193326253](https://gitee.com/violets/typora--images/raw/main/imgs/202204071933425.png)

- If you increase the size of data(i.e. $m$), the variance of $\hat \theta$ will be small

  ![image-20220408144523621](https://gitee.com/violets/typora--images/raw/main/imgs/202204081445704.png)

  ***Statistical Efficiency*** : how efficient of your algorithm from squeezing information from given amount of data.

  ![image-20220408145726716](https://gitee.com/violets/typora--images/raw/main/imgs/202204081457824.png)

**How we can Fight Variance**

- **How we can address variance?**

  1. Increase #Data $m --> \infin$

  2. Regularization

     ![image-20220408193347924](https://gitee.com/violets/typora--images/raw/main/imgs/202204081933083.png)

![image-20220408205348665](https://gitee.com/violets/typora--images/raw/main/imgs/202204082053781.png)

![image-20220408200426581](https://gitee.com/violets/typora--images/raw/main/imgs/202204082004799.png)

![image-20220408205205753](https://gitee.com/violets/typora--images/raw/main/imgs/202204082052960.png)

- **Fix high bias**

  Enlarge $H$

  Reduce bias, magnify variance.

### **Empirical Risk Minimizer**

![image-20220409142957707](https://gitee.com/violets/typora--images/raw/main/imgs/202204091430933.png)
$$
\begin{aligned}
\hat{\varepsilon}(h)&=\frac{1}{m} \sum_{i=1}^{m} 1\left\{h\left(x^{(i)}\right) \neq y^{(i)}\right\}\\
\hat{\theta} & =\arg \min _{\theta} \hat{\varepsilon}\left(h_{\theta}\right)
\end{aligned}
$$


#### Uniform Convergence

*e.g.* How the risk curve converges uniformly to the generalization risk curve.

1. A given hypothesis $h$ have some amount of training error $\hat{\varepsilon}(h)$, what does that say about its generalization error $\varepsilon(h)$

   $\hat{\varepsilon}(h) \space \text{vs} \space \varepsilon(h)$

   ![image-20220409160020182](https://gitee.com/violets/typora--images/raw/main/imgs/202204091600298.png)

   ==*Solution*==
   $$
   m \geq \frac{1}{2 \gamma^{2}} \log \frac{2 k}{\delta}
   $$
   
2. How does the generalization error of our learned hypothesis $\varepsilon(\hat {h})$ compare to the best possible generalization error in that class $\varepsilon({h} ^*)$

   $\varepsilon(\hat {h}) \space \text{vs} \space \varepsilon({h} ^*)$

   ![image-20220409171006478](https://gitee.com/violets/typora--images/raw/main/imgs/202204091710602.png)

   ![image-20220409171411755](https://gitee.com/violets/typora--images/raw/main/imgs/202204091714826.png)

- **Finite hypothesis class**

  ![image-20220409165820253](https://gitee.com/violets/typora--images/raw/main/imgs/202204091658405.png)

- **Infinite hypothesis class**

  The number of examples for a wanted sample complexity is generally an order of the VC dimension to get good results.

  ![image-20220409171922175](https://gitee.com/violets/typora--images/raw/main/imgs/202204091719297.png)

**Tools**

1. **Union Bound**

   ![image-20220409150627187](https://gitee.com/violets/typora--images/raw/main/imgs/202204091506263.png)

2. **Hoeffding's Inequality**

   The probability of your estimate deviating more than a certain margin only reduces as you increase m.
   $$
   P\left(\left|\varepsilon\left(h_{i}\right)-\hat{\varepsilon}\left(h_{i}\right)\right|>\gamma\right) \leq 2 \exp \left(-2 \gamma^{2} m\right)
   $$
   ![image-20220409153642053](https://gitee.com/violets/typora--images/raw/main/imgs/202204091536280.png)

## Lecture 10 Decision Tree and Ensemble Methods

Decision tree is a classical example model class to use with various ensembling methods

### [Decision Tree](https://aman.ai/cs229/decision-trees/)

**PROS**

- Easy to explain
- Interpretable
- Categorical Variables
- Fast

**CONS**

- High variance
- Bad at additive
- Low predictive accuracy

![image-20220411091029696](https://gitee.com/violets/typora--images/raw/main/imgs/202204110910929.png)

![image-20220411091617025](https://gitee.com/violets/typora--images/raw/main/imgs/202204110916112.png)

***How to choose split?***

Define $L(R)$ : Loss in R given C classes, define $\hat P_c$ to be the proportion of examples in $R$ that are of class $C$.

==Misclassification Loss==

$L _{misclass} = 1 - \underset{c}{max} \space \hat P_c$

![image-20220411193028414](https://gitee.com/violets/typora--images/raw/main/imgs/202204111930498.png)

![image-20220411111301378](https://gitee.com/violets/typora--images/raw/main/imgs/202204111113597.png)

*<u>Misclassification loss is not sensitive enough. (Right one makes better decision than left, but misclassification loss is same.)</u>* To solve this, define ==cross entropy loss==

Want to pick  a split that decrease the loss as much as possible.

$\underset{j,t}{max} \space \underbrace{L(R_p)}_{parent \space loss} - \underbrace{(L(R_1)+L(R_2))}_{children \space loss} $

==Cross Entropy Loss==

#bits you need to communicate to tell someone who already knows  what the probabilities are what class you are looking at.

$L_{cross} = - \underset{c}{\Sigma} \space \hat {P_c}log_2 \hat {P_c}$

 ![image-20220411192900233](https://gitee.com/violets/typora--images/raw/main/imgs/202204111929349.png)

- Strictly concave curve CAN successfully used for decision splits.

==Gini Loss==

$L_{gini} =\underset{c}{\sum} \space \hat {P_c}(1-\hat {P_c})$

#### Regression Tree

![image-20220411200937329](https://gitee.com/violets/typora--images/raw/main/imgs/202204112009442.png)

Decision trees for regression. Predict the mean of the values left instead of predict the majority of the class.

*e.g, predict the amount of snowfall you would expect in that area around that time.*
$$
\begin{aligned}
\hat y_m & = \frac{\underset{i ∈ R_m}{\sum} y_i}{|R_m|} \\

L_{squared} & =  \frac{\underset{i ∈ R_m}{\sum} (y_i - \hat y_m)^2}{|R_m|}
\end{aligned}
$$

#### Categorical Variable

![image-20220411200955312](https://gitee.com/violets/typora--images/raw/main/imgs/202204112009397.png)

if you have $q$ categories, there will be $2^q$ possible splits.

#### Regularization of Decision Trees

1. min leaf size
2. max depth
3. max #nodes
4. min decrease in loss
5. Pruning (misclassification with a validation set)

#### Runtime

![image-20220411212411097](https://gitee.com/violets/typora--images/raw/main/imgs/202204112124160.png)

![image-20220411212357987](https://gitee.com/violets/typora--images/raw/main/imgs/202204112123106.png)

#### No additive structure

![image-20220411212649153](https://gitee.com/violets/typora--images/raw/main/imgs/202204112126227.png)

### [Ensembling](https://aman.ai/cs229/ensemble-methods/)

Take $X_i$'s which are random variables that are independent identically distributed (IID)
$$
\begin{aligned}
Var(X_i) &= \sigma ^2 \\
Var(\bar X) = Var(\frac{1}{n} & \underset{i}{\sum} X_i) = \frac {\sigma ^2}{n}
\end{aligned}
$$
Drop the independence assumption, so now $X_i$'s just identically distributed(ID), $X$'s correlated by $\rho$.
$$
Var(\bar X) = \rho \sigma ^2 +\frac{1-\rho}{n} \sigma ^2
$$

#### Ways to Ensemble

1. ~~different algorithms~~

2. ~~different training sets~~

3. ==Bagging== (*e.g. Random Forests*)

   Try to approximate having different training sets.

4. ==Boosting== (*e.g. AdaBoost, XGBoost*)

#### Bagging - Bootstrap Aggregation

Take a bunch of bootstrap samples, train separate models on each and then average their outputs.

**Bootstrap**

1. Have a true population P
2. Training set $S \sim P$
3. Assume $P=S$
4. Bootstrap samples $Z \sim S$

**Bootstrap Aggregation**

1. Bootstrap samples $Z_1, Z_2, Z_3..., Z_M$
2. Train model $G_m$ on $Z_m$
3. Define a meta model $G(m) = \frac {\displaystyle \sum^{M}_{m=1} G_m(x)}{M}$

**Bias-Variance Analysis**

$Var(\bar X) = \rho \sigma ^2 +\frac{1-\rho}{M} \sigma ^2$

- Bootstrapping is driving down $\rho$

  *<u>Increasing the number of bootstrap models in your training, doesn't cause you to overfit anymore than you were beforehand.</u>*

- More $M$ leads to less variance. *(There is  a lower bound, can't make variance 0)*

- Bias is slightly increased because of random subsampling. *(Because the bootstrap samples $Z$ are actually subsets of the original set $S$, so you model become less complex, that increases bias)*

 **Decision trees + Bagging**

Decision trees are high variance, low bias.

Ideal fit for bagging

##### Random Forests

At each split, consider only a fraction of total features.

- Decrease $\rho$
- Decorrelate Models

#### Boosting

*Adaboost, XGBoost, gradient boost machine*

- Decrease bias
- Additive

==AdaBoost==

![image-20220412141131214](https://gitee.com/violets/typora--images/raw/main/imgs/202204121411433.png)

## lecture 11 Introduction to Neural Networks

---

**Outline**

- Logistic Regression (in DP view)
- Neural Network

**Deep Learning**

- Computational power
- Data available
- algorithms

---

### Logistic Regression (in DP view)

**Goal 1: Find cats in images**

![image-20220419200601535](https://gitee.com/violets/typora--images/raw/main/imgs/202204192006790.png)

`#Parameters` for this Logistics Model is `#weights + #bias` (i.e. `12288+1`), so the `#Parameters` depends on the size of the input.
$$
\begin{aligned}
neuron & = linear + activation \\
model & = architecture + parameters
\end{aligned}
$$

- **<u>*neuron*</u>**

  ![image-20220419201724267](https://gitee.com/violets/typora--images/raw/main/imgs/202204192017336.png)

  `wx+b`(or `z`) is the linear part, `σ` is the activation.

- **<u>*model*</u>**

  - architecture

    *In this case the architecture is a one-neuron neural network*

    **<u>Tips :</u>** One thing needs to be considered for architecture: *<u>Is the output layer have the same number of neurons as you wants. (`#classes` for classification and `1`  for regression)</u>*

  - parameters
  
    `w` and `b`
  
- Loss function

  $\mathcal L = -\left(y \log \hat y + \left(1-y \right) \log \left(1 -\hat y \right) \right)$

**Goal 2: Find cat/lion/iguana in images**

![image-20220419214245518](https://gitee.com/violets/typora--images/raw/main/imgs/202204192142671.png)

- Red part shows how to represent labels in the dataset.

- `#parameters = #neuron * #parameters_pre_neuron`

- Loss Function

  **How to train these parameters?**

  $\mathcal L _{3N} = -\displaystyle \sum ^3_{k=1} \left(y_k \log \hat y_k + \left(1-y_k \right) \log \left(1 -\hat y_k \right) \right)$

  $\mathcal L _{3N}$ stands for loss function for 3 neurons.
  
  The derivative of one specific k won't be more complex than one-neuron network.

**Goal 3: add constraint *<u>Unique animal in one image</u>* ==Softmax Multi-class <u>Network/Regression</u>==**

![image-20220419215120470](https://gitee.com/violets/typora--images/raw/main/imgs/202204192151585.png)

- [Softmax](https://e2eml.school/softmax.html) formula

  Instead of getting a probabilistic output for each $\hat y_i$, we will get a probability distribution over all the classes.

  *<u>Make same layer's neurons have "dependencies" (i.e. probabilities sum to 1) on each other due to the implement of Softmax.</u>*

- Labels' format

  ![image-20220419220141945](https://gitee.com/violets/typora--images/raw/main/imgs/202204192201016.png)

- `#parameters = #neuron * #parameters_pre_neuron`

- Loss Function ==Softmax Cross-entropy Loss==
  $$
  \begin{aligned}
  \mathcal L_{CE} = -\sum_{k=1}^m y_k \log \hat y_k
  \end{aligned}
  $$
  **<u>*Softmax Cross-entropy Loss is very often used in multi-classification*</u>**

### Neural Networks

**GOAL: Predict cat's age in the image**

- For neuron's activation, use ==ReLU== [(**<u>Rectified Linear Unit</u>**)](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)) 

- Modify Loss function to $|y-\hat y|$ or $\|y-\hat y\|^2$

  $L1$ and $L2$ norm loss is much easier to optimize for a regression task than it is for a classification task and vice versa.

**Goal 1: Given an image, tells if there's cat or no cat**

![image-20220420101623630](https://gitee.com/violets/typora--images/raw/main/imgs/202204201016813.png)

- `#parameters = sum(#parameters_in_each_layer)` (e.g, `#parems_in_1st_layer = 3n+3`)

- **Terminology** 

  ==Hidden Layer== : we don't really know what it's going to figure out, but with enough data, it should understand very complex information about the data.

  Similarly, 1st layer of the neuron network called ==Input Layer==, last layer is ==Output Layer==

  ==Fully connected layer==: all the neurons among previous layer are connected to each other in its next layer.

  ==End-to-end learning/Blackbox model== : Training just based on the input and output. (i.e. train the network without adding constraints for the hidden layers.)

  <img src="https://gitee.com/violets/typora--images/raw/main/imgs/202204201036632.png" alt="image-20220420103642501" style="zoom: 67%;" />

#### Propagation Equation

**Forward Propagation Equation**
$$
\begin{aligned}
z^{[\ell]}&=W^{[\ell]} a^{[\ell-1]}+b^{[\ell]} \\
a^{[\ell]}&=g^{[\ell]}\left(z^{[\ell]}\right)
\end{aligned}
$$


<img src="https://gitee.com/violets/typora--images/raw/main/imgs/202204201429095.png" alt="image-20220420142934957" style="zoom:80%;" />

**<u>*What happens for an input batch of m examples?*</u>**

![image-20220420143052904](https://gitee.com/violets/typora--images/raw/main/imgs/202204201430005.png)

- Need to broadcast $b^{[i]}$ to make the linear algebra work.

**Backward Propagation Equation**

<img src="https://gitee.com/violets/typora--images/raw/main/imgs/202204201431693.png" alt="image-20220420143143551" style="zoom:80%;" />

<img src="https://gitee.com/violets/typora--images/raw/main/imgs/202204201425232.png" alt="image-20220420142530063" style="zoom:80%;" />

- Need correspond Forward Propagation Equation to remember the path to take in your chain rule

#### Improving Your Neural Networks

- **different Activation Functions**

  - Sigmoid

    **<u>Pros</u>** Used in classification problems

    **<u>Cons</u>** Sigmoid activation works good in its linear regime, but has trouble working in saturating regime. (Because if z is very high or very low, your gradient is very close to 0, makes z hard to update)

  - ReLU

    ***ReLU is mostly used***
    $$
    ReLU'(z) = \mathbb I\{z>0\}
    $$
    

    Slope is 1, so ReLU is actually just directing the gradient to some entry.

  - tanh
    $$
    \begin{aligned}
    \tanh(z) & = \frac{e^z - e^{-z}}{e^z + e^{-z}} \\
    \tanh '(z) & = 1 - \tanh(z)^2
    \end{aligned}
    $$
    **<u>Pros and Cons</u>** same as sigmoid

  *<u>**Why we need activation functions?**</u>*
  
  If you don't use activation functions (i.e. use identity function as activation), your neural network is going to be equivalent to a linear regression, no matter how deep it is.
  
- **Initialization methods**

  - Normalize your inputs

    <img src="https://gitee.com/violets/typora--images/raw/main/imgs/202204211018031.png" alt="image-20220421101825896" style="zoom:50%;" />

    <img src="https://gitee.com/violets/typora--images/raw/main/imgs/202204211018418.png" alt="image-20220421101803227" style="zoom:50%;" />

  - Vanishing and exploding gradients

    Errors add up by multiply each other. When neurons' weights is bigger than one, explode situation; when it's less than one, vanish situation.

    To solve this issue,  we need to ***initialize neurons' weights properly.***

    <img src="https://gitee.com/violets/typora--images/raw/main/imgs/202204211037277.png" alt="image-20220421103735153" style="zoom:50%;" />

    <img src="C:%5CUsers%5CLENOVO%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20220421103705786.png" alt="image-20220421103705572" style="zoom:50%;" />

    ***Some commonly used weight initialization techniques***

    ```python
    # Commonly used weights initialization techinques
    ## Initialize one layer's weight proportionally to #former_layer_inpus (works very well for sigmoid activations)
    ### add random to avoid symmetry problems(every is going to learn the same thing)
    w_l = np.random.randn(shape)*np.sqrt(1/n_(l-1))
    ### Modify above function a little bit, makes it works better on ReLU
    w_l = np.random.randn(shape)*np.sqrt(2/n_(l-1))
    ## Xavier Initialization (for tanh activations)
    ## He Initialization (very often used), doing the same thing but also for the back propagated gradients
    ```

    <img src="https://gitee.com/violets/typora--images/raw/main/imgs/202204211050932.png" alt="image-20220421105004806" style="zoom:50%;" />

- Optimization

  - ==Mini-batch gradient descent==

    *A trade-off between stochastic gradient descent and batch GD. (i.e. a trade-off between stochasticity and vectorization)*

    <img src="https://gitee.com/violets/typora--images/raw/main/imgs/202204211119227.png" alt="image-20220421111907156" style="zoom:50%;" />

    ```pseudocode
    /*Mini-batch gradient descent*/
    mini_m = #examples_per_minibatch
    for t in (1, #iteration):
    	select a batch x[t],y[t]
    	forward_propagate(batch) /*J  = 1/mini_m * sum(L[t])*/
    	backward_propagate(batch)
    	update w[l],b[l]
    ```

    <img src="https://gitee.com/violets/typora--images/raw/main/imgs/202204211114130.png" alt="image-20220421111455037" style="zoom:50%;" />

    <center> left: Batch GD right: Mini-Batch GD

    The smaller the batch, the more stochasticity, the more noise on the cost function graph.

  - ==(GD +)Momentum Algorithm==
    $$
    \begin{array}{l}
    v_{d W^{[\ell]}}=\beta v_{d W^{[\ell]}}+(1-\beta) \frac{\partial J}{\partial W^{[\ell]}}\\
    W^{[\ell]}=W^{[\ell]}-\alpha v_{d W[\ell]}
    \end{array}
    $$
    <img src="https://gitee.com/violets/typora--images/raw/main/imgs/202204211127799.png" alt="image-20220421112719707" style="zoom:50%;" />

    *Take past updates into consideration (Look at the past update and take the average of it) in order to find the right way to go.*

  - RMS prop (CS230)

  - Atom (CS230)

- Regularization

## Lecture 13 Debugging ML Models and Error Analysis

---

***Outline***

1. Diagnostics for debugging learning algorithms
2. Error analyses and ablative analysis
3. How to get started in a machine learning  problem.
   - Premature (statistical) optimization

---

### Diagnostics for debugging learning algorithms

**Workflow to figure out what's the problem**

- Bias & Variance Diagnostics

- Optimization Diagnostics

  - Is the algorithm converging?
  - Is there something wrong with the numerical model?
  - Did we use the right cost function $J(\theta)$

- Error analysis

  *Figure the differences between where you are and perfect performance.*

  When you have a  complex machine learning pipeline, error analysis helps you break down the error (i.e. attribute the error to different components), which let you focus on what to work on.

- Ablative Analysis

  *Figure the differences between where you are and something much worse.*

## Lecture 14 Expectation-Maximization Algorithms

EM implements a softer way of assigning points to the different cluster centroids.

- To do EM, we need a concave function

1. **E-step**
   $$
   Q _i(z^{(i)}) = P(z ^{(i)} \mid x ^{(i)};\theta)
   $$
   *$Q _i(z^{(i)})$ is $w _j^{(i)}$*

2. **M-step**
   $$
   \theta := \arg \max _\theta \sum_i \sum_{z ^{(i)}} Q _i(z^{(i)}) \log \frac {P(x ^{(i)}, z^{(i)} ; \theta)}{Q _i (z^{(i)})}
   $$

Iterative E-step and M-step, the algorithm should converge to a local optima.

*<u>Question: Why don't we just maximize $\max _\theta  l (\theta)$ ?</u>*

*Because there's no known way to solve that.*

### Clustering (K-means)

**<u>How to choose $K$ ?</u>**

*<u>Issue:</u>* `#clusters` might be ambiguous.

*<u>Solution:</u>* 

1. [AIC](https://en.wikipedia.org/wiki/Akaike_information_criterion) or BIC criteria for automatically choosing `#clusters`
2. **Choose manually**

**<u>What to do when K-means stuck in local minima?</u>**

Run K-means on different iteration times and different initializations of cluster centroids. Pick the lowest cost function $J(c,\mu)$ run.

### Density Estimation

<img src="C:%5CUsers%5CLENOVO%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20220424143810471.png" alt="image-20220424143810471" style="zoom:50%;" />

**EM algorithm visualization:**

- At E-step, constructing a lower bound (green curve) for the log-likelihood.

  Green curve has two properties

  1. Green curve is a lower bound (i.e. green curve lies blow the blue curve)

  2. Its value is equal to the blue curve at the current value of $\theta$. (<u>*This property guarantees that when you optimize the green function, you can improving blue function too.*</u>)
     $$
     \log {E_{z^{(i)} \sim Q_i}\left[\frac {P(x ^{(i)}, z^{(i)} ; \theta)}{Q _i (z^{(i)})}\right]} = E_{z^{(i)} \sim Q_i}\left[\log {\frac {P(x ^{(i)}, z^{(i)} ; \theta)}{Q _i (z^{(i)})}}\right]
     $$
     To ensure this property, we need
     $$
     Q _i(z^{(i)}) = P(z ^{(i)} \mid x ^{(i)};\theta)
     $$
     

- At M-step, take the green curve and find its maximum

  Move $\theta$ from green value to the red value.

#### Mixture of Gaussian Model

<img src="https://gitee.com/violets/typora--images/raw/main/imgs/202204231727286.png" alt="image-20220423172723025" style="zoom:50%;" />

**Gaussian Discrimination Analysis(GDA) vs Mixture of Gaussians**

1. **Dataset** *(supervised or unsupervised)*

   For a given dataset, GDA has example $(x^{(i)}, y^{(i)})$ where $y^{(i)}$ is observed, MG doesn't.

2. **Probability Distribution**

   For GDA, $y^{(i)} \sim Bernoulli(\phi)$

   For MG, $z^{(i)} \sim Multinomial(\phi)$

3. **Variance for Gaussian Distribution** $\Sigma$

   For GDA, $x^{(i)} \mid y^{(i)}_j \sim \mathcal N (\mu _j ,\Sigma)$

   For MG, $x^{(i)} \mid z^{(i)}_j \sim \mathcal N (\mu _j ,\Sigma _j)$

#### Jensen's Inequality

Let $f$ be a convex function (i.e. $f''(x) \gt 0$), let $x$ be a random variable, then $f(EX) \le E[f(x)]$

**<u>Addendum</u>**

- If $f''(x) \gt 0$ (i.e. $f$ is strictly convex), then $f(EX) = E[f(x)] \Longleftrightarrow X = E[X] \text { with probablity 1 }(i.e. x \text{ is a constant})$ 
- *<u>Jensen's Equality in concave form:</u>* Let $f$ be a ***concave*** function (i.e. $f''(x) \lt 0$), let $x$ be a random variable, then $f(EX) \ge E[f(x)]$

## Lecture 15 EM Algorithms & Factor Analysis

***

**Outline :**

- **EM convergence**

  How to monitor if EM is converging

- **Gaussian Properties**

  Map the EM equations back to mixture of Gaussian models $Q _i(z^{(i)}) \Rightarrow w _j^{(i)}$

- **Factor Analysis** :star:

  A useful model for datasets which is very high-dimensional but very few training examples

- **Gaussian Marginals & Conditionals**

- **EM Steps**

  Derive for the Factor Analysis model

***

### EM convergence

*Different $Q _i(z^{(i)})$ means different choices of lower bounds. In algorithm perspective, we write code to compute $w ^{(i)}_j$*

### Factor Analysis $z^{(i)} \sim \mathcal N$

Factor analysis can take very high dimensional data and model them to a lower dimensional subspace with a little bit of fuzz.

$\color{WildStrawberry} \text {If the data doesn't lie in a  subspace, model may not be the best model. } $

$\color{NavyBlue} \text{But it's still a reasonable way to fit a Factor Analysis to n>>m dataset. }$ [*[Time Label]*](https://www.youtube.com/watch?v=tw6cmL5STuY&t=2759s)

A continuous $z$ EM model

- **One Other View of EM**

- Coordinate Ascent $J(\theta, Q)$

- Comparison between Mixture of Gaussian & Factor Analysis

  - When $m \gg n$, use MG

    <img src="https://gitee.com/violets/typora--images/raw/main/imgs/202204242111882.png" alt="img" style="zoom:25%;" />

  - else FA

If `#Training Examples` < the dimension of the data, usual MLE function of covariance $\Sigma$ will be singular(non-invertible)

### Gaussian Marginals & Conditionals

<img src="https://gitee.com/violets/typora--images/raw/main/imgs/202204251504385.png" alt="image-20220425150407136" style="zoom:50%;" />

**Marginal :** calculate $P(x_1)$ and we have $x_1 \sim \mathcal N (\mu _1, \Sigma _{11})$

**Conditional :** calculate $P(x_1 \mid x_2)$ we have $x_1 \mid x_2 \sim \mathcal N (\mu _{1 \mid 2}, \Sigma _{1 \mid 2})$
$$
\begin{aligned}
\mu _{1 \mid 2} & = \mu_1 + \Sigma_{12} \Sigma_{22}^{-1} (x_2 - \mu _2)  \\
\\
\Sigma _{1 \mid 2} & = \Sigma_{11} - \Sigma_{12} \Sigma_{22}^{-1} \Sigma_{21}
\end{aligned}
$$


### EM Steps

1. Derive $P(x, z)$, $z$ and $x$ has a joint Gaussian distribution
   $$
   \begin{aligned}
   \begin{pmatrix}
   z \\
   x 
   \end{pmatrix} & \sim \mathcal N (\mu _{x,z}, \Sigma) \\
   z & \sim  \mathcal N (0,I) \\
   x & = \mu + \Lambda z + \epsilon
   \end{aligned}
   $$
   <img src="https://gitee.com/violets/typora--images/raw/main/imgs/202204251533971.png" alt="image-20220425153323755" style="zoom:50%;" />

   Simplify above equations. Finally we have
   $$
   \begin{aligned}
   \mu _{x,z} & = \begin{pmatrix}
   0 \\
   \mu 
   \end{pmatrix} \\
   \Sigma & = \begin{pmatrix}
   I & \Lambda ^T \\
   \Lambda & \Lambda \Lambda ^T + \Psi
   \end{pmatrix}
   
   \end{aligned}
   $$
   Putting everything together, we have
   $$
   \left[\begin{array}{l}
   z \\
   x
   \end{array}\right] \sim \mathcal{N}\left(\left[\begin{array}{l}
   \vec{0} \\
   \mu
   \end{array}\right],\left[\begin{array}{lc}
   I & \Lambda^{T} \\
   \Lambda & \Lambda \Lambda^{T}+\Psi
   \end{array}\right]\right)
   $$
   <font color=DarkBlue>There's no parameters' closed form when solve derivatives of</font> $\color{DarkBlue} P(x^{(i)})$<font color=DarkBlue>'s log likelihood.</font>

   <font color=Crimson>So, we use EM to solve these parameters.</font>

2. **EM for Factor Analysis**

   - **E-Step**
     $$
     \begin{aligned}
     Q _i(z^{(i)}) & = P(z ^{(i)} \mid x ^{(i)};\theta) \\
     z^{(i)} \mid x^{(i)} & \sim \mathcal N (\mu _{z^{(i)} \mid x^{(i)}}, \Sigma _{z^{(i)} \mid x^{(i)}})
     \end{aligned}
     $$
     Where
     $$
     \begin{aligned}
     \mu _{z^{(i)} \mid x^{(i)}} & = \vec 0 + \Lambda ^T (\Lambda \Lambda^T + \Psi) ^ {-1} (x^{(i)} - \mu) \\
     \Sigma _{z^{(i)} \mid x^{(i)}} & = I - \Lambda ^T (\Lambda \Lambda^T + \Psi) ^ {-1} \Lambda
     \end{aligned}
     $$
     
- **M-step**
     $$
     \begin{aligned}
     \Lambda & =\left(\sum_{i=1}^{m}\left(x^{(i)}-\mu\right) \mu_{z^{(i)} \mid x^{(i)}}^{T}\right)\left(\sum_{i=1}^{m} \mu_{z^{(i)} \mid x^{(i)}} \mu_{z^{(i)} \mid x^{(i)}}^{T}+\Sigma_{z^{(i)} \mid x^{(i)}}\right)^{-1} \\
     \mu & = \frac{1}{m} \sum_{i=1}^{m} x^{(i)} \\
     \Psi &= \frac{1}{m} \sum_{i=1}^{m} x^{(i)} x^{(i)^{T}}-x^{(i)} \mu_{z^{(i)} \mid x^{(i)}}^{T} \Lambda^{T}-\Lambda \mu_{z^{(i)} \mid x^{(i)}} x^{(i)^{T}}+\Lambda\left(\mu_{z^{(i)} \mid x^{(i)}} \mu_{z^{(i)} \mid x^{(i)}}^{T}+\Sigma_{z^{(i)} \mid x^{(i)}}\right) \Lambda^{T}
     \end{aligned}
     $$
     
   
  ***Tips:***
  
  Simplify integrals by
  $$
  \underset {z^{(i)}} \int {Q_i(z^{(i)}) {z^{(i)}}} d{z^{(i)}} = E[z^{(i)}] = \mu _{z^{(i)} \mid x^{(i)}}
  $$

## Lecture 16 Independent Components Analysis & Reinforced Learning

---

**Outline :**

- **Independent Components Analysis**
  - CDF (cumulative distribution functions) 
  - ICA model
- Reinforcement Learning
  - MDP (Markov decision processes)

---

### Independent Components Analysis

- If the data is Gaussian, ICA is not possible. Because Gaussian distribution is rotational symmetric, so there will be a rotational ambiguity (*i.e. Any axis can be your component.*).

  <font color=Salmone>Gaussian density is the only distribution that is rotationally symmetric.</font>

1. Compute density $x$

   $P_x (X) = P_s(WX) |W|$

2. Choose the density of s $\longleftrightarrow$ $P_s (S) = ?$

   Common choice of $P_s (S)$

   - Laplacian Distribution (Double-Sided Exponential Function)
   - Derivation of SIgmoid

To summarize, we have
$$
\begin{aligned}
P_x(X) & = P_s(WX) |W| \\
& = \left(\prod_{j=1}^n P_s\left(w_j^T x \right)\right) |W|
\end{aligned}
$$
MLE of this is
$$
l(w) = \sum _{i=1}^m \log \left(\prod_j P_s(w_j ^T x^{(i)}) |W| \right)
$$

### Reinforcement Learning

When you don't have a mapping from X to Y, you can't use supervised learning.

Reinforcement Learning specify a reward function.

Our goal is to write a cost function of a reward function give good results a high reward.

**Challenge :**  Credit Assignment

#### MDP (Markov decision processes)

MDP is a five-element tuple, **(S, A, {$P_{Sa}$}, $\gamma$, R)**. MDP provides the formalism in which RL problems are usually posed.

- $S$ --- set of states

- $A$ --- set of actions

- $P_{Sa}$ --- state transition probabilities ($\sum _{s'} P_{Sa}(s') = 1$)

- $\gamma$ --- discount factor ($\gamma \in [0,1)$ usually to be chosen slightly less than 1)

  $\gamma$ is the power of the time that reward is multiplied by. *(e.g. encourages the robot to get deposited rewards faster or postpone the negative rewards.)*

  $\gamma$ guarantees that total payoff is a bounded value.

- $R$ --- reward function

***Goal of RL*** : Choose actions over time to maximize the expected value of the total payoff. $E\left[\sum_i \gamma ^i R(s_i) \right]$

Output a optimal policy/controller $\pi (s)$ / $a = \pi (s)$ that maps states to actions *(i.e. $\pi : S \rightarrow A$)*

**Value Function:** $V ^{\pi} (s)$ is simply the expected sum of discounted rewards upon starting in state s, and taking actions according to $\pi$
$$
V^{\pi}(s)=\mathrm{E}\left[R\left(s_{0}\right)+\gamma R\left(s_{1}\right)+\gamma^{2} R\left(s_{2}\right)+\cdots \mid s_{0}=s, \pi\right]
$$

#### Bellman equations

Bellman equation can be used to efficiently solve for $V^\pi$
$$
\begin{aligned}
V^{\pi}(s) &= R(s)+\gamma \sum_{s^{\prime} \in S} P_{s \pi(s)}\left(s^{\prime}\right) V^{\pi}\left(s^{\prime}\right) \\
&= R(s)+\gamma E_{s^{\prime} \sim P_{s \pi(s)}}\left[V^{\pi}\left(s^{\prime}\right)\right]
\end{aligned}
$$

Bellman equation sets up a solvable system of linear equations.

## Lecture 17 MDPs & Value/Policy Iteration

*<u>Challenge of finding a  policy: There's exponentially large number of possible policies.</u>*

So how to compute the optimal policy?

- finding $V^*$
- finding $\pi ^ *$

---

**What if don't know $P_{sa}$ ?**

Can perform Laplace smoothing to avoid $\frac{0}{0}$ evaluation, but not necessary. Because unlike Naive Bayes, Reinforcement learning is not that sensitive for 0 values.

---

 **MDP with unknown state transition probabilities**

```code
1.Initialize π randomly
2.Repeat{
        (a)Execute π in the MDP for some number of trials.
        (b)Using the accumulated experience in the MDP,update our esti-
        mates for $P_{sa}$ (and R,if applicable).
        (c)Apply value iteration with the estimated state transition probabil-
        ities and rewards to get a new estimated value function V.
        (d)Update π to be the greedy policy with respect to V.
        }
```

### Value Iteration

- **Focus on finding $V^*$**

For absorbing state, set its $P_{sa}$ to 0

Value iteration converges very quickly. (Due to $\gamma$, converges exponentially quickly) 

### Policy Iteration

- **Focus on finding $\pi ^ *$** 

### Comparison

**Value iteration VS Policy Iteration**

- Policy iteration works good for relatively small `#states` problems, poor on large.

  For small problem, policy iteration converge faster than value iteration.

- Value Iteration will converge to $V^*$, but won't ever get to exactly $V^*$

### **Exploration VS Exploitation Problem**

When you acting a MDP, how aggressively of how greedy should you be at just taking actions to maximize your rewards?

Using **epsilon greedy** to solve this problem.

```code
1.Initialize π randomly
2.Repeat{
        (a)Execute EPSILON-GREEDY π in the MDP for some number of trials.
        (b)Using the accumulated experience in the MDP,update our esti-
        mates for $P_{sa}$ (and R,if applicable).
        (c)Apply value iteration with the estimated state transition probabil-
        ities and rewards to get a new estimated value function V.
        (d)Update π to be the greedy policy with respect to V.
        }
```

**Questions**

- Is $\epsilon$ in **epsilon greedy** have to be constant?

  No, it doesn't have to be. *Boltzmann exploration

- Can you get a reward for reaching states you've never seen before?

  Intrinsic reinforcement learning / Intrinsic Motivation.

- How many actions should you take before updating $\pi$?

  Run as frequently as it can.

## Lecture 18 Continuous State MDP & Model Simulation

---

**Outline**

- Discretization

- Models/Simulation

- Fitted Value Iteration

  Fitted Value Iteration works best with a model/simulator of the MDP

---

### Discretization

<font color=darkRed>CONS</font>

1. Not smooth

2. Curse of dimensionality

   If state space is in $s \in \mathbb R ^n$, and discretize each into $k$ values, get $k^n$ discrete states.

   Besides, large $n$ will also cause computational complexity problem,

### Models/Simulation of MDP

<img src="https://gitee.com/violets/typora--images/raw/main/imgs/202205051517700.png" alt="image-20220505151714544" style="zoom:50%;" />

**How to build a model?**

- Physics simulator

- Learn model from data

  1. Get $m$ examples
     $$
     \begin{align}
     & s_{0}^{(1)} \stackrel{a_{0}^{(1)}}{\longrightarrow} s_{1}^{(1)} \stackrel{a_{1}^{(1)}}{\longrightarrow} s_{2}^{(1)} \stackrel{a_{2}^{(1)}}{\longrightarrow} \cdots \cdot \stackrel{a_{T-1}^{(1)}}{\longrightarrow} s_{T}^{(1)} \\
     & s_{0}^{(2)} \stackrel{a_{0}^{(2)}}{\longrightarrow} s_{1}^{(2)} \stackrel{a_{1}^{(2)}}{\longrightarrow} s_{2}^{(2)} \stackrel{a_{2}^{(2)}}{\longrightarrow} \cdots \stackrel{a_{T-1}^{(2)}}{\longrightarrow} s_{T}^{(2)} \\
     & s_{0}^{(m)} \stackrel{a_{0}^{(m)}}{\longrightarrow} s_{1}^{(m)} \stackrel{a_{1}^{(m)}}{\longrightarrow} s_{2}^{(m)} \stackrel{a_{2}^{(m)}}{\longrightarrow} \cdots \stackrel{a_{T-1}^{(m)}}{\longrightarrow} s_{T}^{(m)} \\
     \end{align}
     $$
     
2. Apply supervised learning to estimate $s_{t+1}$ as function of $s_t, a_t$.
  
   *e.g. linear regression version* 
  
   - Deterministic model
   
     $s_{t+1} = A s_t + B a_t$
   
     A lot of deterministic model will learn a  brittle model.
   
     Exception: LQR and LQG will use deterministic models.
   
   - Stochastic model
   
     $s_{t+1} = A s_t + B a_t + \epsilon_t$ where $\epsilon_t \sim \mathcal N (0, \sigma ^2 I)$ 
   
   $$
     \arg \min _{A, B} \sum_{i=1}^{m} \sum_{t=0}^{T-1}\left\|s_{t+1}^{(i)}-\left(A s_{t}^{(i)}+B a_{t}^{(i)}\right)\right\|^{2}
   $$
  
   

==Model-based reinforcement learning==: build a model of the robot and train the RL algorithm in a simulator, and take the learned policy and apply it back on your real robot.

**Choose feature $\phi (s)$ of state $s$ **

When you designing features, pick a bunch of features that you think hope convey how well is your robot doing.
$$
V(s) = \theta ^T \phi(s)
$$

### Fitted Value Iteration

<img src="https://gitee.com/violets/typora--images/raw/main/imgs/202205051637190.png" alt="image-20220505163700073" style="zoom:80%;" />

**Tricks**

![image-20220505191832726](https://gitee.com/violets/typora--images/raw/main/imgs/202205051918895.png)

![image-20220505192110935](https://gitee.com/violets/typora--images/raw/main/imgs/202205051921065.png)

- Important to have noise in the simulator in model based RL, but when you're deploying, set noise $\epsilon = 0$ and $k=1$

## Lecture 19 Reward Model & Linear Dynamical System

---

**Outline**

- State-action rewards

- Finite horizon MDP

- Linear dynamical systems

  *Can compute the exact value function (without approximation) even though the state space is continuous.*

  - Model
  - LQR (Linear Quadratic Regulation)

---

### State-action rewards

Rewards is a function mapping from states and actions to the rewards (i.e. $R(s, a) : S \times A  \longmapsto \mathbb R$)

In this case, Bellman's equation is
$$
V^*(s) = \max _a \left(R(s, a) + \gamma \sum _{s^\prime} P_{sa}(s^\prime)V^*(s^\prime)\right)
$$
Can use value iteration to solve $V^*(s)$, then you can get optimal policy, which is
$$
\pi^* (s)  = \arg \max _a \left(R(s, a) + \gamma \sum _{s^\prime} P_{sa}(s^\prime)V^*(s^\prime)\right)
$$

### Finite horizon MDP

$\left(\mathcal{S}, \mathcal{A}, P_{s a}^{(t)}, T, R^{(t)}\right)$

Replace discount factor $\gamma$ with a horizon time $T$, MDP will run a finite number of $T$  steps.

- Action you take might depend on what time it is on the clock.

  Thus $\pi$ should be time dependent (i.e. $\pi _t^*(s)$) ==Non-stationary policy==

  **Non-stationary state transitions** $s_{t+1} \sim P_{s_t a_t}^{(t)}$

  **Non-stationary Reward** $R ^{(t)} (s,a)$

  *Examples*

  - Changing dynamics
  - Weather forecasts
  - Industrial automation

**How to solve for a finite horizon MDP**

1. Define the optimal value function
   $$
   V_{t}(s)=\mathbb{E}\left[R^{(t)}\left(s_{t}, a_{t}\right)+\cdots+R^{(T)}\left(s_{T}, a_{T}\right) \mid s_{t}=s, \pi\right]
   $$
   $V_{t}(s)$ it the total payoff start on state $s$ at time $t$ execute $\pi$

2. Value iteration (a dynamic programming problem in this case)
   $$
   \forall t<T, s \in \mathcal{S}: \quad V_{t}^{*}(s):=\max _{a \in \mathcal{A}}\left[R^{(t)}(s, a)+\mathbb{E}_{s^{\prime} \sim P_{s a}^{(t)}}\left[V_{t+1}^{*}\left(s^{\prime}\right)\right]\right]
   $$
   Base case (the final step)
   $$
   V^*_T (s) = \max _a R(s,a)
   $$
   Get the optimal policy
   $$
   \pi_t^*(s) =  \arg \max _a \left(R(s, a) + \sum _{s^\prime} P_{sa}(s^\prime)V_{t+1}^*(s^\prime)\right)
   $$
    *<u>Make $R, P_{sa}$ be $R^{(t)}, P_{sa}^{(t)}$ for non-stationary problems</u>*

   - If make $T$ infinite, value function $V_t^* (s)$ will be unbounded, thus simply make $T$ infinite wouldn't get a <u>discounted MDP formalism (traditional MDP)</u> value iteration, we also need discount $\gamma$ to ensure a value function bound.

### Linear Quadratic Regulation (LQR)

Convenient to develop with the finite horizon setting $\left(\mathcal{S}, \mathcal{A}, P_{s a}^{(t)}, T, R^{(t)}\right)$, but also works with discounted MDP formalism  $\left(\mathcal{S}, \mathcal{A}, P_{s a}, \gamma, R \right)$

**Where to get $A,B$ ?**

- Learn from data

  Linear regression on m examples

- Linearize a non-linear model

  

***A remarkable property***

- Using LQR make the value function a quadratic function, can solve $V^\star$ exactly.

#### Dynamic programming for LQR

1. Base case
   $$
   \begin{aligned}
   V_T^{\star}(s_T) &= \max_{a_T} \space R\left(s_T,a_T\right) \\
   &= - s_T^T U s_T \\
   \pi^{\star}_T (s_T) &= \vec 0
   \end{aligned}
   $$
   ==Initialize $\Phi _T = -U, \Psi_T = \vec 0$==

2. The key step

   It's can be show that
   $$
   \begin{align}
   \text {if } \space V_{t+1}^{*}\left(s_{t+1}\right) & = s_{t+1}^{\top} \Phi_{t+1} s_{t+1}+\Psi_{t+1} \\  
   \text {then }  \space V_{t}^{*}\left(s_{t}\right) & = s_{t}^{\top} \Phi_{t} s_{t}+\Psi_{t} 
   \end{align}
   $$
   Then we can solve LQR recursively.

   ==Recursive calculate $\Phi_t, \Psi_t$   using $\Phi_{t+1}, \Psi_{t+1}$ for $t=T-1,T-2,...,0$==
   $$
   \begin{array}{l}
   \Phi_{t}=A_{t}^{\top}\left(\Phi_{t+1}-\Phi_{t+1} B_{t}\left(B_{t}^{\top} \Phi_{t+1} B_{t}-W_{t}\right)^{-1} B_{t} \Phi_{t+1}\right) A_{t}-U_{t} \\
   \Psi_{t}=-\operatorname{tr}\left(\Sigma_{t} \Phi_{t+1}\right)+\Psi_{t+1}
   \end{array}
   $$
   ==Calculate $L_t$==

   ==$\pi ^\star (s_t) = L_t s_t$==
   $$
   \begin{aligned}
   L_{t} &=\left[\left(B_{t}^{\top} \Phi_{t+1} B_{t}-W_{t}\right)^{-1} B_{t} \Phi_{t+1} A_{t}\right]\\
   \pi ^\star(s_t) &= a_{t}^{*} =\left[\left(B_{t}^{\top} \Phi_{t+1} B_{t}-V_{t}\right)^{-1} B_{t} \Phi_{t+1} A_{t}\right] \cdot s_{t} \\
   &=L_{t} \cdot s_{t} \\
   \end{aligned}
   $$
   ***Takeaway: Optimal action is a linear function of state $s_t$***

*<u>Fun Fact about LQR</u>*

- $L_t$ depends on $\Phi_{t+1}$ but not $\Psi_{t+1}$, means that in order to take action, constant item for the quadratic function doesn't matter. :warning: ==So it's NOT NECESSARY to calculate $\Psi$ in the LQR algorithm==

  Thus $\pi ^\star, L_t$ don't depend on $\Sigma_w$, but $V ^\star$ does.

## Lecture 20 RL Debugging and Diagnostics

---

**Outline :**

- RL debugging/diagnostics
- Policy search
- Conclusion

---

### RL debugging/diagnostics

1. Improving simulator
2. Modify value function $V_\pi (s)$
3. Modify RL algorithms $R(s)$

### (Direct) Policy Search

Solve $\pi ^ \star$ directly (instead of using $V^\star$ to solve $\pi ^ \star$)

DPS focus on coming up with the class of policies you'll entertain or come up with the set of functions you use to approximate the policy.

***New definition:*** A stochastic policy is a function $\pi : S \times A \longmapsto \mathbb R$ when $\pi(s,a)$ is the probability of taking action $a$ in state $s$ ($\sum_a \pi(s,a)=1$)

**Goal :** Find $\theta$ so that when we execute $\pi_\theta (s,a)$, we maximize total payoff (expected sum of rewards), i.e.
$$
\max_ \theta E\left[R\left(s_0,a_0\right)+...+R\left(s_T,a_T\right)\mid \pi_\theta \right]
$$
 **How to do that** Derive a stochastic gradient ascent algorithm as a function of $\theta$ solve the equation above.

==Reinforce Algorithm== (*Very inefficient*)

```pseudocode
loop{
/*Sample*/
s_0, a_0, s_1,a_1 ... s_T,a_T
/* Compute payoff*/
$R(s_0,a_0),...,R(s_T,a_T)$
/*Update $\theta$ (using gradient ascent)*/
Implement $\theta$ update rule
}
```

$\theta$ update rule
$$
\theta := \theta + \alpha \left[\frac{\nabla \pi_\theta(s_0 ,a_0)}{\pi_\theta(s_0 ,a_0)} +\frac{\nabla \pi_\theta(s_1 ,a_1)}{\pi_\theta(s_1 ,a_1)}+...+\frac{\nabla \pi_\theta(s_T ,a_T)}{\pi_\theta(s_T ,a_T)} \right] \left(R(s_0,a_0),...,R(s_T,a_T)\right)
$$


***Note:*** One difference between policy search and estimated value function is that in Direct policy search $s_0$ is a fixed initial state $s_0$ or there's a fixed distribution over initial state $s_0$.

Direct policy search also works for continuous value function, in that situation, we have $a = \theta^T + \text{Gaussian noise}$.

#### Direct Policy Search vs Value-function-based Approach

When to use DPS:

1. POMDP (Partially Observable MDP)

   At each step, get a partial (and potentially noisy) measurement of the state. Have to choose an action $a$ using that.

   *<u>If we just have partially observed value of the state, even if we know $V^\star (s), \pi ^\star (s)$,  because we can't ensure what the state is, we still can not apply then. So we can only use DPS</u>*

   Can apply [Kalman filter](https://en.wikipedia.org/wiki/Kalman_filter) to estimate full state vector using partial observation state vectors, and plug them as features into policy search.

2. When $\pi^\star$ is simpler than $V^\star$

   For low-level control task, we often have simple map form $\mathcal S \longmapsto \mathcal A$ i.e. simpler policy $\pi^\star$. For multi-step reasoning problems, we should prefer to choose value function based approaches.

CONS for Reinforce Algorithm

- Very inefficient

  Gradient estimate for reinforcement algorithm turns out to be very noisy, even though the expected value is right.

# Andrew Ng Rocks！