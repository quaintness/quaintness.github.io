{"config":{"lang":["en","ja"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to MkDocs","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"CS/Intro_CS/","title":"Intro CS","text":"<p>Here is some lecture notes and assignments for computer science courses.</p>"},{"location":"CS/Basics/6_0001/","title":"Introduction to Computer Science and Programming in Python","text":"<p>[toc]</p>"},{"location":"CS/Basics/6_0001/#lecture-5-tuples-lists-aliasing-mutability-and-cloning","title":"Lecture 5 Tuples, Lists, Aliasing, Mutability and Cloning","text":""},{"location":"CS/Basics/6_0001/#tuples","title":"Tuples ()","text":"<p>Tuples hold collection of data.</p>"},{"location":"CS/Basics/6_0001/#characteristics","title":"Characteristics","text":"<ul> <li> <p>Tuples are immutable</p> </li> <li> <p>conveniently used to swap varoable values</p> </li> </ul> <pre><code>(x,y) = (y,x)\n</code></pre> <ul> <li>used to return more than one value from a function</li> </ul> <pre><code>def quotient_and_remainder(x,y)\n  quotient = x // y\n    remainder = x % y\n    return (quotient,remainder)\n\n(quot,rem) = quotient_and_remainder(4,5)\n</code></pre>"},{"location":"CS/Basics/6_0001/#lists","title":"Lists []","text":""},{"location":"CS/Basics/6_0001/#characteristics_1","title":"Characteristics","text":"<ul> <li> <p>Lists are mutable objects.</p> </li> <li> <p>Often used to convert list to strings and vice versa for better operation.</p> </li> </ul> <p></p> <ul> <li>If you don't want side effects of list, you should clone it.</li> </ul> <p>Side effect means if you alter a  list, all variables point to this list changed their value. <ul> <li>Don't modify a list's length when you iterate it. *Solution: Clone it first (b = a[:]) *</li> </ul>"},{"location":"CS/Basics/6_0001/#methods-for-lists","title":"Methods for lists","text":"<p>Assume L is a list.</p> <ol> <li> <p>L.append()</p> </li> <li> <p>L.extend()</p> </li> <li> <p>L.pop()</p> </li> </ol> <p>remove element at the end of list. This function's return value gonna to be the element removed. <ol> <li>L.remove(element)</li> </ol> <p>remove certain element from list L, but only remove the very first occurance of it.</p> <ol> <li>L.split(element)</li> </ol> <p></p> <ol> <li>sort() AND sorted()</li> </ol> <p>sorted(L) :arrow_right: returns sorted list, doesn't mutate L</p> <p>L.sort() \u200b:arrow_right: mutates a sorted L</p> <p>L.reverse() :arrow_right: mutates a reversed L</p>"},{"location":"CS/Basics/6_0001/#lecture-6-recursion-and-dictionaries","title":"Lecture 6 Recursion and Dictionaries","text":""},{"location":"CS/Basics/6_0001/#recursion-arrow_forward-devide-and-conquer","title":"Recursion :arrow_forward: Devide and conquer","text":"<p>Semantically, recursion means a function call itself in its definition.</p> <p>To avoid infinite recursion, you should ensure there's at least one or more base cases that are easy to solve.</p> <p>for and while loop have  a state variable tells you exactly how your loop going.</p> <p>Typecally, recursion contains two steps:</p> <ol> <li>Recurise step:</li> </ol> <p>think how to reduce problem to a simpler/smaller version of the same problem.</p> <ol> <li>Base case:</li> </ol> <p>keep resucing problem until reach a simple case that can be solved directly </p>"},{"location":"CS/Basics/6_0001/#characteristics_2","title":"Characteristics","text":"<ol> <li>More intuitive to understand than iterative loop (for, while).</li> <li>more efficient to write</li> </ol> <pre><code># Use Recursion to solve Hannoi Tower Problem\ndef Hannoi_Tower(n,fr,to,spare):\n    if n == 1:\n        print(\"Move from\", fr, \"to\", to)\n    else:\n        Hannoi_Tower(n-1, fr, spare, to)\n        Hannoi_Tower(1, fr, to, spare)\n        Hannoi_Tower(n-1, spare, to, fr)\n\nHannoi_Tower(4, \"T1\", \"T2\", \"T3\")\n</code></pre> <p>Calculate fibonacci number This recursion has more than one base case.</p> <pre><code>def Fibonacci(n):\n    \"\"\"\n    Assume n is int &gt;= 0\n    Returns fibonacci of n\n    \"\"\"\n    if n == 0 or n == 1:\n        return 1\n    else:\n        return Fibonacci(n-1)+Fibonacci(n-2)\n\n\nprint(Fibonacci(3))\nprint(Fibonacci(4))\nprint(Fibonacci(5))\n</code></pre>"},{"location":"CS/Basics/6_0001/#dictionary","title":"Dictionary {}","text":""},{"location":"CS/Basics/6_0001/#characteristics_3","title":"Characteristics","text":"<ol> <li>keys need to be unique and immutable</li> </ol> <p>Keys' data type can only be ints, floats, strings, tuples, booleans.</p> <ol> <li> <p>Can directly mutate dictionary; makes it easier to iterate.</p> </li> <li> <p>Dictionaries are not only valuable for store away data, but also valuable on procedure calls when intermediate values are not going to change.</p> </li> </ol> <p>Fibonacci with a dictionary can be very efficient.</p> <pre><code>#####################################\n# EXAMPLE: comparing fibonacci using memoization\n#####################################\n\n\ndef fib(n):\n    if n == 1:\n        return 1\n    elif n == 2:\n        return 2\n    else:\n        return fib(n-1) + fib(n-2)\n\n\ndef fib_efficient(n, d):\n    if n in d:\n        return d[n]\n    else:\n        ans = fib_efficient(n-1, d)+fib_efficient(n-2, d)\n        d[n] = ans\n        return ans\n\nd = {1:1, 2:2}\n\nargToUse = 34\n#print(\"\")\n#print('using fib')\n#print(fib(argToUse))\n#print(\"\")\n#print('using fib_efficient')\n#print(fib_efficient(argToUse, d))\n</code></pre>"},{"location":"CS/Basics/6_0001/#methods","title":"Methods","text":"<p>Assume grades is  a dictionary.</p> <ul> <li>grades.keys()</li> </ul> <p>returns dictionary keys in arbitary order.</p> <ul> <li>grades.values()</li> </ul>"},{"location":"CS/Basics/6_0001/#lecture-7-testing-debugging-exceptions-and-assertions","title":"Lecture 7 Testing, debugging, exceptions and assertions","text":""},{"location":"CS/Basics/6_0001/#testing","title":"Testing","text":"<ol> <li>Unit testing</li> <li>validate each piece of program</li> <li>testing each function separately</li> <li>Regression testing</li> <li>add test for bugs as you find them</li> <li>catch reintroduced errors that were previously fixed</li> <li>Integration testing</li> <li>does overall program work?</li> <li>tend to rush to do this</li> </ol>"},{"location":"CS/Basics/6_0001/#testing-approaches","title":"Testing approaches","text":"<ol> <li>random testing (Not Recommend)</li> <li>Black box testing</li> <li>Glass box testing</li> </ol>"},{"location":"CS/Basics/6_0001/#exceptions-and-assertions","title":"Exceptions and assertions","text":"<p>Exceptions and assertions make it easier to locate a source of bug.</p> <ul> <li> <p>Exceptions help you when you get wrong input. </p> </li> <li> <p>Assertions make you know where the program get unwanted values and terminate you program immediately.</p> </li> </ul> <pre><code>######################################\n# EXAMPLE: Raising your own exceptions\n######################################\ndef get_ratios(L1, L2):\n    \"\"\" Assumes: L1 and L2 are lists of equal length of numbers\n        Returns: a list containing L1[i]/L2[i] \"\"\"\n    ratios = []\n    for index in range(len(L1)):\n        try:\n            ratios.append(L1[index]/L2[index])\n        except ZeroDivisionError:\n            ratios.append(float('nan')) #nan = Not a Number\n        except:\n            raise ValueError('get_ratios called with bad arg')\n        else:\n            print(\"success\")\n        finally:\n            print(\"executed no matter what!\")\n    return ratios\n\nprint(get_ratios([1, 4], [2, 4]))\n</code></pre> <pre><code># avg function: version with assert\ndef avg(grades):\n    assert len(grades) != 0, 'warning: no grades data'\n    return sum(grades)/len(grades)\n\n\ntest_grades = [[['peter', 'parker'], [80.0, 70.0, 85.0]], \n              [['bruce', 'wayne'], [100.0, 80.0, 74.0]],\n              [['captain', 'america'], [80.0, 70.0, 96.0]],\n              [['deadpool'], []]]\n\nprint(get_stats(test_grades))\n</code></pre>"},{"location":"CS/Basics/6_0001/#lecture-8-object-oriented-programming","title":"Lecture 8  Object Oriented Programming","text":"<p>Object Oriented Programming makes you able to bundle this data  and bundle some internal representation and some ways to interact with a program into these packages, and with packages, you can create objects, and all objects are going to behave exact the same way (same internal representation and same way that you can interact with them).</p> <p>Object: a instance of your class that has the type, the name of your class.</p> <p>Attributes: data and procedures that \"belong\" to the class.</p> <p>Method: (procedural attributes) consider it as a function that only works with this particular type of object.</p> <pre><code>class Coordinate(object):\n    # define attributes here\n    def __init__(self, x, y)\n        \"\"\"\n        Generally you define data attributes inside this __init__\n        \"\"\"\n        self.x = x\n        self.y = y\n    def __str__(self):\n        \"\"\"\n        Python calls __str__ method when used with print on your class object\n        \"\"\"\n        return \"&lt;\"+str(self.x)+\", \"+str(self.y)+\"&gt;\"\n\n# Coordinate is a subclass of object, inherits all its attributes\n# object is a superclass of Coordinate\nc = Coordinate(3, 4)\nprint(isinstance(c, Coordinate))\n</code></pre> <p>__init__  is a special method tells Python when you first create an object of this type, call this method.</p> <p>__str__ has to return a string</p> <p>self represents a particular instance of the class. (For methods that belong to the class, the first parameter is always going to be self (by convention).)</p>"},{"location":"CS/Basics/6_0001/#advantages","title":"Advantages","text":"<ol> <li>Bundle data into packages</li> <li>devide-and-conquer development</li> <li>classes make it easier to reuse code</li> <li>Allows us to add layers of abstraction to our code</li> </ol> <p>## Lecture 9 Classes and Inheritance</p>"},{"location":"CS/Basics/6_0001/#getters-and-setters","title":"Getters and Setters","text":"<p>Getters and Setters can prevent bugs from come into play latter on someone decides to change implementation.</p> <p>getters: return  the values of any of the data attributes</p> <p>setters: going to set the data attributes to whatever is passed in</p> <p>:warning: Getters and setters should be used outside of class to access data attributes. (Reason: Information hiding)</p> <pre><code>class Animal(object):\n    def __init__(self, age):\n        self.age = age\n        self.name = None\n    def get_age(self):\n        return self.age\n    def get_name(self):\n        return self.name\n    def set_age(self, newage):\n        self.age = newage\n    def set_name(self, newname=\"\"):\n        self.name = newname\n    def __str__(self):\n        return \"animal:\"+str(self.name)+\":\"+str(self.age)\n\nprint(\"\\n---- animal tests ----\")\na = Animal(3)\n# Two ways to access data attributes\na.age\na.get_age()\n# Better to use getters and setters to access data attributes\n</code></pre>"},{"location":"CS/Basics/6_0001/#default-arguments","title":"Default Arguments","text":"<p>Default arguments are passed into functions/methods.</p>"},{"location":"CS/Basics/6_0001/#inheritance","title":"Inheritance","text":"<ul> <li>If subclass doesn't have __init__, it'll use its superclass's __init__. </li> </ul> <p>This characteristic remains true for any other methods.</p> <ul> <li>Class variables' value gonna shared between all of the instances in the class. While instance variables can have different values for each instance.</li> </ul> <p>You should access your class variables use class name. <code>self.rid = Rabbit.tag</code> Rabbit is a class name, Rabbit.tag is a class variable. </p> <pre><code>import random\n\n#################################\n## Animal abstract data type \n#################################\nclass Animal(object):\n    def __init__(self, age):\n        self.age = age\n        self.name = None\n    def get_age(self):\n        return self.age\n    def get_name(self):\n        return self.name\n    def set_age(self, newage):\n        self.age = newage\n    def set_name(self, newname=\"\"):\n        self.name = newname\n    def __str__(self):\n        return \"animal:\"+str(self.name)+\":\"+str(self.age)\n\nprint(\"\\n---- animal tests ----\")\na = Animal(4)\nprint(a)\nprint(a.get_age())\na.set_name(\"fluffy\")\nprint(a)\na.set_name()\nprint(a)\n\n\n\n#################################\n## Inheritance example \n#################################\nclass Cat(Animal):\n    def speak(self):\n        print(\"meow\")\n    def __str__(self):\n        return \"cat:\"+str(self.name)+\":\"+str(self.age)\n\nprint(\"\\n---- cat tests ----\")\nc = Cat(5)\nc.set_name(\"fluffy\")\nprint(c)\nc.speak()\nprint(c.get_age())\n#a.speak() # error because there is no speak method for Animal class\n\n\n#################################\n## Inheritance example\n#################################\nclass Person(Animal):\n    def __init__(self, name, age):\n        Animal.__init__(self, age)\n        self.set_name(name)\n        self.friends = []\n    def get_friends(self):\n        return self.friends\n    def speak(self):\n        print(\"hello\")\n    def add_friend(self, fname):\n        if fname not in self.friends:\n            self.friends.append(fname)\n    def age_diff(self, other):\n        diff = self.age - other.age\n        print(abs(diff), \"year difference\")\n    def __str__(self):\n        return \"person:\"+str(self.name)+\":\"+str(self.age)\n\nprint(\"\\n---- person tests ----\")\np1 = Person(\"jack\", 30)\np2 = Person(\"jill\", 25)\nprint(p1.get_name())\nprint(p1.get_age())\nprint(p2.get_name())\nprint(p2.get_age())\nprint(p1)\np1.speak()\np1.age_diff(p2)\n\n\n#################################\n## Inheritance example\n#################################\nclass Student(Person):\n    def __init__(self, name, age, major=None):\n        Person.__init__(self, name, age)\n        self.major = major\n    def __str__(self):\n        return \"student:\"+str(self.name)+\":\"+str(self.age)+\":\"+str(self.major)\n    def change_major(self, major):\n        self.major = major\n    def speak(self):\n        r = random.random()\n        if r &lt; 0.25:\n            print(\"i have homework\")\n        elif 0.25 &lt;= r &lt; 0.5:\n            print(\"i need sleep\")\n        elif 0.5 &lt;= r &lt; 0.75:\n            print(\"i should eat\")\n        else:\n            print(\"i am watching tv\")\n\nprint(\"\\n---- student tests ----\")\ns1 = Student('alice', 20, \"CS\")\ns2 = Student('beth', 18)\nprint(s1)\nprint(s2)\nprint(s1.get_name(),\"says:\", end=\" \")\ns1.speak()\nprint(s2.get_name(),\"says:\", end=\" \")\ns2.speak()\n\n\n\n#################################\n## Use of class variables  \n### Class variables' value gonna shared between all of the instances in the class.\n#################################\nclass Rabbit(Animal):\n    # a class variable, tag, shared across all instances\n    tag = 1\n    def __init__(self, age, parent1=None, parent2=None):\n        Animal.__init__(self, age)\n        self.parent1 = parent1\n        self.parent2 = parent2\n        self.rid = Rabbit.tag\n        Rabbit.tag += 1\n    def get_rid(self):\n        # zfill used to add leading zeroes 001 instead of 1\n        return str(self.rid).zfill(3)\n    def get_parent1(self):\n        return self.parent1\n    def get_parent2(self):\n        return self.parent2\n    def __add__(self, other):\n        # returning object of same type as this class\n        return Rabbit(0, self, other)\n    def __eq__(self, other):\n        # compare the ids of self and other's parents\n        # don't care about the order of the parents\n        # the backslash tells python I want to break up my line\n        parents_same = self.parent1.rid == other.parent1.rid \\\n                       and self.parent2.rid == other.parent2.rid\n        parents_opposite = self.parent2.rid == other.parent1.rid \\\n                           and self.parent1.rid == other.parent2.rid\n        return parents_same or parents_opposite\n    def __str__(self):\n        return \"rabbit:\"+ self.get_rid()\n\nprint(\"\\n---- rabbit tests ----\")\nprint(\"---- testing creating rabbits ----\")\nr1 = Rabbit(3)\nr2 = Rabbit(4)\nr3 = Rabbit(5)\nprint(\"r1:\", r1)\nprint(\"r2:\", r2)\nprint(\"r3:\", r3)\nprint(\"r1 parent1:\", r1.get_parent1())\nprint(\"r1 parent2:\", r1.get_parent2())\n\nprint(\"---- testing rabbit addition ----\")\nr4 = r1+r2   # r1.__add__(r2)\nprint(\"r1:\", r1)\nprint(\"r2:\", r2)\nprint(\"r4:\", r4)\nprint(\"r4 parent1:\", r4.get_parent1())\nprint(\"r4 parent2:\", r4.get_parent2())\n\nprint(\"---- testing rabbit equality ----\")\nr5 = r3+r4\nr6 = r4+r3\nprint(\"r3:\", r3)\nprint(\"r4:\", r4)\nprint(\"r5:\", r5)\nprint(\"r6:\", r6)\nprint(\"r5 parent1:\", r5.get_parent1())\nprint(\"r5 parent2:\", r5.get_parent2())\nprint(\"r6 parent1:\", r6.get_parent1())\nprint(\"r6 parent2:\", r6.get_parent2())\nprint(\"r5 and r6 have same parents?\", r5 == r6)\nprint(\"r4 and r6 have same parents?\", r4 == r6)\n</code></pre>"},{"location":"CS/Basics/6_0001/#lecture-10-understanding-programming-efficiency","title":"Lecture 10 Understanding Programming Efficiency","text":"<p>Program efficiency  refers both to space and time, but primarily to time.</p> <ul> <li>Order of growth (the big O notation)</li> </ul> <p>Two ways to understand program efficiency</p> <ul> <li>How can you reason about(predict) the time a program takes to solve a particular size problem</li> <li>How can we relate choices in algorithm design to the time efficiency of resulting algorithm</li> </ul> <p>Choices of designing algorithms</p> <ul> <li>(choice of implementation) Different ways to achieve an operation</li> </ul> <p>e.g. Using for loop or while loop <ul> <li>(choice of algorithm) Choose different algorithms</li> </ul> <p>e.g. Do it recursively or iterally"},{"location":"CS/Basics/6_0001/#how-to-evaluate-efficiency-three-ways","title":"How to evaluate efficiency (Three ways)","text":"<p>What we want to evaluate here is the algorithm, not the machine or implementations. And especially want to understand how does it scale? if you change size of  the algorithm, in which way it will affect algorithm efficiency.</p>"},{"location":"CS/Basics/6_0001/#1-set-a-timer-and-time-your-algorithm","title":"1. Set a timer and time your algorithm","text":"<p>Timer evaluate time between different algorithms, but it varies on a lot of  other factors.</p> <ol> <li> <p>varies between implementations</p> </li> <li> <p>varies between computers</p> </li> <li> <p>not predictable based on small inputs</p> </li> </ol> <p>Because for  really large sized problems will have issues like get things out of memory and bring them back in to the computer</p>"},{"location":"CS/Basics/6_0001/#2-count-the-operations","title":"2. Count the operations","text":"<ol> <li>count depends on algorithm :heavy_check_mark:</li> <li>count depends  on implementations :heavy_multiplication_x:</li> <li>count independent of computers :heavy_check_mark:</li> <li>no clear definition of which operations to count :heavy_multiplication_x:</li> <li>count varies for different inputs and can come up with a relationship between inputs and the count :heavy_check_mark:</li> </ol>"},{"location":"CS/Basics/6_0001/#3-order-of-growth-o","title":"3. Order of growth \\(O ()\\)","text":"<p>Going to look for as tight as possible an upper bound of growth as a function of the size of input in the worst case.</p> <p>Focus on what happens when the size of the problem gets arbitrarily large. And express size of efficiency in terms of size of input. We want to know what the relationship between if you increase the size of input and the runtime after you do that.</p> <p>Usually, when we talk about complexity, we gonna focus on the worst case behavior. Change in average speed doesn't mean it change the order of growth of one algorithm.</p> <p>most appropriate way of assessing the impact of choices of algorithm in solving a problem; and in measuring the inherent difficulty in solving a problem.</p> <p>Worst case asymptotic complexity</p> <ul> <li> <p>ignore additive constants</p> </li> <li> <p>ignore multiplicative constants</p> </li> </ul> <p>Law of Addition for \\(O()\\):</p> <p>Used with sequential statements $$ O(f(n)) + O(g(n)) = O(f(n)+g(n)) $$ Law of  for \\(O()\\):</p> <p>Used with nested statements/loops: $$ O(f(n)) * O(g(n)) = O(f(n)g(n)) $$ Nested loops typically(not always) have that kind of behavior. (Nested loop might be implicit. e.g. <code>ele in Alist</code>*)</p> <p>Some basic order of growth</p> <ol> <li> <p>\\(O(1)\\) independent of input problem's size</p> </li> <li> <p>\\(O(n)\\) problem reduces linearly each time. (e.g. loop)</p> </li> <li> <p>\\(O(log(n))\\) problem divides it size each time (e.g. bisection)</p> </li> <li> <p>\\(O(n\\space log(n))\\) (e.g. merge sort algorithm)</p> </li> <li> <p>\\(O(n^C)\\) [Polynomial Complexity] nested loops or recursive function calls</p> </li> <li> <p>\\(O(C ^ n)\\) [Exponential Complexity] </p> </li> </ol> <p>Two situations for exponential complexity (Use recurrence relation to calculate complexity):</p> <ol> <li> <p>recursive functions where more than one recursive call for each size of problem. e.g. Hanoi Tower</p> </li> <li> <p>Can also be buried inside of how you growth the size of the problem.  Loop grows in size each time around.</p> <pre><code>def genSubsets(L):\n    if len(L) == 0:\n        return [[]] #list of empty list\n    smaller = genSubsets(L[:-1]) # all subsets without last element\n    extra = L[-1:] # create a list of just last element\n    new = []\n    for small in smaller:\n        new.append(small+extra)  # for all smaller solutions, add one with last element\n    return smaller+new  # combine those with last element and those without\n</code></pre> <p>The for loop size (i.e. <code>len(smaller)</code>) going to be growing exponentially.  If  <code>len(L) = n</code> the loop size gonna to be \\(2^{n-1}\\) in this case. </p> </li> </ol>"},{"location":"CS/Basics/6_0001/#lecture-11-searching-and-sorting","title":"Lecture 11 Searching and Sorting","text":"<p>Exhaustive Enumeration</p> <p>Searching is to find items in a collection. That collection can be either implicit (e. g. find square roots, the collection is all the numbers between some point and some other point ) or explicit.</p>"},{"location":"CS/Basics/6_0001/#sorting-algorithms","title":"Sorting Algorithms","text":"<ol> <li>Monkey/Bogo sort [Complexity: \\(O(n^n)\\)]</li> </ol> <p>Randomly permute elements in a collection, check if the sequence of elements is sorted.</p> <ol> <li>Bubble sort [Complexity: \\(O(n^2)\\)]</li> </ol> <p>Compare elements pairwise, move the smaller on to the first of these two, continue until no more swap can be done.</p> <pre><code>def bubble_sort(L):\n    swap = False\n    while not swap:\n        print('bubble sort: ' + str(L))\n        swap = True\n        for j in range(1, len(L)):\n            if L[j-1] &gt; L[j]:\n                swap = False\n                temp = L[j]\n                L[j] = L[j-1]\n                L[j-1] = temp\n\ntestList = [1,3,5,7,2,6,25,18,13]\n\nprint('')\nprint(bubble_sort(testList))\nprint(testList)\n</code></pre> <ol> <li>Selection sort [Complexity: \\(O(n^2)\\)]</li> </ol> <p>Find the smallest element, put it in front.</p> <pre><code>def selection_sort(L):\n    suffixSt = 0\n    while suffixSt != len(L):\n        print('selection sort: ' + str(L))\n        for i in range(suffixSt, len(L)):\n            if L[i] &lt; L[suffixSt]:\n                L[suffixSt], L[i] = L[i], L[suffixSt]\n        suffixSt += 1\n\ntestList = [1,3,5,7,2,6,25,18,13]\n\nprint('')\nprint(selection_sort(testList))\nprint(testList)\n</code></pre> <ol> <li>Merge sort</li> </ol> <p>Divide list into two sublists (continue divide until length of these sublists at most 2), sort them, then compare the first element of each, take the smaller one add to result. Keep doing that until one of  the list is empty and copy the reminder of the other list.</p> <p></p> <pre><code>def merge(left, right):\n    result = []\n    i,j = 0,0\n    while i &lt; len(left) and j &lt; len(right):\n        if left[i] &lt; right[j]:\n            result.append(left[i])\n            i += 1\n        else:\n            result.append(right[j])\n            j += 1\n    while (i &lt; len(left)):\n        result.append(left[i])\n        i += 1\n    while (j &lt; len(right)):\n        result.append(right[j])\n        j += 1\n    print('merge: ' + str(left) + '&amp;' + str(right) + ' to ' +str(result))\n    return result\n\ndef merge_sort(L):\n    print('merge sort: ' + str(L))\n    if len(L) &lt; 2:\n        return L[:]\n    else:\n        middle = len(L)//2\n        left = merge_sort(L[:middle])\n        right = merge_sort(L[middle:])\n        return merge(left, right)\n\ntestList = [1,3,5,7,2,6,25,18,13]\n\n#print('')\n#print(merge_sort(testList))\n</code></pre> <p>Calculate the complexity of merge sort</p> <p></p> <p>To compute the total cost represented by the recurrence of merge sort, we simply add up the costs of all the levels. Where constant \\(c\\) represents the time required to solve problems of size 1 as well as the time per array element of the divide and combine steps. The recursion tree has \\(log{n} +1\\) levels, each costing \\(cn\\), for a total cost of \\(cn(log{n}+1)=cnlog{n}+cn\\). Ignoring the low-order term and the constant c gives the desired result of \\(O (nlog{n})\\).</p>"},{"location":"CS/Basics/6_006/","title":"MIT 6.006 Introduction to Algorithms","text":""},{"location":"CS/Basics/6_006/#quick-overview","title":"Quick Overview","text":""},{"location":"CS/Basics/6_006/#summary-of-data-structures","title":"Summary of Data Structures","text":""},{"location":"CS/Basics/6_006/#sequence-data-structures","title":"Sequence Data Structures","text":"<ul> <li> <p>If you have a  sequence, being able to push and pop at the end of a Dynamic Array (e.g. python list).</p> </li> <li> <p>If it's necessary to dynamically update order of items or insert something in the middle of your sequence, use Sequence AVL.</p> </li> </ul>"},{"location":"CS/Basics/6_006/#set-data-structures","title":"Set Data Structures","text":"<ul> <li>For intrinsic operations(find, insert, delete etc.)</li> </ul> <p>Hash table turns out to be super good if you only want to support dictionary operations.</p> <ul> <li> <p>Order-perserving operations</p> </li> <li> <p>If you want to maintain order dynamically, Set AVL is the way to go.</p> </li> <li>If you don't need it dynamic, but you still need those order operations, a Sorted Array would be good enough.</li> </ul>"},{"location":"CS/Basics/6_006/#sorting-algorithms-in-different-contexts","title":"Sorting Algorithms in different contexts","text":"<ul> <li>Insertion Sort and Selection Sort ------ Priority Queue</li> </ul>"},{"location":"CS/Basics/6_006/#graph-problems","title":"Graph Problems","text":""},{"location":"CS/Basics/6_006/#applying-graph-materials-to-a-recursive-framework","title":"Applying Graph Materials to a Recursive Framework","text":""},{"location":"CS/Basics/6_006/#contents","title":"Contents","text":"LEC # TOPICS KEY DATES PS key words Unit 1: Introduction 1 Algorithmic thinking, peak finding Problem set 1 out Asymptotic Practice &amp; 2D Peek-Finding 2 Models of computation, Python cost model, document distance Unit 2: Sorting and Trees 3 Insertion sort, merge sort<code>Sort.c</code> Problem set 1 due Problem set 2 out Asymptotic Practice of Tree&amp; Heap 4 Heaps and heap sort<code>Heap.py</code> <code>HeapSort.c</code> 5 Binary search trees, BST sort 6 AVL trees, AVL sort<code>BBST.cpp</code> Problem set 2 due 7 Counting sort, radix sort, lower bounds for sorting and searching<code>CountingSort.c</code> Problem set 3 out Augmented AVL Trees &amp; BBST Unit 3: Hashing 8 Hashing with chaining 9 Table doubling, Karp-Rabin<code>StringMatching.cpp</code><code>HashTable.cpp</code> <code>SimpleHashTable.cpp</code> Problem set 3 due Problem set 4 out Hash Table 10 Open addressing, cryptographic hashing<code>HasTableOA.cpp</code> Problem set 4 due Quiz 1 Unit 4: Numerics 11 Integer arithmetic, Karatsuba multiplication Problem set 5 out 12 Square roots, Newton's method Unit 5: Graphs 13 Breadth-first search (BFS) 14 Depth-first search (DFS), topological sorting Problem set 5 due Problem set 6 out Unit 6: Shortest Paths 15 Single-source shortest paths problem 16 Dijkstra 17 Bellman-Ford 18 Speeding up Dijkstra Problem set 6 due Quiz 2 Unit 7: Dynamic Programming 19 Memoization, subproblems, guessing, bottom-up; Fibonacci, shortest paths Problem set 7 out 20 Parent pointers; text justification, perfect-information blackjack<code>BlackJack.cpp</code> 21 String subproblems, psuedopolynomial time; parenthesization, edit distance, knapsack<code>EditDistance.cpp</code><code>LongestCommonSubsequence.cpp</code> 22 Two kinds of guessing; piano/guitar fingering, Tetris training, Super Mario Bros. Problem set 7 due Unit 8: Advanced Topics 23 Computational complexity 24 Algorithms research topics"},{"location":"CS/Basics/6_006/#dijkstra","title":"Dijkstra","text":""},{"location":"CS/Basics/6_006/#reconstruct-the-path","title":"Reconstruct the path","text":"<ul> <li>Relaxation</li> </ul> <p>d[v] : length of the current shortest path from s (source) to v.</p> <p>\\(\\delta (s, v)\\) : length of a shortest path. (whether unique or not).</p> <p>Relaxation : Continually reduce these d values down to delta values \\(\\delta (s, v)\\) </p> <p>When all the vertices are converge to its delta value (\\(\\delta (s, v)\\)), algorithm is down.</p> <ul> <li>Predecessor ---- $\\Pi(v) $</li> </ul> <p>$\\Pi(v) $ : predecessor of v in the shortest path from s</p> <p>You CAN follow the predecessor chain to reconstruct the shortest path once you've converged (All values are down to the \\(\\delta (s, v)\\)).</p>"},{"location":"CS/Basics/6_006/#dynamic-programming","title":"Dynamic Programming","text":"<p>Memoization : General way to simplify bad algorithms.</p> <pre><code>/* generic memoization dynamic program */\nmemo = {}\ndef d(subproblem):\n    if subproblem in memo:\n        return memo[subproblem]\n    base case /*And set its corraleted memo[s]*/\n    recurse via relation /*And set its corraleted memo[s]*/\n</code></pre> <p>Bottom-up DP Algorithms</p>"},{"location":"CS/Basics/6_006/#dynamic-programming-steps-srtbot","title":"Dynamic Programming Steps: SRTBOT","text":"<ol> <li> <p>Subproblem definition</p> </li> <li> <p>Describe the meaning of a subproblem in words, in terms of parameters</p> </li> <li> <p>For <code>seq S</code>, try prefixes <code>S[:i]</code> suffixes <code>S[i:]</code>, substrings <code>S[i:j]</code>,  if there's more than one sequence, take the product of those spaces.</p> </li> <li>Add subproblems &amp; constraints to \"remember state\"</li> <li> <p>Often use smaller integers than a  given integer</p> </li> <li> <p>Relate subproblem solutions recursively</p> </li> <li> <p>Topological Sort</p> </li> <li> <p>Base Cases</p> </li> <li> <p>Original Problem</p> </li> <li> <p>Time</p> </li> </ol>"},{"location":"CS/Basics/6_006/#lecture-complexity","title":"Lecture Complexity","text":""},{"location":"CS/Basics/Computer-Networking-Lecture-CS144-Stanford/","title":"Computer Networking","text":""},{"location":"CS/Basics/Computer-Networking-Lecture-CS144-Stanford/#stanford-cs144","title":"Stanford CS144","text":"<ul> <li> <p>Note from huangrt01</p> </li> <li> <p>CS144\u89c6\u9891\uff08b\u7ad9\uff09</p> </li> <li>CS144\u8bfe\u7a0b\u7f51\u7ad9\uff08\u5305\u62ecPdf\u3001Lab\uff09</li> <li>\u6211\u7684CS144 Lab\u7b14\u8bb0</li> </ul> <p>[toc]</p>"},{"location":"CS/Basics/Computer-Networking-Lecture-CS144-Stanford/#1-0-the-internet-and-ip-introduction","title":"1-0 The Internet and IP Introduction","text":"<p>internet layer: Internet Protocol, IP address, packet's path</p> <p>\u5f69\u86cb\uff1a\u4e16\u4e00\u5927\u60fa\u60fa\u76f8\u60dc </p> <p>\u7528<code>ping</code>\u548c<code>traceroute</code>\u770bIP\u5730\u5740; \u5149\u7ea42/3\u5149\u901f\uff0c8637km -&gt; RTT=86ms</p>"},{"location":"CS/Basics/Computer-Networking-Lecture-CS144-Stanford/#1-1-a-day-in-the-life-of-an-application","title":"1-1 A day in the life of an application","text":"<ul> <li>Networked Applications: connectivity, bidirectional and reliable data stream</li> <li>Byte Stream Model: A - Internet - B, server\u548cA\u3001B\u5747\u53ef\u4e2d\u65ad\u8fde\u63a5</li> <li>World Wide Web (HTTP: HyperText Transfer Protocol)</li> <li>request: GET, PUT, DELETE, INFO, 400 (bad request) </li> <li>GET - response(200, OK)  , 200\u4ee3\u8868\u6709\u6548</li> <li>document-centric: \"GET/HTTP/1.1\", \"HTTP/1.1 200 OK \\&lt;contents of the index.html&gt;\"</li> <li>BitTorrent: peer-to-peer model</li> <li>breaks files into \"pieces\" and the clients join and leave \"swarms\" of clients</li> <li>\u5148\u4e0b\u8f7d torrent file -- tracker \u5b58\u50a8 lists of other clients</li> <li>dynamically exchange data</li> <li>Skype: proprietary system, a mixed system</li> <li> <p>two clients\uff1a A -- (Internet + Rendezvous server) -- NAT -- B</p> </li> <li> <p>NAT(Network Address Translator): \u8fde\u63a5\u7684\u5355\u5411\u6027\uff0c\u4f7f\u5f97A\u53ea\u80fd\u901a\u8fc7Rendezvous server\u8be2\u95eeB\u662f\u5426\u76f4\u8fdeA =&gt;reverse connection</p> </li> <li> <p>Rendezvous server</p> </li> <li> <p>\u5982\u679c\u6a21\u5f0f\u662fA -- NAT-- (Internet + Rendezvous server) -- NAT -- B\uff0cSkype\u7528Relay\u6765\u95f4\u63a5\u4f20\u9012\u4fe1\u606f</p> </li> </ul> <p></p>"},{"location":"CS/Basics/Computer-Networking-Lecture-CS144-Stanford/#1-2-the-four-layer-internet-model","title":"1-2 The four layer Internet model","text":"<p>4 layer: \u5229\u4e8ereuse</p> <p>Internet: end-hosts, links and routers * Link Layer: \u5229\u7528 link \u5728 end host\u548crouter \u6216 router\u548crouter\u4e4b\u95f4 \u4f20\u8f93\u6570\u636e, hop-by-hop\u9010\u8df3\u8f6c\u53d1   * e.g. Ethernet and WiFi * Network Layer: datagrams, Packet: (Data, Header(from, to))   * packets\u53ef\u80fd\u5931\u53bb/\u635f\u574f/\u590d\u5236\uff0cno guarantees   * must use the IP   * may be out of order * Transport Layer: TCP(Transmission Control Protocol) \u8d1f\u8d23\u4e0a\u8ff0Network\u5c42\u7684\u5c40\u9650\u6027\uff0ccontrols congestion   * sequence number -&gt; \u4fdd\u5e8f   * ACK(acknowledgement of receipt)\uff0c\u5982\u679c\u53d1\u4fe1\u4eba\u6ca1\u6536\u5230\u5c31resend   * \u6bd4\u5982\u89c6\u9891\u4f20\u8f93\u4e0d\u9700\u8981TCP\uff0c\u53ef\u4ee5\u7528UDP(User Datagram Protocol),\u4e0d\u4fdd\u8bc1\u4f20\u8f93 * Application Layer</p> <p>two extra things * IP is the \"thin waist\"   ,\u8fd9\u4e00\u5c42\u7684\u9009\u62e9\u6700\u5c11 * the 7-layer OSI Model</p> <p></p>"},{"location":"CS/Basics/Computer-Networking-Lecture-CS144-Stanford/#1-3-the-ip-service","title":"1-3 The IP Service","text":"<ul> <li>Link Frame (IP Datagram(IP Data(Data, Hdr), IP Hdr), Link Hdr )</li> <li>The IP Service Model\u7684\u7279\u70b9</li> <li>Datagram: (Data, IP SA, IP DA)\uff0c\u6bcf\u4e2a router \u6709 forwarding table\uff0c\u7c7b\u6bd4\u4e3a postal service \u4e2d\u7684 letter</li> <li>Unreliable: \u5931\u53bb/\u635f\u574f/\u590d\u5236\uff0c\u4fdd\u8bc1\u53ea\u5728\u5fc5\u8981\u7684\u65f6\u5019\u4e0d\u53ef\u9760\uff08\u6bd4\u5982queue congestion\uff09</li> <li>Best-effort attempt</li> <li>Connectionless : no per-flow state, mis-sequenced</li> <li>IP\u8bbe\u8ba1\u7b80\u5355\u7684\u539f\u56e0</li> <li>minimal, faster, streamlined</li> <li>end-to-end (\u5728end points implement features)</li> <li>build a variety of reliable/unreliable services on top</li> <li> <p>works over any link layer</p> </li> <li> <p>the IP Service Model</p> </li> <li>tries to prevent packets looping forever (\u5b9e\u73b0\uff1a\u5728\u6bcf\u4e2adatagram\u7684header\u52a0hop-count field: time to live TTL field, \u6bd4\u5982\u4ece128\u5f00\u59cbdecrement)</li> <li>will fragment packets if they're too long (e.g. Ethernet, 1500bytes)</li> <li>header checksum\uff1a\u589e\u5f3a\u53ef\u9760\u6027</li> <li>allows for new versions of IP</li> <li>allows for new options to be added to header (\u7531router\u5904\u7406\u65b0\u7279\u6027\uff0c\u614e\u91cd\u4f7f\u7528)</li> </ul>"},{"location":"CS/Basics/Computer-Networking-Lecture-CS144-Stanford/#1-4-a-day-in-the-life-of-a-packet","title":"1-4 A Day in the Life of a Packet","text":"<ul> <li>3-way handshake</li> <li>client: SYN </li> <li>server: SYN/ACK</li> <li>client: ACK</li> <li>IP packets</li> <li>IP address + TCP port (web server\u901a\u5e38\u662f80)</li> <li>hops, Routers: wireless access point (WiFi\u7684\u7b2c\u4e00\u6b21hop)</li> <li>forwarding table</li> <li>default router</li> </ul>"},{"location":"CS/Basics/Computer-Networking-Lecture-CS144-Stanford/#1-5-principle-packet-switching-principle","title":"1-5 Principle: Packet switching principle","text":"<p>packet: self-contained</p> <p>packet switching: independently for each arriving packet, pick its outgoing link. If the link is free, send it. Else hold the packet for later.</p> <p>source packet: (Data, (dest, C, B, A))  \u53d1\u5c55\u6210\u53ea\u5b58destination\uff0c\u6bcf\u4e2aswitch\u6709table</p> <p>two consequences * simple packet forwarding: No per-flow state required\uff0cstate\u4e0d\u9700\u8981store/add/remove * efficient sharing of links: busty data traffic; statistical multiplexing =&gt; \u5bf9packet\u4e00\u89c6\u540c\u4ec1\uff0c\u53ef\u5171\u4eablinks</p>"},{"location":"CS/Basics/Computer-Networking-Lecture-CS144-Stanford/#1-6-principle-layering","title":"1-6 Principle: Layering","text":"<ul> <li>\u4e00\u79cd\u8bbe\u8ba1\u7406\u5ff5\uff0clayers are functional components, they communicate sequentially </li> <li> <p>edit -&gt; compile -&gt; link -&gt; execute</p> </li> <li> <p>compiler: self-contained, e.g. lexical analysis, parsing the code, preprocessing declarations, code generation and optimization</p> </li> <li>\u6709\u65f6\u9700\u8981 break layering</li> <li>\u6bd4\u5982Linux\u5185\u6838\u7684\u90e8\u5206\u4ee3\u7801C\u8bed\u8a00\u76f4\u63a5\u7528\u6c47\u7f16 =&gt; code\u4e0d\u518dlayer-independent</li> <li>a continual tension to improve the Internet by making cross-layer optimizations and the resulting loss of flexibility. e.g. NATs=&gt;\u5f88\u96be\u52a0\u5176\u5b83\u7c7b\u578b\u7684\u4f20\u8f93\u5c42</li> <li>epoll\u8fd9\u4e2a\u63a5\u53e3\u662flinux\u72ec\u6709\u7684\uff0cFreeBSD\u91cc\u662fkqueue</li> <li> <p>UDP header \u7684 checksum \u8ba1\u7b97\u7528\u5230 IP header</p> </li> <li> <p>layering\u7684\u539f\u56e0\uff1a1.modularity 2.well defined service 3.reuse 4.separation of concerns 5.continuous improvement 6.p2p communications</p> </li> </ul>"},{"location":"CS/Basics/Computer-Networking-Lecture-CS144-Stanford/#1-7-principle-encapsulation","title":"1-7 Principle: Encapsulation","text":"<ul> <li> <p>TCP segment is the payload of the IP packet. IP packet encapsulates the TCP segment.</p> </li> <li> <p>\u4e00\u5c42\u5c42\uff0c\u5957footer\u548cheader</p> <ul> <li>\u4e24\u79cd\u5199\u6cd5\uff0c\u5e95\u5c42\u7684\u5199\u6cd5(switch design)header\u5728\u53f3\u8fb9\uff0csoftware\u7684\u5199\u6cd5(protocol)header\u5728\u5de6\u8fb9\uff08IETF\uff09</li> <li>VPN: (Eth, (IP, (TCP, (TLS, IP Packet))))\uff0c\u5916\u5c42\u7684TCP\u6307\u5411VPN gateway</li> </ul> </li> </ul>"},{"location":"CS/Basics/Computer-Networking-Lecture-CS144-Stanford/#1-8-byte-order","title":"1-8 Byte Order","text":"<ul> <li>2^32 ~ 4GB ~  0x0100000000</li> <li>1024=0x0400   \u5927\u7aef\uff1a0x04 0x00\uff1b\u5c0f\u7aef: 0x00 0x04. </li> <li>Little endian: x86, big endian: ARM, network byte order</li> <li>e.g. <code>uint16_t http_port=80; if(packet-&gt;port==http_port){...}</code> IPv4\u7684packet_length\u6ce8\u610f\u5927\u5c0f\u7aef</li> <li>\u51fd\u6570\uff1a<code>htons(),ntohs(),htonl(),ntohl()</code></li> <li>host/network, short/long</li> <li><code>#include&lt;arpa/inet.h&gt;</code></li> </ul>"},{"location":"CS/Basics/Computer-Networking-Lecture-CS144-Stanford/#1-9-ipv4-addresses","title":"1-9 IPv4 addresses","text":"<p>goal: * stitch many different networks together * need network-independent, unique address</p> <p>IPv4: * layer 3 address * 4 octets  a.b.c.d * \u5b50\u7f51\u63a9\u7801netmask: 255.128.0.0 \u524d9\u4f4d\uff0c1\u8d8a\u5c11\u7f51\u7edc\u8d8a\u5927\uff0csame network\u4e0d\u9700\u8981\u8def\u7531\uff0c\u76f4\u63a5link\u5373\u53ef</p> <p></p> <p>IPv4 Datagram * Total Packet Length: \u5927\u7aef\uff0c\u6700\u591a65535bytes, 1400 -&gt; 0x0578 * Protocol ID: 6-&gt;TCP</p> <p>Address Structure * network+host * class A,B,C: 0,7+24; 10, 14+16; 110, 21+8</p> <p>Classless Inter-Domain Routing (CIDR\uff0c\u65e0\u7c7b\u522b\u57df\u95f4\u8def\u7531) * address block is a pair: address, count * counts\u662f2\u7684\u6b21\u65b9? \u8868\u793anetmask\u957f\u5ea6 * e.g. Stanford 5/16 blocks <code>5*2^(32-16)</code> * \u524d\u7f00\u805a\u5408\uff0c\u9632\u6b62\u8def\u7531\u8868\u7206\u70b8 * IANA(Internet Assigned Numbers Authority): give /8s to RIRs</p>"},{"location":"CS/Basics/Computer-Networking-Lecture-CS144-Stanford/#1-10-longest-prefix-matchlpm","title":"1-10 Longest Prefix Match(LPM)","text":"<p>forwarding table: CIDR entries * LPM\u7684\u524d\u63d0\u662f\u5fc5\u987b\u5148match\uff0c\u518d\u770bprefix * default: 0.0.0.0/0</p>"},{"location":"CS/Basics/Computer-Networking-Lecture-CS144-Stanford/#1-11-address-resolution-protocolarp","title":"1-11 Address Resolution Protocol(ARP)","text":"<p>IP address(host) -&gt; link address(Ethernet card, 48bits)</p> <p>Addressing Problem: \u4e00\u4e2ahost\u5bf9\u5e94\u591a\u4e2aIP\u5730\u5740\uff0c\u4e0d\u5bb9\u6613\u5bf9\u5e94 * \u89e3\u51b3\u65b9\u6848\uff1agateway\u4e24\u4fa7ip\u5730\u5740\u4e0d\u540c\uff0clink address\u786e\u5b9acard\uff0cnetwork address\u786e\u5b9ahost * \u8fd9\u6709\u70b9\u5386\u53f2\u9057\u7559\u95ee\u9898\uff0cip\u548clink address\u7684\u673a\u5236\u6ca1\u6709\u5b8c\u5168\u5730\u5206\u79bb\u5f00\uff0cdecoupled logically but coupled in practice * \u5bf9\u4e8eA\uff0cip\u7684\u76ee\u6807\u662fB\uff0clink\u7684\u76ee\u6807\u662fgateway</p> <p>ARP\uff0c\u5730\u5740\u89e3\u6790\u534f\u8bae\uff1a\u7531IP\u5f97\u5230MAC\u5730\u5740 =&gt; \u8fdb\u4e00\u6b65\u53ef\u5f97\u5230gateway address * \u662f\u4e00\u79cdrequest-reply protocol * nodes cache mappings, cache entries expire * \u8282\u70b9request a link layer broadcast address\uff0c\u7136\u540e\u6536\u5230\u56de\u590d\uff0c\u56de\u590d\u7684packet\u6709redundant data\uff0c\u770b\u5230\u5b83\u7684\u8282\u70b9\u90fd\u80fd\u751f\u6210mapping * reply\uff1a\u539f\u5219\u4e0aunicast\uff0c\u53ea\u56de\u4f20\u7ed9\u53d1\u9001\u8005=&gt;\u5b9e\u9645\u5b9e\u73b0\u65f6\u66f4\u5e38\u89c1broadcast * No \"sharing\" of state: bad state will die eventually * MacOS\u4e2d\u4fdd\u755920min * gratuitous request: \u8981\u6c42\u4e0d\u5b58\u5728\u7684mapping\uff0c\u63a8\u9500\u81ea\u5df1</p> <p></p> <p>e.g. </p> <p>hardware:1(Ethernet)</p> <p>protocol: 0x0800(IP)</p> <p>hardware length:6 (48 bit Ethernet)</p> <p>protocol length:4(32 bit IP)</p> <p>opcode: 1(request) /2(reply)</p> <p>Destination: broadcast (ff:ff:ff:ff:ff:ff)</p>"},{"location":"CS/Basics/Computer-Networking-Lecture-CS144-Stanford/#1-12-recap","title":"1-12 recap","text":""},{"location":"CS/Basics/Computer-Networking-Lecture-CS144-Stanford/#1-13-sip-jon-peterson-interview","title":"1-13 SIP, Jon Peterson Interview","text":"<p>the intersection between technology and public policy * IETF ( The Internet Engineering Task Force) * ICANN\uff08The Internet Corporation for Assigned Names and Numbers\uff09</p> <p>SIP\uff08Session Initiation Protocol\uff0c\u4f1a\u8bdd\u521d\u59cb\u534f\u8bae\uff09 * end-to-end\u7684\u8bbe\u8ba1 * soft switching: \u5c06\u547c\u53eb\u63a7\u5236\u529f\u80fd\u4ece\u4f20\u8f93\u5c42\u5206\u79bb * PSTN ( Public Switched Telephone Network ) -&gt; VOIP(Voice over Internet Protocol): telephony replacement</p> <p>SIP\u7684\u5e94\u7528\u573a\u666f * Skype\u5185\u90e8\u534f\u8bae\u8f6c\u6362\u6210SIP * VOIP, FiOS( a telecom service offered over fiber-optic lines)</p> <p>\u73b0\u4ee3\u6280\u672f * SDN (Software\u2002Defined\u2002Network) * I2RS(interface to the routing system) * CDN(Content Delivery Network): 1.express coverage areas 2.advertise services that they provide, in order to allow collaboration or peering among CDNs =&gt; optimal selections of CDNs * \u8bc6\u522brobo calling</p>"},{"location":"CS/Basics/Computer-Networking-Lecture-CS144-Stanford/#2-0-transport-intro","title":"2-0 Transport (intro)","text":"<ul> <li>\u5173\u6ce8TCP\u7684correctness</li> <li>detect errors\u7684\u4e09\u4e2a\u7b97\u6cd5\uff1achecksums, cyclic redundancy checks, message authentication codes</li> <li>TCP(Transmission Control Protocol)\u3001UDP(User Datagram Protocol)\u3001ICMP(Internet Control Message Protocol)</li> </ul>"},{"location":"CS/Basics/Computer-Networking-Lecture-CS144-Stanford/#2-1-the-tcp-service-model","title":"2-1 The TCP Service Model","text":"<p>The TCP Service Model</p> <ul> <li>reliable, end-to-end, bi-directional, in-sequence, bytestream service</li> <li>Positive acknowledgement with retransmission</li> <li>Peer TCP layers communicate: connection</li> <li> <p>\u4f20\u8f93\u5c42\u65b9\u9762\uff0c\u7531\u4e8e\u94fe\u8def\u5c42\u5e26\u5bbd\u5927\u589e\uff0cTCP window scale option \u88ab\u666e\u904d\u4f7f\u7528\uff0c\u53e6\u5916 TCP timestamps option \u548c TCP selective ack option \u4e5f\u5f88\u5e38\u7528</p> </li> <li> <p>Flow control using sliding window (\u5305\u62ec Nagle \u7b97\u6cd5\u7b49)</p> </li> <li>\u63d0\u9ad8\u541e\u5410\u91cf\uff0c\u5145\u5206\u5229\u7528\u94fe\u8def\u5c42\u5e26\u5bbd</li> <li>tcp connection\u4e92\u4e0d\u611f\u77e5\uff0c\u7f3a\u5c11\u5bf9\u7f51\u5361\u5e26\u5bbd\u7684\u7edf\u7b79\u5b89\u6392</li> <li> <p>\u539f\u6765\u8bbe\u8ba1 TCP \u7684\u65f6\u5019\uff0c\u4eba\u4eec\u8ba4\u4e3a\u4e22\u5305\u901a\u5e38\u662f\u62e5\u585e\u9020\u6210\u7684\uff0c\u8fd9\u65f6\u5e94\u8be5\u653e\u6162\u53d1\u9001\u901f\u5ea6\uff0c\u51cf\u8f7b\u62e5\u585e\uff1b\u65e0\u7ebf\u7f51\u4e2d\uff0c\u4e22\u5305\u53ef\u80fd\u662f\u4fe1\u53f7\u592a\u5f31\u9020\u6210\u7684\uff0c\u8fd9\u65f6\u53cd\u800c\u5e94\u8be5\u5feb\u901f\u91cd\u8bd5\uff0c\u4ee5\u4fdd\u8bc1\u6027\u80fd</p> </li> <li> <p>congestion control</p> </li> <li>\u9632\u6b62\u8fc7\u8f7d\u9020\u6210\u4e22\u5305</li> <li>\u5305\u62ec slow start\u3001congestion avoidance\u3001fast retransmit \u7b49</li> </ul> <p>\u8fc7\u7a0b\uff1a\u4e09\u6b21\u63e1\u624b\u548c\u56db\u6b21\u6325\u624b\uff08\u53c2\u80032-6\u7684\u72b6\u6001\u8f6c\u79fb\u56fe\u7406\u89e3\uff09</p> <p>Techniques to manufacture reliability</p> <p>Remedies * Sequence numbers: detect missing data * Acknowledgments: correct delivery   * Acknowledgment (from receiver to sender)   * Timer and timeout (at sender)   * Retransmission (by sender) * Checksums/MACs: detect corrupted data   * Header checksum (IP)   * Data checksum (UDP) * Window-based Flow-control: prevents overrunning receiver * Forward error correction (FEC) * Retransmission * Heartbeats</p> <p>Correlated failure</p> <p>TCP/DNS</p> <p>Paradox of airplanes</p> <p>The TCP Segment Format </p> <ul> <li>IANA port number: ssh 22, smtp 23, web 80</li> <li>source port: \u521d\u59cb\u5316\u7528\u4e0d\u540c\u7684port\u907f\u514d\u51b2\u7a81</li> <li>Flags</li> <li>PSH flag: push\uff0c\u6bd4\u5982\u952e\u76d8\u6572\u51fb</li> <li>URG\u5e94\u8be5\u5728ACK\u524d\u9762</li> <li>HLEN \u548c (TCP options) \u8054\u7cfb</li> </ul> <p> \u4e94\u4e2a\u90e8\u5206\uff0c104bit</p> <p>\u552f\u4e00\u6027 * \u8981\u6c42source port initiator\u6bcf\u6b21increment: 64k new connections * TCP picks ISN to avoid overlap with previous connection with same ID, \u591a\u4e00\u4e2a\u57df\uff0c\u589e\u52a0\u968f\u673a\u6027 * ISN\u7684\u610f\u4e49\u5728\u4e8e\uff1a1\uff09security\uff0c\u907f\u514d\u81ea\u5df1\u7684window\u88aboverlap 2\uff09\u4fbf\u4e8efilter out\u4e0d\u540c\u7c7b\u578b\u7684\u5305  </p>"},{"location":"CS/Basics/Computer-Networking-Lecture-CS144-Stanford/#2-2-udp-service-model","title":"2-2 UDP service model","text":"<p>\u4e0d\u9700\u8981\u53ef\u9760\u6027\uff1aapp\u81ea\u5df1\u63a7\u5236\u91cd\u4f20\uff0c\u6bd4\u5982\u65e9\u671f\u7248\u672c\u7684NFS (network file system)</p> <p> * Checksum \u5bf9\u4e8e IPv4 \u53ef\u9009\uff0c\u53ef\u4ee5\u4e3a\u51680 * Checksum \u7528\u4e86 IP header\uff0c\u8fdd\u80cc layering principle\uff0c\u662f\u4e3a\u4e86\u80fddetect\u9519\u4f20 * UDP header \u6709 length \u5b57\u6bb5\uff0c\u800cTCP\u6ca1\u6709\uff0c\u56e0\u4e3aTCP\u5bf9\u7a7a\u95f4\u8981\u6c42\u9ad8\uff0c\u7528\u9690\u542b\u7684\u65b9\u5f0f\u8ba1\u7b97 length * port demultiplexing, connectionless, unreliable</p> <p>\u5e94\u7528</p> <p>DNS: domain name system\uff0c\u56e0\u4e3arequest\u5168\u5728\u5355\u4e2adatagram\u91cc</p> <p>DHCP: Dynamic Host Configuration Protocol * new host\u5728join\u7f51\u7edc\u65f6\u5f97\u5230IP * \u8fdeWiFi</p> <p>\u5bf9\u91cd\u4f20\u3001\u62e5\u585e\u63a7\u5236\u3001in-sequence delivery \u6709 special needs \u7684\u5e94\u7528\uff0c\u6bd4\u5982\u97f3\u9891\uff0c\u4f46\u73b0\u5728UDP\u4e0d\u50cf\u4ee5\u524d\u7528\u7684\u90a3\u4e48\u591a\uff0c\u56e0\u4e3a\u5f88\u591a\u662fhttp\uff0c\u57fa\u4e8eTCP\u3002</p>"},{"location":"CS/Basics/Computer-Networking-Lecture-CS144-Stanford/#2-3-the-internet-control-message-protocol-icmp-service-model","title":"2-3 The Internet Control Message Protocol (ICMP) Service Model","text":"<p>report errors and diagnoise problems about network layer</p> <p>\u7f51\u7edc\u5c42work\u7684\u4e09\u4e2a\u56e0\u7d20\uff1aIP\u3001Routing Tables\u3001ICMP</p> <p></p> <p>Message\u7684\u610f\u4e49\u89c1RFC 792</p> <p>\u5e94\u7528\u4e8eping\uff1a\u5148\u53d1\u90018 0( echo request)\uff0c\u518d\u9001\u56de0 0(echo reply)</p> <p>\u5e94\u7528\u4e8etraceroute:  * \u6838\u5fc3\u601d\u60f3\uff1a\u8fde\u7eed\u53d1\u9001TTL\u4ece1\u5f00\u59cb\u9012\u589e\u7684UDP\uff0c\u671f\u5f85\u56de\u590d\u768411 0(TTL expires)   * Source is random and different for each; destination starts with a random number and increases by one for each * \u7531\u4e8e\u8def\u7531\u9009\u62e9\u95ee\u9898\uff0ctraceroute \u65e0\u6cd5\u4fdd\u8bc1\u6bcf\u6b21\u5230\u540c\u4e00\u4e2a\u4e3b\u673a\u7ecf\u8fc7\u7684\u8def\u7531\u90fd\u662f\u76f8\u540c\u7684\u3002 * traceroute \u53d1\u9001\u7684 UDP \u6570\u636e\u62a5\u7aef\u53e3\u53f7\u662f\u5927\u4e8e 30000 \u7684\u3002\u5982\u679c\u76ee\u7684\u4e3b\u673a\u6ca1\u6709\u4efb\u4f55\u7a0b\u5e8f\u4f7f\u7528\u8be5\u7aef\u53e3\uff0c\u4e3b\u673a\u4f1a\u4ea7\u751f\u4e00\u4e2a 3 3(\u7aef\u53e3\u4e0d\u53ef\u8fbe) ICMP\u62a5\u6587\u7ed9\u6e90\u4e3b\u673a\u3002</p>"},{"location":"CS/Basics/Computer-Networking-Lecture-CS144-Stanford/#2-4-end-to-end-principle","title":"2-4 End-to-End Principle","text":"<p>Why Doesn't the Network Help? * e.g.\uff1a\u538b\u7f29\u6570\u636e\u3001Reformat/translate/improve requests\u3001serve cached data\u3001add security\u3001migrate connections across the network * end-to-end principle: function\u7684\u6b63\u786e\u5b8c\u6574\u5b9e\u73b0\u53ea\u4f9d\u8d56\u4e8e\u901a\u4fe1\u7cfb\u7edf\u7684end points</p> <p>end-to-end check  * e.g. File Transfer: link layer\u7684error detection\u53ea\u68c0\u6d4btransmission\u9519\u8bef\uff0c\u4e0d\u68c0\u6d4berror storage * e.g. TCP\u5c0f\u6982\u7387\u4f1a\u51fa\u9519\uff08stack\uff09\u3001BitTorrent * wireless link\u76f8\u6bd4wire link\u529f\u80fd\u590d\u6742\uff0c\u53ef\u9760\u6027\u4f4e\uff0c\u6240\u4ee5\u5728link layer\u91cd\u4f20\uff0c\u53ef\u63d0\u5347TCP\u6027\u80fd * RFC1958: \"strong\" end to end: \u4e0d\u63a8\u8350\u5728 middle \u5b9e\u73b0\u4efb\u4f55\u529f\u80fd\uff0c\u6bd4\u5982\u5728 link layer \u91cd\u4f20\uff0c\u5047\u5b9a\u4e86reliabilty\u7684\u63d0\u5347\u503c\u5f97latency\u7684\u727a\u7272</p>"},{"location":"CS/Basics/Computer-Networking-Lecture-CS144-Stanford/#2-5-error-detection-3-schemes-3-schemes","title":"2-5 Error Detection: 3 schemes: 3 schemes","text":"<ul> <li>detect errors\u7684\u4e09\u4e2a\u7b97\u6cd5\uff1achecksums, CRC(cyclic redundancy checks), MAC(message authentication codes)</li> <li>\u589e\u8865\u65b9\u5f0f</li> <li>append: ethernet CRC, TLS MAC</li> <li>prepend: IP checksum</li> <li>Checksum (IP, TCP)</li> <li>not very robust, \u53ea\u80fd\u68c01\u4f4d\u9519</li> <li>fast and cheap even in software</li> <li>IP, UDP, TCP use one's complement\u7b97\u6cd5\uff1a16-bit word packet\u6c42\u548c\uff0c\u8fdb\u4f4d\u52a0\u5230\u5e95\u90e8\uff0c\u518d\u53d6\u53cd\u7801\uff08\u7279\u4f8b\uff1a0xffff -&gt; 0xffff\uff0c\u56e0\u4e3a\u5728TCP\uff0cchecksum field \u4e3a 0 \u610f\u5473\u7740\u6ca1\u6709 checksum\uff09</li> <li>CRC: computes remainder of a polynomial (Ethernet)\uff0c\u89c1\u901a\u4fe1\u4e0e\u7f51\u7edc\u7b14\u8bb0</li> <li>\u901a\u5e38\u662f\u7531\u7f51\u5361\u786c\u4ef6\u5b8c\u6210\u7684\uff0c\u5728\u53d1\u5305\u7684\u65f6\u5019\u7531\u786c\u4ef6\u586b\u5145 CRC\uff0c\u5728\u6536\u5305\u7684\u65f6\u5019\u7f51\u5361\u81ea\u52a8\u4e22\u5f03 CRC \u4e0d\u5408\u683c\u7684\u5305</li> <li>\u867d\u7136more expensive\uff0c\u4f46\u652f\u6301\u786c\u4ef6\u8ba1\u7b97</li> <li>\u53ef\u5bf9\u62972 bits error\u3001\u5947\u6570error\u3001\u5c0f\u4e8ec bits\u7684\u7a81\u53d1\u9519(burst)</li> <li>\u53efincrementally\u8ba1\u7b97</li> <li>e.g. USB(CRC-16):   \uff0c\u5bf9\u4e8egenerator\u9700\u8981\u7ed9\u5de6\u8fb9pad 1</li> <li>MAC: message authentication code: cryptographic transformation of data(TLS)</li> <li>robust to malicious modifications, but not errors</li> <li>\u68c0\u9519\u80fd\u529b\u6709\u5c40\u9650\uff0c\u53d7\u968f\u673a\u6027\u5f71\u54cd\uff0c\u4e0d\u5982CRC\uff0cno error detection guarantee</li> <li> \uff0cM + c\u610f\u5473\u7740\u5bf9\u65b9\u6709secret\u6216\u8005replay</li> <li>\u5bf9\u4e8ereplay\uff0c<code>ctr++</code>, \u5177\u4f53\u89c1\u6211\u7684\u5bc6\u7801\u5b66\u7b14\u8bb0\u7684TLS\u90e8\u5206\u3010\u76ee\u524d\u5c1a\u672a\u6574\u7406\u3011</li> </ul>"},{"location":"CS/Basics/Computer-Networking-Lecture-CS144-Stanford/#2-6-finite-state-machines","title":"2-6 Finite State Machines","text":"<ul> <li>\u975e\u5e38\u89c4\u8def\u7ebf\u7684\u5904\u7406\uff1a\u6bd4\u5982\u5bf9\u4e8e\u7b2c\u4e8c\u4e2aSYN\u6216\u8005FIN\u4fe1\u53f7\uff0c\u63a5\u6536\u673a\u9009\u62e9\u5ffd\u89c6\uff0c\u5177\u4f53\u89c1<code>bool TCPReceiver::segment_received(const TCPSegment &amp;seg)</code>\u7684\u5b9e\u73b0</li> </ul>"},{"location":"CS/Basics/Computer-Networking-Lecture-CS144-Stanford/#2-7-flow-control-i-stop-and-wait","title":"2-7 Flow Control I: Stop-and-Wait","text":"<ul> <li>\u6838\u5fc3\u662f receiver \u7ed9 sender \u53cd\u9988\uff0c\u8ba9sender\u4e0d\u8981\u9001\u592a\u591a packets</li> <li>\u57fa\u672c\u65b9\u6cd5</li> <li>\u65b9\u6848\u4e00\uff1astop and wait</li> <li>\u65b9\u6848\u4e8c\uff1asliding window</li> </ul> <p>stop and wait * flight \u4e2d\u6700\u591a\u4e00\u4e2a packet * \u9488\u5bf9 ACK Delay\uff08\u6536\u5230ACK\u7684\u65f6\u95f4\u521a\u597d\u5728timeout\u4e4b\u540e\uff09\u7684\u60c5\u5f62\uff0c\u4f1a\u6709duplicates   * \u89e3\u51b3\u65b9\u6848\uff1a\u7528\u4e00\u4e2a1-bit counter \u63d0\u4f9b\u4fe1\u606f   * assumptions\uff1a1\uff09\u7f51\u7edc\u4e0d\u4ea7\u751f\u91cd\u590dpackets\uff1b2\uff09\u4e0ddelay multiple timeouts</p> <p></p>"},{"location":"CS/Basics/Computer-Networking-Lecture-CS144-Stanford/#2-8-flow-control-ii-sliding-window","title":"2-8 Flow Control II: Sliding Window","text":"<ul> <li>Stop-and-Wait\u7684\u6027\u80fd\uff1aRTT=50ms, Bottleneck=10Mbps, Ethernet packet length=12Kb =&gt; \u6027\u80fd(2%)\u8fdc\u8fdc\u4e0d\u5230\u74f6\u9888</li> <li>Sliding Window\u8ba1\u7b97Window size\u586b\u6ee1\u6027\u80fd</li> </ul> <p>Sliding Window Sender</p> <ul> <li>Every segment has a sequence number (SeqNo)</li> <li>Maintain 3 variables</li> <li>Send window size(SWS)</li> <li>Last acknowledgment(LAR)</li> <li>Last segment sent(LSS)</li> <li>Maintain invariant:   </li> <li>Advance LAR on new acknowledgement </li> <li>Buffer up to SWS segments</li> </ul> <p>Sliding Window Receiver * Maintain 3 variables   * Receive window size(RWS)   * Last acceptable segment(LAS)   * Last segment received(LSR) * Maintain invariant:    * \u5982\u679c\u6536\u5230\u7684packet\u6bd4LAS\u5c0f\uff0c\u5219\u53d1\u9001ack   * \u53d1\u9001cumulative acks: \u6536\u52301, 2, 3, 5\uff0c\u53d1\u90013   * TCP acks are next expected data\uff0c\u56e0\u6b64\u8981\u52a0\u4e00\uff0c\u4e0a\u4e2a\u4f8b\u5b50\u6539\u4e3a4\uff0c\u521d\u503c\u4e3a0</p> <p>RWS, SWS, and Sequence Space *    * if   , \"go back N\" protocol ,need SWS+1 sequence numbers (\u9700\u8981\u591a\u91cd\u4f20) * if   , need 2SWS sequence numbers * \u901a\u5e38\u9700\u8981   sequence numbers\uff1a\u8003\u8651\u4e34\u754c\u60c5\u51b5\uff0cSWS\u6700\u5de6\u4fa7\u7684ACK\u6ca1\u6709\u6210\u529f\u53d1\u9001\uff0c\u91cd\u4f20\u540e\u6536\u5230\u4e86RWS\u6700\u53f3\u4fa7\u7684ACK</p> <p>TCP Flow Control</p> <ul> <li>Receiver advertises RWS using window field</li> <li>Sender can only send data up to LAR+SWS</li> </ul>"},{"location":"CS/Basics/Computer-Networking-Lecture-CS144-Stanford/#2-9-retransmission-strategies","title":"2-9 Retransmission Strategies","text":"<p>protocol\u53ef\u80fd\u7684\u8fd0\u8f6c\u65b9\u5f0f (ARQ: automatic repeat request) * Go-back-N: pessimistic\uff0c\u91cd\u4f20ack, ack+1, ack+2 ...   * e.g. RWS=1\u7684\u60c5\u5f62 * Selective repeat: optimistic\uff0c\u91cd\u4f20ack, last_sent, last_sent+1, ...   * e.g. RWS=SWS=N\u7684\u60c5\u5f62   * \u5bf9burst of losses\u6548\u679c\u4e0d\u597d</p>"},{"location":"CS/Basics/Computer-Networking-Lecture-CS144-Stanford/#2-10-tcp-header","title":"2-10 TCP Header","text":"<ul> <li>pseudo header\uff1a\u7c7b\u4f3c2-2\uff0cchecksum\u7684\u8ba1\u7b97\u56ca\u62ec\u4e86IP header</li> <li>ack: \u5982\u679c\u662fbi-directional\uff0c\u4e5f\u643a\u5e26data\u4fe1\u606f\uff1b\u5982\u679c\u662funi-directional\uff0c\u597d\u50cf\u4e0d\u643a\u5e26</li> <li>URG: urgent, PSH: push</li> <li>ACK: \u9664\u4e86\u7b2c\u4e00\u4e2apacket SYN\uff0c\u5176\u5b83seg\u7684ACK\u90fd\u7f6e\u6362\u4e3a1  </li> <li>RST: reset the connection</li> <li>urgent pointer\uff1a\u548cURG\u8054\u7cfb\uff0c\u6307\u51fa\u54ea\u91ccurgent</li> </ul>"},{"location":"CS/Basics/Computer-Networking-Lecture-CS144-Stanford/#2-11-tcp-setup-and-teardown","title":"2-11 TCP Setup and Teardown","text":"<p>\u72b6\u6001\u673a\u7684\u5b9e\u73b0\u5f88\u7b80\u6d01\uff0c\u6838\u5fc3\u662f\u5982\u4f55 set up \u548c clean up (port number, etc)</p> <p>3-way handshake</p> <p>Active opener and Passive opener</p> <ol> <li>client: SYN, \u9001base number(syqno) to identify bytes</li> <li>server: SYN+ACK, \u4e5f\u9001base number</li> <li>client: ACK</li> </ol> <p>\u652f\u6301\u201csimultaneous open\u201d</p> <p>\u4f20\u9001TCP segment\uff0c\u6700\u5c0f\u53ef\u4ee51byte\uff0c\u6bd4\u5982\u5728ssh session\u6253\u5b57</p> <p>connection teardown</p> <ol> <li>client: FIN</li> <li>server: (Data +) ACK</li> <li>server: FIN</li> <li>client: ACK</li> </ol> <p></p> <ul> <li>\u4e3a\u4ec0\u4e48 TCP \u534f\u8bae\u6709 TIME_WAIT \u72b6\u6001</li> <li><code>TIME_WAIT</code> \u4ec5\u5728\u4e3b\u52a8\u65ad\u5f00\u8fde\u63a5\u7684\u4e00\u65b9\u51fa\u73b0\uff0c\u88ab\u52a8\u65ad\u5f00\u8fde\u63a5\u7684\u4e00\u65b9\u4f1a\u76f4\u63a5\u8fdb\u5165 <code>CLOSED</code> \u72b6\u6001\uff0c\u8fdb\u5165 <code>TIME_WAIT</code> \u7684\u5ba2\u6237\u7aef\u9700\u8981\u7b49\u5f85 2 MSL \u624d\u53ef\u4ee5\u771f\u6b63\u5173\u95ed\u8fde\u63a5</li> <li>\u4e0d\u76f4\u63a5\u5173\u95ed\u8fde\u63a5\u7684\u539f\u56e0\uff1a<ul> <li>\u9632\u6b62\u5ef6\u8fdf\u7684\u6570\u636e\u6bb5\u88ab\u5176\u4ed6\u4f7f\u7528\u76f8\u540c\u6e90\u5730\u5740\u3001\u6e90\u7aef\u53e3\u3001\u76ee\u7684\u5730\u5740\u4ee5\u53ca\u76ee\u7684\u7aef\u53e3\u7684 TCP \u8fde\u63a5\u6536\u5230</li> <li>RFC 793</li> <li><code>#define TCP_TIMEWAIT_LEN (60*HZ) /* how long to wait to destroy TIME-WAIT state, about 60 seconds  */</code>         * \u4f46\u662f\u5982\u679c\u4e3b\u673a\u5728\u8fc7\u53bb\u4e00\u5206\u949f\u65f6\u95f4\u5185\u4e0e\u76ee\u6807\u4e3b\u673a\u7684\u7279\u5b9a\u7aef\u53e3\u521b\u5efa\u7684 TCP \u8fde\u63a5\u6570\u8d85\u8fc7 28,232\uff0c\u90a3\u4e48\u518d\u521b\u5efa\u65b0\u7684 TCP \u8fde\u63a5\u5c31\u4f1a\u53d1\u751f\u9519\u8bef\uff0c\u4e5f\u5c31\u662f\u8bf4\u5982\u679c\u6211\u4eec\u4e0d\u8c03\u6574\u4e3b\u673a\u7684\u914d\u7f6e\uff0c\u90a3\u4e48\u6bcf\u79d2\u80fd\u591f\u5efa\u7acb\u7684\u6700\u5927 TCP \u8fde\u63a5\u6570\u4e3a ~470</li> <li>\u4fdd\u8bc1 TCP \u8fde\u63a5\u7684\u8fdc\u7a0b\u88ab\u6b63\u786e\u5173\u95ed\uff0c\u5373\u7b49\u5f85\u88ab\u52a8\u5173\u95ed\u8fde\u63a5\u7684\u4e00\u65b9\u6536\u5230 <code>FIN</code> \u5bf9\u5e94\u7684 <code>ACK</code> \u6d88\u606f</li> <li>\u9632\u6b62TIME-WAIT \u8f83\u77ed\u5bfc\u81f4\u7684\u63e1\u624b\u7ec8\u6b62\uff0c\u670d\u52a1\u7aef\u53d1\u9001<code>RST</code></li> </ul> </li> <li> <p>\u5904\u7406\u65b9\u6848\uff1a\u9664\u4e86\u4e0a\u56fe\u7684\u4e24\u8005\uff0c\u8fd8\u53ef\u4ee5\uff1a</p> <ul> <li>\u4fee\u6539 <code>net.ipv4.ip_local_port_range</code> \u9009\u9879\u4e2d\u7684\u53ef\u7528\u7aef\u53e3\u8303\u56f4\uff0c\u589e\u52a0\u53ef\u540c\u65f6\u5b58\u5728\u7684 TCP \u8fde\u63a5\u6570\u4e0a\u9650\uff1b</li> </ul> </li> <li> <p>Scaling Techniques for Servers with High Connection Rates</p> </li> <li>problems<ul> <li>Servers with high connection/transaction rates</li> <li>TCP servers, e.g. web server</li> <li>UDP servers, e.g. DNS server</li> <li>On multi-core systems, using multiple servicing threads, e.g. one thread per servicing core.</li> <li>The single server socket becomes bottleneck</li> <li>Cache line bounces</li> <li>Hard to achieve load balance</li> <li>Things will only get worse with more cores</li> </ul> </li> <li>Single TCP Server Socket<ul> <li>solution 1: Use a listener thread to dispatch established connections to server threads</li> <li>The single listener thread becomes bottleneck due to high connection rate</li> <li>Cache misses of the socket structure</li> <li>Load balance is not an issue here</li> <li>solution 2: All server threads accept() on the single server socket</li> <li>Lock contention on the server socket</li> <li>Cache line bouncing of the server socket</li> <li>Loads (number of accepted connections per thread) are usually not balanced <ul> <li>Larger latency on busier CPUs</li> <li>It can almost be achieved by accept() at random intervals, but it is hard to decide the interval value, and may introduce latency</li> </ul> </li> </ul> </li> <li>Single UDP Server Socket</li> <li>New Socket Option - SO_REUSEPORT<ul> <li>Allow multiple sockets bind()/listen() to the same local address and TCP/UDP port </li> <li>Every thread can have its own server socket</li> <li>No locking contention on the server socket</li> <li>Every thread can have its own server socket No locking contention on the server socket</li> <li>Load balance is achieved by kernel - kernel randomly picks a socket to receive the TCP connection or UDP request</li> <li>For security reason, all these sockets must be opened by the same user, so other users can not \"steal\" packets</li> </ul> </li> <li>How to enable?<ul> <li>sysctl net.core.allow_reuseport=1</li> <li>Before bind(), setsockopt SO_REUSEADDR and SO_REUSEPORT</li> <li>Then the same as a normal socket - bind()/listen() /accept()</li> </ul> </li> <li>Known Issues<ul> <li>Hash</li> <li>Have not solved the cache line bouncing problem completely</li> <li>Solved: The accepting thread is the processing thread</li> <li>Unsolved: The processed packets can be from another CPU<ul> <li>Instead of distribute randomly, deliver to the thread/socket on the same CPU \uff08input queue\u548cserver thread\u4e00\u4e00\u5bf9\u5e94\uff09</li> <li>But hardware may not support as many RxQs as CPUs</li> </ul> </li> <li>Some scheduler mechanism may harm the performance</li> <li>Affine wakeup - too aggressive in certain conditions, causing cache misses</li> </ul> </li> </ul>"},{"location":"CS/Basics/Computer-Networking-Lecture-CS144-Stanford/#2-12-tcp-recap","title":"2-12 TCP Recap","text":"<p>IP\u548cUDP\u90fd\u662fbest-effort and unreliable\uff0c\u4f46\u662f\u6211\u4eec\u4e0d\u9700\u8981\u62c5\u5fc3truncation\u548ccorruption\uff0c\u56e0\u4e3a\uff1a * Header checksum (IP) * Data checksum (UDP)</p> <p></p> <p></p>"},{"location":"CS/Basics/Computer-Networking-Lecture-CS144-Stanford/#2-13-tcpip-kevin-fall","title":"2-13 TCP/IP -- Kevin Fall","text":"<p>\u300aTCP/IP Illustrated\u300b2nd edition </p> <p>securites: firewalls;  architectural underpinnings</p> <p>packets\u548cdatagrams\u662f\u4e24\u4e2a\u6838\u5fc3\u6982\u5ff5\uff0cdatagrams\u4e3a\u4e86\u660e\u786e\u76ee\u7684\u5730\uff0c\u5728\u8bbe\u8ba1\u65f6\u6709\u66f4\u591a\u7684trade-off</p> <p>3-d printing\u3001\u67aa\u3001DRM(Digital Rights Management)</p>"},{"location":"CS/Basics/Computer-Networking-Lecture-CS144-Stanford/#3-0-packet-switching","title":"3-0 Packet Switching","text":"<p>Packet -&gt; self-contained data unit</p> <p>packet delay</p> <ul> <li>Packetization delay</li> <li>Propagation delay</li> <li>Queueing delay</li> </ul>"},{"location":"CS/Basics/Computer-Networking-Lecture-CS144-Stanford/#3-1-the-history-of-networks","title":"3-1 The History of Networks","text":"<p>Semaphore telegraphs by Chappe (France)\uff0c\u53d1\u5c55\u51fa\u4ee5\u4e0b\u6982\u5ff5\uff1a</p> <ul> <li>Codes</li> <li>Flow Control</li> <li>Synchronization</li> <li>Error detection and retransmission</li> <li>Encryption</li> </ul> <p>Pre-defined messages -&gt; arbitrary messages -&gt; compression -&gt; control signals \"Protocols\"</p>"},{"location":"CS/Basics/Computer-Networking-Lecture-CS144-Stanford/#3-2-what-is-packet-switching","title":"3-2 What is packet switching?","text":"<p>Circuit Switching</p> <ul> <li>telephone: dedicated wire -&gt; circuit switch -&gt; dedicated wire</li> <li>each phone call: 64 kb/s, no share with anybody else (private, guaranteed, isolated data rate from e2e)</li> <li>A 10Gb/s trunk line can carry over 150000 calls</li> </ul> <p>Circuit Switching \u7528\u4e8e Internet \u7684\u7f3a\u70b9</p> <ul> <li>Inefficient: bursty communication (images, ssh connection, web pages)</li> <li>Diverse Rates</li> <li>State Management</li> </ul> <p>Packet Switching </p> <ul> <li>Network = end hosts + links + packet switches</li> <li>forwarding table (routed individually by looking up)</li> <li>All packets share the full capacity of a link</li> <li>The routers maintain no per-communication state</li> <li>have buffers: must send one at a time during periods of congestion</li> <li>\u6709\u4e0d\u540c types: routers\u3001ethernet switches</li> </ul> <p>Why Internet uses packet switching</p> <ul> <li>Efficient use of expensive links</li> <li>Resilience to failure of links &amp; routers</li> <li>the Internet was to be a datagram subnet</li> <li>Internet was designed to be the interconnection of the existing networks</li> </ul>"},{"location":"CS/Basics/Computer-Networking-Lecture-CS144-Stanford/#3-3-terminology-end-to-end-delay-and-queueing","title":"3-3 Terminology, End to End Delay and Queueing","text":"<p>Propagation Delay:   </p> <ul> <li>single bit to travel over a link</li> <li>1000km, 2*10^8m/s ---&gt; 5ms</li> <li>\u4e0d\u53d7 link rate \u5f71\u54cd</li> </ul> <p>Packetization Delay:   </p> <ul> <li>64byte packet, 100Mb/s link ---&gt; 5.12us</li> <li>1kbit (1024bit) packet, 1kb/s link (1000bit/s) ---&gt; 1.024s</li> </ul> <p>E2E delay   </p> <ul> <li> <p>store and forward network</p> </li> <li> <p>router \u7406\u8bba\u4e0a\u80fd\u7b49\u5230 header \u76f4\u63a5\u5f00\u59cb packetization (cut through switching)\uff0cinternet router \u901a\u5e38\u4e0d\u8fd9\u6837\u505a\uff0c\u662f\u6536\u5230\u6574\u4e2a packet \u518d\u53d1\u9001</p> </li> <li>queueing delay -&gt; packet delay variation</li> </ul>"},{"location":"CS/Basics/Computer-Networking-Lecture-CS144-Stanford/#3-4-playback-buffers","title":"3-4 Playback Buffers","text":"<p>Real-time applications (e.g. YouTube and Skype) have to cope with variable queueing delay</p> <p></p> <ul> <li>variable delay \u6709\u4e0b\u754c</li> <li>receive \u66f2\u7ebf\u659c\u7387\u6709\u4e0a\u754c</li> </ul>"},{"location":"CS/Basics/Computer-Networking-Lecture-CS144-Stanford/#3-5-simple-deterministic-queue-model","title":"3-5 Simple Deterministic Queue Model","text":"<p>d(t): \u6c34\u5e73\u622a\u8ddd\u7684\u5dee\uff0c\u8868\u793a\u5355\u4e2a byte \u7684 queueing time</p> <p>Q: Why not send the entire message in one packet?</p> <p>A: parallel transmission across all links -&gt; reduce e2e latency</p> <p>---&gt; Statistical Multiplexing Gain = 2C/R</p>"},{"location":"CS/Basics/Computer-Networking-Lecture-CS144-Stanford/#3-6-queueing-model-properties","title":"3-6 Queueing Model Properties","text":"<p>Queues with Random Arrival Processes (Queueing Theory)</p> <ul> <li>Bustiness increases delay</li> <li>Determinism minimizes delay</li> <li>Little's Result</li> <li> , where d = average delay, lambda = arival rate, L = average number that are in the queue</li> <li>The M/M/1 queue</li> <li>\u7528 Poisson process \u5efa\u6a21 aggregation of many independent random events\uff0clambda = arrival rate</li> <li>network traffic is very bursty =&gt; \u7528 poisson \u8fc7\u7a0b\u5efa\u6a21 the arrival of new flows</li> <li>M/M/1 Queue:   </li> </ul>"},{"location":"CS/Basics/Computer-Networking-Lecture-CS144-Stanford/#3-7-switching-and-forwarding","title":"3-7 Switching and Forwarding","text":""},{"location":"CS/Basics/Computer-Networking-Lecture-CS144-Stanford/#congestion-control","title":"Congestion Control","text":"<ul> <li>Why</li> <li>What if the receiver\u2019s window size is really big?<ul> <li>Sender transmits too many segments. Most overflow router\u2019s queue and are dropped. We call this \u201ccongestion.\u201d</li> <li>Sender must resend the same bytes again and again. Eventually, stream comes out of receiver\u2019s TCP correctly</li> </ul> </li> <li>The problem with unlimited sending: collapse and fairness</li> <li>In networking, almost any problem that involves decentralized resource allocation = congestion control.</li> <li>What</li> <li>a second and tighter window maintained by sender<ul> <li>receiver\u2019s window (advertised from receiver to sender)</li> <li>\u201ccongestion window\u201d cwnd (maintained by sender)</li> </ul> </li> <li>How much data can be \u201con the link\u201d at any moment?<ul> <li>(5 Mbit/s) x (100 ms) = 62.5 kilobytes</li> </ul> </li> <li>Ideal total number of bytes outstanding = bandwidth x delay product (BDP).</li> <li>\u201cNo loss\u201d window: anything less than BDP + max queue size.</li> <li>Note: \u7528 window \u4e0d\u7528 rate\uff0c\u8bef\u5dee\u5c0f</li> <li>How</li> <li>\u201cAdditive Increase, Multiplicative Decrease\u201d algorithm (AIMD)</li> <li>One possibility: increase on success, decrease on loss</li> <li>Start with cwnd at small value (e.g. 3 segments)</li> <li>On success (segment fully acknowledged), increase by 1 segment per RTT<ul> <li>On each byte acknowledged: cwnd += (segment size)/cwnd</li> </ul> </li> <li>On loss, assume congestion. Cut cwnd in half!<ul> <li>Loss inferred when:</li> <li>segment was sent a long time ago, still not acknowledged</li> <li>or several later-sent segments have been acknowledged</li> </ul> </li> <li> <p>Slow-start: exponential growth at the beginning</p> <ul> <li>On each byte acknowledged: cwnd++</li> <li>On first loss, cut cwnd in half and revert to AIMD</li> </ul> </li> <li> <p>rpc\u6846\u67b6congestion control\u53ef\u80fd\u548ctcp congestion control\u76f8\u7ed3\u5408</p> </li> <li>https://capnproto.org/news/2020-04-23-capnproto-0.8.html</li> <li>it queries the send buffer size of the underlying network socket, and sets that as the \u201cwindow size\u201d for each stream.</li> <li>But, the TCP socket buffer size only approximates the BDP of the first hop. A better solution would measure the end-to-end BDP using an algorithm like BBR.</li> <li>Oracle STREAMS's Flow Control</li> <li>\u72b6\u6001\u4ece\u540e\u5f80\u524dpropagation\u7684\u8bbe\u8ba1\uff0c<code>canputnext()</code></li> <li> <p>\u963b\u585e\u5219 <code>putbq</code></p> </li> <li> <p>TCP_NODELAY \u548c Nagle \u7b97\u6cd5</p> </li> <li> <p>tcp\u534f\u8bae\u6808\u9ed8\u8ba4\u5173\u95ednodelay\u7684</p> </li> <li> <p><code>if there is new data to send       if the window size &gt;= MSS and available data is &gt;= MSS         send complete MSS segment now       else         if there is unconfirmed data still in the pipe           enqueue data in the buffer until an acknowledge is received         else           send data immediately         end if       end if     end if</code></p> </li> </ul>"},{"location":"CS/Basics/Computer-Networking-Lecture-CS144-Stanford/#potpourri","title":"potpourri","text":""},{"location":"CS/Basics/Computer-Networking-Lecture-CS144-Stanford/#rfc","title":"RFC","text":"<ul> <li>RFC 792: ICMP Message</li> <li>RFC 821: SMTP</li> <li>RFC 1958:Architectural Principles of the Internet</li> <li>RFC 2606: localhost</li> <li>RFC 6298: Computing TCP's Retransmission Timer</li> <li>RFC 6335: port number</li> <li> <p>RFC 7414: A Roadmap for TCP</p> </li> <li> <p>TCP backlog: syns queue and accept queue</p> </li> <li>What is a REST API?</li> <li>Representational State Transfer (REST)</li> <li>Architecture style</li> <li>Relies on a stateless, client-server protocol, almost alwasys HTTP<ul> <li>GET: retrieve data from a specified resource</li> <li>POST: submit data to be processed to a specified resource</li> <li>PUT: update a specified resource</li> <li>DELETE</li> <li>HEAD: same as get but does not return a body</li> <li>OPTIONS: return the supported HTTP methods</li> <li>PATCH: update partial resources</li> </ul> </li> <li>Treats server objects as resources that can be created or destroyed</li> <li>GitHub REST API: https://docs.github.com/en/rest</li> <li>\u63a8\u8350 Postman \u5de5\u5177</li> <li>AF_INET\u57df\u4e0eAF_UNIX\u57dfsocket\u901a\u4fe1\u539f\u7406\u5bf9\u6bd4</li> </ul> <pre><code>#include &lt;sys/types.h&gt;\n#include &lt;unistd.h&gt;\n#include &lt;sys/un.h&gt;\n#include &lt;sys/socket.h&gt;\n#include &lt;sys/types.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;stddef.h&gt;\n\n#define UNIX_SOCK_PATH_MAX_LEN (sizeof(((struct sockaddr_un*)0)-&gt;sun_path))\n#define COMMAND_MAX_LEN 64\n//file name format is project_pid.sock\n#define SOCK_PATH_FORMAT \"/dev/shm/project_%llu.sock\"\n\nchar control_sock_path[UNIX_SOCK_PATH_MAX_LEN] = {'\\0',};\n\nint main(int argc, char *argv[]) {\n  int fd = socket(AF_UNIX, SOCK_STREAM, 0);\n    if(fd &lt; 0){\n        printf(\"socket error\\n\");\n    }\n    snprintf(control_sock_path,\n            UNIX_SOCK_PATH_MAX_LEN,\n            UNIX_SOCK_PATH_FORMAT,\n            (unsigned long long)atoll(argv[1]));\n    printf(\"%s\\n\", control_sock_path);\n    struct sockaddr_un un;\n    memset(&amp;un, 0, sizeof(un));\n    strncpy(un.sun_path, control_sock_path, sizeof(un.sun_path));\n    un.sun_family = AF_UNIX;\n    socklen_t len = offsetof(struct sockaddr_un, sun_path) + strlen(un.sun_path);\n    if (connect(fd, (struct sockaddr *)&amp;un, len) &lt; 0){\n        close(fd);\n        printf(\"connect error\\n\");\n    }\n    process(fd);\n}\n</code></pre> <ul> <li>\u4ee3\u7406\uff0c\u7f51\u5173\uff0c\u96a7\u9053\uff0c\u6709\u4ec0\u4e48\u533a\u522b\u4e0e\u8054\u7cfb\uff1f - \u77e5\u4e4e</li> </ul>"},{"location":"CS/Basics/Computer-Networking-Lecture-CS144-Stanford/#wireshark","title":"wireshark","text":"<p>\u8c08\u8c08Linux\u4e2d\u7684TCP\u91cd\u4f20\u6293\u5305\u5206\u6790</p> <pre><code>telnet cs144.keithw.org http\nGET /hello HTTP/1.1 # path part,\u7b2c\u4e09\u4e2aslash\u540e\u9762\u7684\u90e8\u5206\nHost: cs144.keithw.org # host part,`https://`\u548c\u7b2c\u4e09\u4e2aslash\u4e4b\u95f4\u7684\u90e8\u5206\n\ntcp.port == 90 and ip.addr== XXX\ntcp.len &gt; 0\nip.ttl == XXX\nicmp.code == 0\n</code></pre> <p>\u8bfe\u7a0b\u4f5c\u4e1a\uff1a</p> <p>1.Ping</p> <p>2.SMTP\uff1a\u5728TCP\u4e0a\u5c42</p> <p>3.Traceroute * VM\u7684\u7b2c\u4e00\u8df3\u662f\u5230laptop\uff0c\u4e0d\u4f1adecrement the TTL\uff0c\u56e0\u6b64hop 10\u5bf9\u5e94TTL 9</p>"},{"location":"CS/Basics/Computer-Networking-Lecture-CS144-Stanford/#wifi","title":"WiFi \u4e0e \u8def\u7531\u5668","text":"<ul> <li>WIFI5 \u7684\u8fde\u63a5\u901f\u5ea6\u6700\u9ad8 866.7 Mbps\uff0c\u53ea\u6709\u5f00\u542f WIFI6 \u6a21\u5f0f\uff0c\u5e76\u4e14\u542f\u7528160MHZ\uff0c\u624d\u80fd\u7a81\u7834 866.7 Mbps</li> <li>\u8def\u7531\u5668 LAN-LAN \u7ea7\u8054</li> </ul>"},{"location":"CS/Basics/Computer-Networking-Lecture-CS144-Stanford/#tcp","title":"TCP \u541e\u5410\u7814\u7a76","text":"<ul> <li>\u5343\u5146\u4ee5\u592a\u7f51\u7684\u88f8\u541e\u5410\u91cf\u662f 125MB/s\uff0c\u5e94\u7528\u5c42\u7684\u541e\u5410\u7387\u5927\u7ea6\u5728 117MB/s \u4e0a\u4e0b</li> <li>\u30102022\u5e74\u3011\u666e\u904d\u673a\u5668\u5e26\u5bbd\u662f25or100GB/s\u4e86\uff0c\u91cf\u7ea7\u548c\u5185\u5b58\u5e26\u5bbd\u63a5\u8fd1\uff0c\u6781\u9650\u60c5\u51b5\u4e0b\u7684\u5305\u76f8\u5173memcpy\u4e0d\u53ef\u5ffd\u7565</li> <li>\u5728\u4e0d\u8003\u8651 jumbo frame \u7684\u60c5\u51b5\u4e0b\uff0c\u8ba1\u7b97\u8fc7\u7a0b\u662f: \u5bf9\u4e8e\u5343\u5146\u4ee5\u592a\u7f51\uff0c\u6bcf\u79d2\u80fd\u4f20\u8f93 1000Mbit \u6570\u636e\uff0c\u5373 125000000B/s\uff0c\u6bcf\u4e2a\u4ee5\u592a\u7f51 frame \u7684\u56fa\u5b9a\u5f00\u9500\u6709:preamble(8B)\u3001MAC(12B)\u3001type(2B)\u3001payload (46B ~ 1500B)\u3001CRC(4B)\u3001gap(12B)\uff0c\u56e0\u6b64\u6700\u5c0f\u7684\u4ee5\u592a\u7f51\u5e27\u662f 84B\uff0c\u6bcf\u79d2\u53ef\u53d1\u9001\u7ea6 1488000 \u5e27(\u6362\u8a00\u4e4b\uff0c\u5bf9\u4e8e\u4e00\u95ee\u4e00\u7b54\u7684 RPC\u3001\u5176 qps \u4e0a\u9650\u7ea6\u662f 700k/s)\uff0c\u6700\u5927\u7684\u4ee5\u592a\u7f51\u5e27\u662f 1538B\uff0c\u6bcf\u79d2\u53ef\u53d1\u9001 81 274 \u5e27\u3002 </li> <li>\u518d\u6765\u7b97 TCP \u6709\u6548\u8f7d\u8377:\u4e00\u4e2a TCP segment \u5305\u542b IP header(20B)\u548c TCP header(20B)\uff0c\u8fd8\u6709 Timestamp option(12B)\uff0c\u56e0\u6b64 TCP \u7684\u6700\u5927\u541e\u5410\u91cf\u662f 81274 \u00d7 (1500-52) = 117MB/s\uff0c\u5408 112MiB/s\u3002</li> </ul>"},{"location":"CS/Basics/Computer-Networking-Lecture-CS144-Stanford/#_1","title":"\u8bba\u6587\u9605\u8bfb","text":""},{"location":"CS/Basics/Computer-Networking-Lecture-CS144-Stanford/#ethane-taking-control-of-the-enterprise-sigcomm-07","title":"\u300aEthane: Taking Control of the Enterprise, SIGCOMM 07\u300b","text":"<p>make networks more manageable and more secure\uff0c\u4e00\u79cd\u601d\u8def\u662f\u5168\u65b9\u4f4d\u7684\u589e\u52a0\u63a7\u5236\uff0c\u76f8\u5f53\u4e8e\u65b0\u589e\u4e00\u5c42\uff0c\u53ea\u662fhide\u4e86\u590d\u6742\u5ea6\uff1b\u4e8e\u662f\u63d0\u51faethane\uff1a</p> <p>Ethane\u7684\u601d\u60f3</p> <ul> <li>The network should be governed by policies declared over high-level names</li> <li>Policy should determine the path that packets follow</li> <li>The network should enforce a strong binding between a packet   and its origin.</li> </ul> <p>Ethane\u7684\u4f18\u52bf</p> <ul> <li>Security follows management.</li> <li>Incremental deployability.</li> <li>Significant deployment experience.</li> </ul> <p>\u8bbe\u8ba1\u601d\u60f3</p> <ul> <li>Controllers: \u51b3\u5b9a\u662f\u5426\u5141\u8bb8packet\u4f20\u8f93</li> <li>Switches: a simple flow table and a secure channel to the Controller</li> <li>flow\u662f\u4e00\u4e2a\u91cd\u8981\u7684\u5c5e\u6027\u6982\u5ff5</li> <li>Binding: When machines use DHCP to request an IP address, Ethane assigns it knowing to which switch port the machine is connected, enabling Ethane to attribute an arriving packet to a physical port.</li> </ul> <p>\u5176\u5b83\u7ec6\u8282</p> <ul> <li>Replicating the Controller: Fault-Tolerance and Scalability</li> <li>cold-standby (having no network binding state) or warm-standby (having network binding state) modes</li> </ul>"},{"location":"CS/Basics/Linux-tutorial/","title":"Linux tutorial","text":"<p>This note are taken from the freeCodeCamp.org</p> <ul> <li>Linux tutorial<ul> <li>Chap 2 Linux Philosophy and Concepts<ul> <li>Chap 2 summary</li> </ul> </li> <li>Chap 3 Linux Basics and System Startup<ul> <li>==System startup process==</li> </ul> </li> <li>Chapter 7 Command Line Operations<ul> <li>Command Line advantages</li> <li>Some basic utilities</li> <li>Basic operations</li> <li>Working with files</li> </ul> </li> <li>Regular expression<ul> <li>Matching a string: re package</li> </ul> </li> </ul> </li> </ul>"},{"location":"CS/Basics/Linux-tutorial/#chap-2-linux-philosophy-and-concepts","title":"Chap 2 Linux Philosophy and Concepts","text":"<p>Basic terms</p> <ul> <li>Kernel (Glue links the hardwares and applications)</li> <li>Distribution(distros) (Group of applications)</li> <li>Boot loader (Start OS)</li> <li>Service (Program run as a background process)</li> <li>File system (Method for storing and organizing file)</li> <li>X Windows system (Provide standard toolkits and protocols to build GUI)</li> <li>Desktop environment (GUI on top of OS)</li> <li>Command Line (Interface for typing commands)</li> <li>Shell (Command Line Interpreter)</li> </ul> <p>==Linux Distribution== = A kernel + other software tools</p>"},{"location":"CS/Basics/Linux-tutorial/#chap-2-summary","title":"Chap 2 summary","text":"<ul> <li>Linux borrows from Unix</li> <li>Linux accesses features through files and file-like objects</li> <li>Linux is a multitasking multi-user OS</li> <li>Linux distro includes kernel and tools</li> </ul>"},{"location":"CS/Basics/Linux-tutorial/#chap-3-linux-basics-and-system-startup","title":"Chap 3 Linux Basics and System Startup","text":""},{"location":"CS/Basics/Linux-tutorial/#system-startup-process","title":"==System startup process==","text":"<p><code>Power on</code> :arrow_right: <code>BIOS</code> :arrow_right: <code>MBR</code> :arrow_right: <code>Boot Loader</code> :arrow_right: <code>Kernel (Linux OS)</code> :arrow_right: <code>Initial RAM disk - initramfs image</code> :arrow_right: <code>/sbin/init (parent process)</code></p> <ul> <li> <p><code>BIOS</code> (Basic Input and Output Systems) = POST (Power on self-test)</p> </li> <li> <p>BIOS software is stored on a ROM chip on the motherboard</p> </li> <li> <p>After BIOS, remainder of the boot process is controlled by the OS</p> </li> <li> <p><code>MBR</code> is abbreviation for <code>Master Boot Record</code></p> </li> <li> <p>MBR also known as First Sector of Hard Disk</p> </li> <li> <p><code>Bootloader</code> is stored on hard disks in the system, either in the boot sector (traditional BIOS or MBR) or EFI partition or unified extensible firmware interfaces(UEFI)</p> </li> <li> <p>Most common one: GRand Unified Bootloader (GRUB)/ ISO Linux (Boot from removable media)/DOS u-boot (Boot on embedded devices appliances)</p> </li> <li> <p>When booting Linux, the bootloader is responsible for loading the kernel image and the initial RAM disk or file system into memory</p> </li> <li> <p>Bootloader has 2 distinct stages</p> <p>First stage Bootloader</p> <ol> <li>For systems using BIOS MBR method</li> </ol> <p>Bootloader resides at the MBR (Size of MBR is 512 bytes), bootloader examines the partition table and finds a bootable partition. </p> <p>After that, bootloader searches for the second stage Bootloader (e.g. GRUB) and loads it into RAM.</p> <ol> <li>For systems using EFI/UEFI method</li> </ol> <p>UEFI firmware reads its boot manager data to determine which UEFI application is to be launched and from where, then firmware launches the UEFI application (e.g. GRUB)</p> <p>Second stage Bootloader (Resides under <code>\\boot</code>)</p> <p>Allow user to choose which OS to boot, after choosing, the bootloader loads the kernel of the selected OS into RAM and passes control to it.</p> <p>Then, kernel usually uncompress itself first and then check, analyze hardwares and initialize any hardwares built into the kernel.</p> </li> <li> <p><code>Initial RAM disk - initramfs image</code></p> <p>Contains programs and binary files that perform all actions needed to mount the proper root file system.</p> <p></p> <p>Bootloader loads the kernel and initial file system into memory so that it can be directly used by the kernel. Then kernel initializes and configures the computer's memory, configures all hardwares attached, loads some necessary user space applications.</p> </li> <li> <p><code>/sbin/init</code></p> <p>After kernel's initial setting, the kernel runs <code>/sbin/init/</code>, which is a initial process to start other process to get the system running.</p> <p><code>../init/</code>is responsible for starting the system, keeping the system running and shutting it down cleanly. Besides that, <code>../init/</code>is responsible for acting as a manager when necessary for all non-kernel processes.</p> <ul> <li><code>systemd</code> takes over <code>../init/</code> process.</li> </ul> <p><code>/lib/systemd/systemd</code> using aggressive parallelization techniques instead of sequential serialized set of steps, starts systems faster than <code>../init/</code></p> <p>one <code>systemd</code> command <code>systemctl</code> is used for most basic tasks.</p> </li> <li> <p>Linux Filesystems Basics</p> <p>Filesystem is the embodiment of a method storing and organizing arbitrary collections of data in a human usable form. </p> <p></p> <p></p> </li> </ul> <p>Linux use Filesystem Hierarchy Standard (FHS) to organize its system file. </p>"},{"location":"CS/Basics/Linux-tutorial/#chapter-7-command-line-operations","title":"Chapter 7 Command Line Operations","text":""},{"location":"CS/Basics/Linux-tutorial/#command-line-advantages","title":"Command Line advantages","text":"<ul> <li>No GUI overhead is incurred,</li> <li>Virtually any and every task can be accomplished while sitting at the command line.</li> <li>Capable of implementing scripts for often-used (or easy-to-forget) tasks and series of procedures.</li> <li>Can sign into remote machines anywhere on the Internet.</li> <li>Can initiate graphical applications directly from the command line instead of hunting through menus.</li> </ul>"},{"location":"CS/Basics/Linux-tutorial/#some-basic-utilities","title":"Some basic utilities","text":"<p>For file operation:</p> <ul> <li> <p><code>cat</code> used to type out a file (or combine files)</p> </li> <li> <p><code>head</code> used to show the first few lines of a file</p> </li> <li> <p><code>tail</code> used to show the last few lines of a file</p> </li> <li> <p><code>man</code> used to view documentation</p> </li> <li> <p>By stopping GUI, one can use <code>sudo systemctl stop gdm</code> or <code>sudo telinit 3</code></p> </li> </ul> <p>To restart GUI service, using <code>sudo systemctl start gdm</code> or <code>sudo telinit 5</code></p>"},{"location":"CS/Basics/Linux-tutorial/#basic-operations","title":"Basic operations","text":"<ul> <li><code>cd</code></li> <li><code>cat</code></li> <li><code>echo</code></li> <li><code>ls</code></li> <li><code>rmdir</code></li> <li><code>man</code></li> <li><code>exit</code></li> <li><code>login</code></li> <li><code>mkdir</code></li> </ul> <p>Login and logout</p> <p>After login your own machine, you can also connect and log into remote systems by using SSH.</p> <p>Reboot and shut down the system</p> <p>Preferred method to shut down the system is to use the <code>shutdown</code> command.</p> <ul> <li><code>halt</code> and <code>poweroff</code> issue<code>shutdown -h</code> to halt the system.</li> <li><code>reboot</code> issue <code>shutdown -r</code> to reboot the machine.</li> </ul> <p>Tips:</p> <ul> <li> <p><code>reboot</code> and <code>shutdown</code> from cmd require super user or root access</p> </li> <li> <p>Command to notify all users prior shutdown</p> </li> </ul> <pre><code>shutdown -h 10:00 \"Shutting down for scheduled maintenance\"\n</code></pre> <p>Locating applications</p> <p>In general, executable programs and scripts should live in the <code>/bin</code> or <code>/usr/bin</code> or <code>/sbin</code> or <code>/usr/sbiin</code> or <code>/usr/local/bin</code> or <code>/usr/local/sbin</code> or  somewhere under <code>/opt</code> or  an user account space, such as <code>/home/username/bin</code></p> <ul> <li>One way to locate the programs is to use the <code>which</code> utility.</li> <li><code>whereis</code> is a good alternative of <code>which</code> when it can not find the program, because it look for a broader range of system directories.</li> </ul> <p>Accessing directories</p> <p>Terminal's default directory is <code>/home</code> directory.</p> <p>One can print the exact path by typing</p> <pre><code>echo $HOME\n</code></pre> Command Result <code>pwd</code> Displays the present directory <code>cd ~</code> or <code>cd</code> Change to your home directory (shortcut name is ~ (tilde)) <code>cd ..</code> Change to parent directoty (<code>..</code>) <code>cd -</code> Change to previous directory (<code>-</code> (minus)) <code>pushd</code> Saves the current working directory in memory (via a directory stack) so it can be returned to at any time, places the new filepath at the top of the stack, and changes to the new filepath. <code>popd</code> returns to the path at the top of the directory stack. This directory stack is accessed by the command <code>dirs</code> in Unix or <code>Get-Location -stack</code> in Windows PowerShell. <p>:warning: You can use the combination <code>pushd [pathname]</code> :arrow_right: <code>popd</code> :arrow_right: <code>cd -</code> to do a fast return.</p> <p>Absolute and relative paths</p> <p></p> <ul> <li><code>.</code> present directory</li> <li><code>..</code> parent directory</li> <li><code>~</code> your home directory</li> </ul> <p>Exploring the filesystem</p> Command Usage <code>cd /</code> Change your current working directory to root (/) directory or path you supply <code>la</code> list of contents of the present working directory <code>ls -a</code> List of all files, including hidden files and directories (those who named start with <code>.</code>) <code>tree</code> Displays a tree view of the filesystem <p>Hard links</p> <pre><code>quaint@jarvis:~/Desktop$ touch file1\nquaint@jarvis:~/Desktop$ ln file1 file2\nquaint@jarvis:~/Desktop$ ls -li file?  # `-li` prints the node number (which is unique) in the 1st col\n303920 -rw-rw-r-- 2 quaint quaint 0  7\u6708  8 18:17 file1\n303920 -rw-rw-r-- 2 quaint quaint 0  7\u6708  8 18:17 file2\n</code></pre> <p>By create hard link, you can have multiple file names for one single file. (In this case, <code>file1</code>)</p> <p>Soft (Symbolic) links</p> <p>Created with <code>-s</code> option. </p> <pre><code>quaint@jarvis:~/Desktop$ ln -s file1 file3\nquaint@jarvis:~/Desktop$ ls -li file1 file3 #file3 is not a regular file\n303920 -rw-rw-r-- 2 quaint quaint 0  7\u6708  8 18:17 file1\n304063 lrwxrwxrwx 1 quaint quaint 5  7\u6708  8 18:42 file3 -&gt; file1 # file3 has a different enote number `304063`\n</code></pre> <p>Tips:</p> <ul> <li>Soft links are convenient cause it can easily be modified to point to different places (e.g. create ==shortcuts== to a location).</li> <li>Soft links can point to objects even on different locations. if the object doesn't exist, it is a dangly link.</li> </ul>"},{"location":"CS/Basics/Linux-tutorial/#working-with-files","title":"Working with files","text":"Command Function <code>wc</code> Word count <code>cat [-n]</code> View file contents [with line number] <code>less [-N]</code> View file with page view (scroll with space bar) [with line number] <code>tac</code> Print the entire file backwards <ul> <li><code>touch</code> resets the file's time stamp to match the current time.</li> </ul> <p><code>touch &lt;filename&gt;</code>  create a empty file as a placeholder, alternatively, you can use <code>echo &gt; &lt;filename&gt;</code></p> <p><code>touch -t &lt;timestamp&gt; &lt;filename&gt;</code> can set the date and time stamp of the give file to a specific value. e.g. <code>touch -t 201804301015 somefile</code></p> <ul> <li><code>mkdir</code> and <code>rmdir</code></li> </ul> <p><code>mkdir &lt;dirname&gt;</code>  create a directory</p> <p><code>rmdir</code></p> <p><code>rm -rf</code> </p> <ul> <li>Moving, renaming or removing a file</li> </ul> Command Usage <code>mv</code> Rename a file <code>rm</code> Remove a file <code>rm -f</code> Forcefully remove a file <code>rm -i</code> Interactively remove a file <ul> <li>Moving, renaming or removing a directory</li> </ul> Command Usage <code>mv</code> Rename a file <code>rmdir</code> Remove an empty directory <code>rm -rf</code> Forcefully remove a directory recursively <ul> <li>Modifying the command prompt</li> </ul> <p>Can modify command's <code>PS1</code> value to change its default behavior.</p> <ul> <li>Standard file stream</li> </ul> <p>File descriptor</p> <p></p> <ul> <li>I/O Redirection <code>&gt;</code> <code>&lt;</code></li> </ul> <pre><code>do_something &lt; input-file #`input-file` is the input data that can be consumed for program `do_something`\ndo_something &gt;  output-file #Write outputs to a file\ndo_something 2&gt; error-file\n</code></pre> <p>A special shorthand notation can send anything written to file descriptor 2(standard error) to the same place  as file descriptor 1 (standard out). Using the following command:</p> <pre><code>do_something &gt; all-output-file 2&gt;&amp;1\ndo_something &gt;&amp; all-output-file #bash's alternative command (easier)\n</code></pre> <ul> <li>Pipes <code>|</code></li> </ul> <p>You can pipe one program's output into a new program as its input. As following:</p> <pre><code>command1 | command2 | command3 # Pipeline example\n</code></pre> <p>Pros: </p> <ol> <li>Saving spaces: No need to store temporary data</li> <li> <p>More efficient: Reducing reading and writing from disk</p> </li> <li> <p>Searching for files</p> </li> <li> <p><code>locate</code></p> <p>Take advantage of a pre-constructed file database, can use <code>grep</code> to filter and constrain output list.</p> <p><code>grep</code> print only the lines that contain one or more specified string.</p> <pre><code>locate zip | grep bin #This command lists all files and directories with both `zip` and `bin` in their name\n</code></pre> </li> </ol>"},{"location":"CS/Basics/Linux-tutorial/#regular-expression","title":"Regular expression","text":"<ul> <li> <p>Interactive Tutorial : Rexone</p> </li> <li> <p>Language Guide for Python</p> </li> </ul> <ol> <li> <p>Raw Python Strings: Using raw Python strings (i.e. <code>r\"strings\"</code>), which is easier to read, instead regular Python strings</p> </li> <li> </li> </ol> <pre><code>matchObject = re.search(pattern, input_str, flags=0)\n</code></pre> <ol> <li>Capturing groups:</li> </ol> <pre><code># perform a global search over the whole input string, return a list\nmatchList = re.findall(pattern, input_str, flags=0)\n# returns an iterator of re.MatchObjects to walk through\nmatchList = re.finditer(pattern, input_str, flags=0)\n</code></pre> <ol> <li>Finding and replacing strings</li> </ol> <pre><code>replacedString = re.sub(pattern, replacement_pattern, input_str, count, flags=0)\n</code></pre> <ol> <li> <p><code>re</code> Flags</p> </li> <li> <p><code>re.IGNORECASE</code> makes the pattern case insensitive so that it matches strings of different capitalizations</p> </li> <li><code>re.MULTILINE</code> is necessary if your input string has newline characters (\\n), this flag allows the start and end metacharacter (^ and $ respectively) to match at the beginning and end of each line instead of at the beginning and end of the whole input string</li> <li> <p><code>re.DOTALL</code> allows the dot (.) metacharacter match all characters, including the newline character (\\n)</p> </li> <li> <p>Compiling a pattern for performance</p> </li> </ol> <pre><code>regexObject = re.compile(pattern, flags=0)\n</code></pre> <p>Links</p> <ul> <li> <p>Python Documentation for Regular Expressions</p> </li> <li> <p>Python Compatible Regex Tester</p> </li> </ul>"},{"location":"CS/Basics/Linux-tutorial/#matching-a-string-re-package","title":"Matching a string: <code>re</code> package","text":""},{"location":"CS/ML/CS229/","title":"Machine Learning","text":""},{"location":"CS/ML/CS229/#overview","title":"Overview","text":"<p>Programming  Environment: Octave</p> <ol> <li>Supervised Learning</li> </ol> <p>Dataset offers the correct answer.</p> <ul> <li> <p>Regression</p> <p>To predict a continuous valued output.</p> </li> <li> <p>Classification</p> <p>To predict  a discrete valued output.</p> <ul> <li>Support Vector Machine</li> </ul> <p>Allow a computer to deal with an infinite number of features.</p> </li> <li> <p>Unsupervised Learning</p> </li> </ul> <p>Given a dataset and find some structure in the data.</p> <ul> <li> <p>Clustering Algorithm</p> <p>e.g. Organize computing clusters, Social network analysis, Market segmentation, Astronomical data analysis</p> </li> <li> <p>Cocktail Party Algorithm</p> <p>Find structures in the data and separate out them</p> </li> </ul>"},{"location":"CS/ML/CS229/#linear-regression","title":"Linear Regression","text":"<p>Terms</p> <ul> <li> <p>Training set</p> </li> <li> <p>\\(m\\) --- # training examples </p> </li> <li> <p>\\(x\\) --- \"input\" variable / features</p> </li> <li> <p>\\(y\\) --- \"output\" variable / features</p> </li> <li> <p>\\((x,y)\\) --- one training example</p> </li> <li> <p>\\((x^{(i)},y^{(i)})\\) --- \\(i_{th}\\) training example</p> </li> <li> <p>\\(h\\) --- hypothesis</p> <p>Univariate Liner Regression / Linear Regression with one variable $$ h_\\Theta(x) = \\Theta _0 + \\Theta _1 x $$</p> </li> </ul> <p>Cost Function \\(J(\\Theta _0,\\Theta _1)\\) --- Overall Objective Function</p> <p>Goal\uff1a minimize \\(J(\\Theta _0,\\Theta _1)\\)</p> <p>Square Error Function $$ \\displaylines {J(\\Theta 0,\\Theta _1) = \\frac{1}{2m} {\\displaystyle \\sum \\ \\space \\underset {\\Theta _0, \\Theta _1}  {minimize} \\space J(\\Theta _0,\\Theta _1) } $$}^m (h_\\theta (x^{(i)})-(y^{(i)}))^2 </p> <p>Contour Plot  is  a better way to visualize \\(J(\\Theta _0,\\Theta _1)\\) .</p> <p></p>"},{"location":"CS/ML/CS229/#multivariate-linear-regression","title":"Multivariate linear regression","text":"<ul> <li>Feature scaling: Get every feature into approximately a \\(-1 \\le x _i \\le 1\\) range.</li> </ul> <p>Make sure features are on a similar scale, then gradient descent can converge more quickly.</p> <p></p> <ul> <li>Mean normalization: Replace \\(x_i\\) with \\(x_i-\\mu_i\\) to make features have approximately zero mean. (Do not apply to \\(x_0 = 1\\))</li> </ul> <p></p> <ul> <li>\"Debugging\": How to make sure gradient descent is working correctly.</li> </ul> <p>Declare convergence if \\(J (\\theta)\\) decreases by less than \\(\\epsilon\\) in one iteration, but choose a proper threshold \\(\\epsilon\\) is hard.</p> <ul> <li> <p>How to choose Learning rate \\(\\alpha\\)</p> </li> <li> <p>If \\(\\alpha\\) is too small: slow convergence</p> </li> <li>If \\(\\alpha\\) is too large: \\(J(\\theta)\\) may not decrease on every iteration; may not converge; slow convergence.</li> </ul> <p>Try a range of value for \\(\\alpha\\), every value is roughly 3 times bigger than its previous value. (e.g. \\(...0.001, 0.003, 0.01,0.03...\\)), Find a value too small, and a value that is too large, pick the largest possible value between them.</p>"},{"location":"CS/ML/CS229/#gradient-descent","title":"Gradient Descent","text":"<p>Have some function \\(J (\\theta _0, \\theta _1)\\) can be \\(\\theta _0, \\theta _1 ... \\theta _n\\)</p> <p>Want \\(\\underset {\\theta _0, \\theta _1}  {minimize} \\space J(\\theta _0,\\theta _1)\\)</p> <p>Outline:</p> <ul> <li>Start with some \\(\\theta _0, \\theta _1\\)</li> <li>Keep changing \\(\\theta _0, \\theta _1\\) to reduce \\(J (\\theta _0, \\theta _1)\\) until we hopefully end up at a minimum.</li> </ul> <p>Subtlety:</p> <ol> <li> <p>All \\(\\theta _j\\) should be updated simultaneously.</p> </li> <li> <p>As we approach a local minimum, gradient descent will automatically take smaller steps. So, no need to decrease \\(\\alpha\\) over time.</p> </li> </ol> <p></p>"},{"location":"CS/ML/CS229/#batch-gradient-descent","title":"\u201cBatch\u201d Gradient Descent","text":"<p>Each step of gradient descent uses all the training examples.</p>"},{"location":"CS/ML/CS229/#normal-equation","title":"Normal Equation","text":"<p>Method to solve for \\(\\theta\\) analytically $$ \\theta = (X^T X)^{-1} X^T y $$ \\(X\\) --- design matrix</p> <p></p> <p>Comparison between Gradient descent &amp; Normal equation</p> <p></p>"},{"location":"CS/ML/CS229/#cs-229","title":"CS 229","text":""},{"location":"CS/ML/CS229/#lecture-5-gda-naive-bayes","title":"Lecture 5 GDA &amp; Naive Bayes","text":"<p>Two examples</p> <ol> <li>Continuous value features (e.g. tumor classification) \u2014\u2014 GDA</li> <li>Discrete features (e.g. email spam, NLP)</li> </ol>"},{"location":"CS/ML/CS229/#gaussian-discriminant-analysis","title":"Gaussian Discriminant Analysis","text":"<p>Multi-variate Gaussian</p>"},{"location":"CS/ML/CS229/#lecture-6-support-vector-machine","title":"Lecture 6  Support Vector Machine","text":"<p>Notation :</p> <p></p> <p></p> <p>Labels \\(y \\in \\{-1, +1\\}\\) </p>"},{"location":"CS/ML/CS229/#optimal-margin-classifier","title":"Optimal Margin Classifier","text":""},{"location":"CS/ML/CS229/#functional-margin","title":"Functional Margin","text":"<p>\\(\\Gamma \\gamma\\)</p> <p></p> <p>If classifier has a large functional margin, two \"if\" statements listed above are TRUE.</p> <p></p> <p>\\(\\Gamma _{i} ^{hat} \\gt\\gt 0\\)</p> <p></p> <ul> <li>Easy to cheat and increase the functional margin --- (Solution: Normalize the length of your parameters (or any length you want, doesn't change the classification))</li> </ul> <p></p>"},{"location":"CS/ML/CS229/#geometric-margin-11230","title":"Geometric Margin [1:12:30]","text":""},{"location":"CS/ML/CS229/#lecture-7-kernels","title":"Lecture 7 Kernels","text":""},{"location":"CS/ML/CS229/#l-_1-norm-soft-margin-svm","title":"\\(L _1\\) norm soft margin SVM","text":"<p>For some kernel, we can use dynamic programming Knuth-Morris-Pratt to make \\(\\phi (x)^T \\phi (z) = K(x,z)\\)</p>"},{"location":"CS/ML/CS229/#lecture-8-data-splits-models-cross-validation","title":"Lecture 8 Data Splits, Models &amp; Cross-validation","text":""},{"location":"CS/ML/CS229/#bias-and-variance","title":"Bias and Variance","text":""},{"location":"CS/ML/CS229/#regularization-lambda2-theta-2","title":"Regularization  \\(\\lambda/2 \\|\\Theta\\| ^2\\)","text":"<p>regularization is the most effective way to prevent over-fitting. </p> <p></p> <p>The optimization objective of the support vector machine was to minimize \\(\\|w\\|^2\\), this turns out to maximize the geometric margin SVM</p> <p>In order to make sure the \\(\\lambda\\) on the same scale, a common pre-processing step we're using learning algorithms is normalizing different sized features to a similar scale. [the normalization also makes gradient descent run faster]</p> <p>Another way to think about Regularization</p> <p></p> <p></p> <p>Statistics world Frequentist VS Bayesian</p> <p></p> <p>Regularization and choose polynomial degree </p> <p></p>"},{"location":"CS/ML/CS229/#cross-validation","title":"Cross Validation","text":"<p>Different mechanistic procedures to find the optimum point</p> <ol> <li>Split your dataset into \\(S _{train}, S _{dev}, S _{test}\\)</li> </ol> <p>\\(S_{dev}\\) also called Cross Validation Set</p> <ol> <li> <p>Train each model i (option for degree of polynomial) on \\(S _{train}\\), get some hypothesis \\(h_i\\)</p> </li> <li> <p>Measure error on \\(S _{dev}\\). Pick model with lowest error on \\(S _{dev}\\)</p> </li> </ol> <p>Don't evaluate algorithms on training set. Cause over-fit, because more complex algorithm will always do better on the training set.</p> <ol> <li>[Optional] Evaluate the algorithm on a separate test set (\\(S _{test}\\)) and report that error.</li> </ol> <p>How do you decide how much data should go into  \\(S _{train}, S _{dev}, S _{test}\\) ?</p> Common Weight on Dataset \\(S _{train}\\) \\(S _{dev}\\) \\(S _{test}\\) Small dataset(without dev set) 70% - 30% Small dataset(with dev set) 60% 20% 20% Large dataset 90% 5% 5% <ul> <li>Choose \\(S _{train}, S _{dev}\\) to be big enough </li> <li>If you want to tease out very small differences(e.g. 0.001%), you may want a large \\(S_{test}\\) .</li> <li>If you want to compare algorithms with large accuracy differences(e.g. 1% vs 2%),  small-size \\(S_{test}\\) is enough.</li> </ul> <p>Do not make ANY decisions about your model using the test set.</p>"},{"location":"CS/ML/CS229/#simple-hold-out-cross-validation","title":"(Simple) Hold-out Cross Validation","text":"<p>==When you have a large dataset, Cross Validation can be used to choose==</p> <ol> <li>==the model of polynomial==</li> <li>==the regularization parameter \\(\\lambda\\) or \\(C\\) or \\(\\tau\\)==</li> </ol>"},{"location":"CS/ML/CS229/#k-fold-cross-validation","title":"K-fold Cross Validation","text":"<ul> <li>Makes more efficient of the data</li> <li>Computationally expensive</li> </ul> <p>==When you have a small dataset, without too much data waste==</p> <p>\\(K = 10\\) is typical</p> <p>==[Use when \\(m=50\\)(roughly) or less]== When \\(K = m\\) , this method called Leave-one-out Cross Validation</p>"},{"location":"CS/ML/CS229/#feature-selection","title":"Feature Selection","text":"<p>A special case for model selection</p> <p>If you have a lot of features, one way to reduce over-fitting is to try to find a small subset of most useful features for your task.</p>"},{"location":"CS/ML/CS229/#forward-search","title":"Forward Search","text":"<p>Keep iterating until adding more features now hurts performance, then pick whichever feature subset allows you to have the best possible performance of dev set.</p>"},{"location":"CS/ML/CS229/#backward-search","title":"Backward Search","text":""},{"location":"CS/ML/CS229/#lecture-9-approxestimation-error-erm","title":"Lecture 9 Approx/Estimation Error &amp; ERM","text":"<p>Assumptions</p> <ol> <li>Data distribution D</li> </ol> <p></p> <ol> <li>Independent samples</li> </ol> <p></p> <p>Bias and Variance</p> <p>Bias and Variance correspond to first and second moment of the sampling distribution.</p> <p>There's no correlation between bias and variance.</p> <ul> <li>Parameter View</li> </ul> <p></p> <ul> <li>If you increase the size of data(i.e. \\(m\\)), the variance of \\(\\hat \\theta\\) will be small</li> </ul> <p></p> <p>Statistical Efficiency : how efficient of your algorithm from squeezing information from given amount of data.</p> <p></p> <p>How we can Fight Variance</p> <ul> <li> <p>How we can address variance?</p> </li> <li> <p>Increase #Data \\(m --&gt; \\infin\\)</p> </li> <li> <p>Regularization</p> <p></p> </li> </ul> <p></p> <p></p> <p></p> <ul> <li>Fix high bias</li> </ul> <p>Enlarge \\(H\\)</p> <p>Reduce bias, magnify variance.</p>"},{"location":"CS/ML/CS229/#empirical-risk-minimizer","title":"Empirical Risk Minimizer","text":"<p> $$ \\begin{aligned} \\hat{\\varepsilon}(h)&amp;=\\frac{1}{m} \\sum_{i=1}^{m} 1\\left{h\\left(x^{(i)}\\right) \\neq y^{(i)}\\right}\\ \\hat{\\theta} &amp; =\\arg \\min {\\theta} \\hat{\\varepsilon}\\left(h\\right) \\end{aligned} $$</p>"},{"location":"CS/ML/CS229/#uniform-convergence","title":"Uniform Convergence","text":"<p>e.g. How the risk curve converges uniformly to the generalization risk curve.</p> <ol> <li>A given hypothesis \\(h\\) have some amount of training error \\(\\hat{\\varepsilon}(h)\\), what does that say about its generalization error \\(\\varepsilon(h)\\)</li> </ol> <p>\\(\\hat{\\varepsilon}(h) \\space \\text{vs} \\space \\varepsilon(h)\\)</p> <p></p> <p>==Solution==    $$    m \\geq \\frac{1}{2 \\gamma^{2}} \\log \\frac{2 k}{\\delta}    $$</p> <ol> <li>How does the generalization error of our learned hypothesis \\(\\varepsilon(\\hat {h})\\) compare to the best possible generalization error in that class \\(\\varepsilon({h} ^*)\\)</li> </ol> <p>\\(\\varepsilon(\\hat {h}) \\space \\text{vs} \\space \\varepsilon({h} ^*)\\)</p> <p></p> <p></p> <ul> <li>Finite hypothesis class</li> </ul> <p></p> <ul> <li>Infinite hypothesis class</li> </ul> <p>The number of examples for a wanted sample complexity is generally an order of the VC dimension to get good results.</p> <p></p> <p>Tools</p> <ol> <li>Union Bound</li> </ol> <p></p> <ol> <li>Hoeffding's Inequality</li> </ol> <p>The probability of your estimate deviating more than a certain margin only reduces as you increase m.    $$    P\\left(\\left|\\varepsilon\\left(h_{i}\\right)-\\hat{\\varepsilon}\\left(h_{i}\\right)\\right|&gt;\\gamma\\right) \\leq 2 \\exp \\left(-2 \\gamma^{2} m\\right)    $$    </p>"},{"location":"CS/ML/CS229/#lecture-10-decision-tree-and-ensemble-methods","title":"Lecture 10 Decision Tree and Ensemble Methods","text":"<p>Decision tree is a classical example model class to use with various ensembling methods</p>"},{"location":"CS/ML/CS229/#decision-tree","title":"Decision Tree","text":"<p>PROS</p> <ul> <li>Easy to explain</li> <li>Interpretable</li> <li>Categorical Variables</li> <li>Fast</li> </ul> <p>CONS</p> <ul> <li>High variance</li> <li>Bad at additive</li> <li>Low predictive accuracy</li> </ul> <p></p> <p></p> <p>How to choose split?</p> <p>Define \\(L(R)\\) : Loss in R given C classes, define \\(\\hat P_c\\) to be the proportion of examples in \\(R\\) that are of class \\(C\\).</p> <p>==Misclassification Loss==</p> <p>\\(L _{misclass} = 1 - \\underset{c}{max} \\space \\hat P_c\\)</p> <p></p> <p></p> <p>Misclassification loss is not sensitive enough. (Right one makes better decision than left, but misclassification loss is same.) To solve this, define ==cross entropy loss==</p> <p>Want to pick  a split that decrease the loss as much as possible.</p> <p>$\\underset{j,t}{max} \\space \\underbrace{L(R_p)}{parent \\space loss} - \\underbrace{(L(R_1)+L(R_2))} $</p> <p>==Cross Entropy Loss==</p>"},{"location":"CS/ML/CS229/#bits-you-need-to-communicate-to-tell-someone-who-already-knows-what-the-probabilities-are-what-class-you-are-looking-at","title":"bits you need to communicate to tell someone who already knows  what the probabilities are what class you are looking at.","text":"<p>\\(L_{cross} = - \\underset{c}{\\Sigma} \\space \\hat {P_c}log_2 \\hat {P_c}\\)</p> <p></p> <ul> <li>Strictly concave curve CAN successfully used for decision splits.</li> </ul> <p>==Gini Loss==</p> <p>\\(L_{gini} =\\underset{c}{\\sum} \\space \\hat {P_c}(1-\\hat {P_c})\\)</p>"},{"location":"CS/ML/CS229/#regression-tree","title":"Regression Tree","text":"<p>Decision trees for regression. Predict the mean of the values left instead of predict the majority of the class.</p> <p>e.g, predict the amount of snowfall you would expect in that area around that time. $$ \\begin{aligned} \\hat y_m &amp; = \\frac{\\underset{i \u2208 R_m}{\\sum} y_i}{|R_m|} \\</p> <p>L_{squared} &amp; =  \\frac{\\underset{i \u2208 R_m}{\\sum} (y_i - \\hat y_m)^2}{|R_m|} \\end{aligned} $$</p>"},{"location":"CS/ML/CS229/#categorical-variable","title":"Categorical Variable","text":"<p>if you have \\(q\\) categories, there will be \\(2^q\\) possible splits.</p>"},{"location":"CS/ML/CS229/#regularization-of-decision-trees","title":"Regularization of Decision Trees","text":"<ol> <li>min leaf size</li> <li>max depth</li> <li>max #nodes</li> <li>min decrease in loss</li> <li>Pruning (misclassification with a validation set)</li> </ol>"},{"location":"CS/ML/CS229/#runtime","title":"Runtime","text":""},{"location":"CS/ML/CS229/#no-additive-structure","title":"No additive structure","text":""},{"location":"CS/ML/CS229/#ensembling","title":"Ensembling","text":"<p>Take \\(X_i\\)'s which are random variables that are independent identically distributed (IID) $$ \\begin{aligned} Var(X_i) &amp;= \\sigma ^2 \\ Var(\\bar X) = Var(\\frac{1}{n} &amp; \\underset{i}{\\sum} X_i) = \\frac {\\sigma ^2}{n} \\end{aligned} $$ Drop the independence assumption, so now \\(X_i\\)'s just identically distributed(ID), \\(X\\)'s correlated by \\(\\rho\\). $$ Var(\\bar X) = \\rho \\sigma ^2 +\\frac{1-\\rho}{n} \\sigma ^2 $$</p>"},{"location":"CS/ML/CS229/#ways-to-ensemble","title":"Ways to Ensemble","text":"<ol> <li> <p>~~different algorithms~~</p> </li> <li> <p>~~different training sets~~</p> </li> <li> <p>==Bagging== (e.g. Random Forests)</p> </li> </ol> <p>Try to approximate having different training sets.</p> <ol> <li>==Boosting== (e.g. AdaBoost, XGBoost)</li> </ol>"},{"location":"CS/ML/CS229/#bagging-bootstrap-aggregation","title":"Bagging - Bootstrap Aggregation","text":"<p>Take a bunch of bootstrap samples, train separate models on each and then average their outputs.</p> <p>Bootstrap</p> <ol> <li>Have a true population P</li> <li>Training set \\(S \\sim P\\)</li> <li>Assume \\(P=S\\)</li> <li>Bootstrap samples \\(Z \\sim S\\)</li> </ol> <p>Bootstrap Aggregation</p> <ol> <li>Bootstrap samples \\(Z_1, Z_2, Z_3..., Z_M\\)</li> <li>Train model \\(G_m\\) on \\(Z_m\\)</li> <li>Define a meta model \\(G(m) = \\frac {\\displaystyle \\sum^{M}_{m=1} G_m(x)}{M}\\)</li> </ol> <p>Bias-Variance Analysis</p> <p>\\(Var(\\bar X) = \\rho \\sigma ^2 +\\frac{1-\\rho}{M} \\sigma ^2\\)</p> <ul> <li>Bootstrapping is driving down \\(\\rho\\)</li> </ul> <p>Increasing the number of bootstrap models in your training, doesn't cause you to overfit anymore than you were beforehand.</p> <ul> <li> <p>More \\(M\\) leads to less variance. (There is  a lower bound, can't make variance 0)</p> </li> <li> <p>Bias is slightly increased because of random subsampling. (Because the bootstrap samples \\(Z\\) are actually subsets of the original set \\(S\\), so you model become less complex, that increases bias)</p> </li> </ul> <p>Decision trees + Bagging</p> <p>Decision trees are high variance, low bias.</p> <p>Ideal fit for bagging</p>"},{"location":"CS/ML/CS229/#random-forests","title":"Random Forests","text":"<p>At each split, consider only a fraction of total features.</p> <ul> <li>Decrease \\(\\rho\\)</li> <li>Decorrelate Models</li> </ul>"},{"location":"CS/ML/CS229/#boosting","title":"Boosting","text":"<p>Adaboost, XGBoost, gradient boost machine</p> <ul> <li>Decrease bias</li> <li>Additive</li> </ul> <p>==AdaBoost==</p> <p></p>"},{"location":"CS/ML/CS229/#lecture-11-introduction-to-neural-networks","title":"lecture 11 Introduction to Neural Networks","text":"<p>Outline</p> <ul> <li>Logistic Regression (in DP view)</li> <li>Neural Network</li> </ul> <p>Deep Learning</p> <ul> <li>Computational power</li> <li>Data available</li> <li>algorithms</li> </ul>"},{"location":"CS/ML/CS229/#logistic-regression-in-dp-view","title":"Logistic Regression (in DP view)","text":"<p>Goal 1: Find cats in images</p> <p></p> <p><code>#Parameters</code> for this Logistics Model is <code>#weights + #bias</code> (i.e. <code>12288+1</code>), so the <code>#Parameters</code> depends on the size of the input. $$ \\begin{aligned} neuron &amp; = linear + activation \\ model &amp; = architecture + parameters \\end{aligned} $$</p> <ul> <li>neuron</li> </ul> <p></p> <p><code>wx+b</code>(or <code>z</code>) is the linear part, <code>\u03c3</code> is the activation.</p> <ul> <li> <p>model</p> </li> <li> <p>architecture</p> <p>In this case the architecture is a one-neuron neural network</p> <p>Tips : One thing needs to be considered for architecture: Is the output layer have the same number of neurons as you wants. (<code>#classes</code> for classification and <code>1</code>  for regression)</p> </li> <li> <p>parameters</p> <p><code>w</code> and <code>b</code></p> </li> <li> <p>Loss function</p> </li> </ul> <p>\\(\\mathcal L = -\\left(y \\log \\hat y + \\left(1-y \\right) \\log \\left(1 -\\hat y \\right) \\right)\\)</p> <p>Goal 2: Find cat/lion/iguana in images</p> <p></p> <ul> <li> <p>Red part shows how to represent labels in the dataset.</p> </li> <li> <p><code>#parameters = #neuron * #parameters_pre_neuron</code></p> </li> <li> <p>Loss Function</p> </li> </ul> <p>How to train these parameters?</p> <p>\\(\\mathcal L _{3N} = -\\displaystyle \\sum ^3_{k=1} \\left(y_k \\log \\hat y_k + \\left(1-y_k \\right) \\log \\left(1 -\\hat y_k \\right) \\right)\\)</p> <p>\\(\\mathcal L _{3N}\\) stands for loss function for 3 neurons.</p> <p>The derivative of one specific k won't be more complex than one-neuron network.</p> <p>Goal 3: add constraint Unique animal in one image ==Softmax Multi-class Network/Regression==</p> <p></p> <ul> <li>Softmax formula</li> </ul> <p>Instead of getting a probabilistic output for each \\(\\hat y_i\\), we will get a probability distribution over all the classes.</p> <p>Make same layer's neurons have \"dependencies\" (i.e. probabilities sum to 1) on each other due to the implement of Softmax.</p> <ul> <li>Labels' format</li> </ul> <p></p> <ul> <li> <p><code>#parameters = #neuron * #parameters_pre_neuron</code></p> </li> <li> <p>Loss Function ==Softmax Cross-entropy Loss==   $$   \\begin{aligned}   \\mathcal L_{CE} = -\\sum_{k=1}^m y_k \\log \\hat y_k   \\end{aligned}   $$   Softmax Cross-entropy Loss is very often used in multi-classification</p> </li> </ul>"},{"location":"CS/ML/CS229/#neural-networks","title":"Neural Networks","text":"<p>GOAL: Predict cat's age in the image</p> <ul> <li> <p>For neuron's activation, use ==ReLU== (Rectified Linear Unit) </p> </li> <li> <p>Modify Loss function to \\(|y-\\hat y|\\) or \\(\\|y-\\hat y\\|^2\\)</p> </li> </ul> <p>\\(L1\\) and \\(L2\\) norm loss is much easier to optimize for a regression task than it is for a classification task and vice versa.</p> <p>Goal 1: Given an image, tells if there's cat or no cat</p> <p></p> <ul> <li> <p><code>#parameters = sum(#parameters_in_each_layer)</code> (e.g, <code>#parems_in_1st_layer = 3n+3</code>)</p> </li> <li> <p>Terminology </p> </li> </ul> <p>==Hidden Layer== : we don't really know what it's going to figure out, but with enough data, it should understand very complex information about the data.</p> <p>Similarly, 1st layer of the neuron network called ==Input Layer==, last layer is ==Output Layer==</p> <p>==Fully connected layer==: all the neurons among previous layer are connected to each other in its next layer.</p> <p>==End-to-end learning/Blackbox model== : Training just based on the input and output. (i.e. train the network without adding constraints for the hidden layers.)</p> <p></p>"},{"location":"CS/ML/CS229/#propagation-equation","title":"Propagation Equation","text":"<p>Forward Propagation Equation $$ \\begin{aligned} z^{[\\ell]}&amp;=W^{[\\ell]} a^{[\\ell-1]}+b^{[\\ell]} \\ a^{[\\ell]}&amp;=g^{[\\ell]}\\left(z^{[\\ell]}\\right) \\end{aligned} $$</p> <p></p> <p>What happens for an input batch of m examples?</p> <p></p> <ul> <li>Need to broadcast \\(b^{[i]}\\) to make the linear algebra work.</li> </ul> <p>Backward Propagation Equation</p> <p></p> <p></p> <ul> <li>Need correspond Forward Propagation Equation to remember the path to take in your chain rule</li> </ul>"},{"location":"CS/ML/CS229/#improving-your-neural-networks","title":"Improving Your Neural Networks","text":"<ul> <li> <p>different Activation Functions</p> </li> <li> <p>Sigmoid</p> <p>Pros Used in classification problems</p> <p>Cons Sigmoid activation works good in its linear regime, but has trouble working in saturating regime. (Because if z is very high or very low, your gradient is very close to 0, makes z hard to update)</p> </li> <li> <p>ReLU</p> <p>ReLU is mostly used $$ ReLU'(z) = \\mathbb I{z&gt;0} $$</p> <p>Slope is 1, so ReLU is actually just directing the gradient to some entry.</p> </li> <li> <p>tanh     $$     \\begin{aligned}     \\tanh(z) &amp; = \\frac{e^z - e^{-z}}{e^z + e^{-z}} \\     \\tanh '(z) &amp; = 1 - \\tanh(z)^2     \\end{aligned}     $$     Pros and Cons same as sigmoid</p> </li> </ul> <p>Why we need activation functions? <p>If you don't use activation functions (i.e. use identity function as activation), your neural network is going to be equivalent to a linear regression, no matter how deep it is.</p> <ul> <li> <p>Initialization methods</p> </li> <li> <p>Normalize your inputs</p> <p></p> <p></p> </li> <li> <p>Vanishing and exploding gradients</p> <p>Errors add up by multiply each other. When neurons' weights is bigger than one, explode situation; when it's less than one, vanish situation.</p> <p>To solve this issue,  we need to initialize neurons' weights properly.</p> <p></p> <p></p> <p>Some commonly used weight initialization techniques</p> <pre><code># Commonly used weights initialization techinques\n## Initialize one layer's weight proportionally to #former_layer_inpus (works very well for sigmoid activations)\n### add random to avoid symmetry problems(every is going to learn the same thing)\nw_l = np.random.randn(shape)*np.sqrt(1/n_(l-1))\n### Modify above function a little bit, makes it works better on ReLU\nw_l = np.random.randn(shape)*np.sqrt(2/n_(l-1))\n## Xavier Initialization (for tanh activations)\n## He Initialization (very often used), doing the same thing but also for the back propagated gradients\n</code></pre> <p></p> </li> <li> <p>Optimization</p> </li> <li> <p>==Mini-batch gradient descent==</p> <p>A trade-off between stochastic gradient descent and batch GD. (i.e. a trade-off between stochasticity and vectorization)</p> <p></p> <pre><code>/*Mini-batch gradient descent*/\nmini_m = #examples_per_minibatch\nfor t in (1, #iteration):\n    select a batch x[t],y[t]\n    forward_propagate(batch) /*J  = 1/mini_m * sum(L[t])*/\n    backward_propagate(batch)\n    update w[l],b[l]\n</code></pre> <p></p> <p> left: Batch GD right: Mini-Batch GD <p>The smaller the batch, the more stochasticity, the more noise on the cost function graph.</p> <li> <p>==(GD +)Momentum Algorithm==     $$     \\begin{array}{l}     v_{d W^{[\\ell]}}=\\beta v_{d W^{[\\ell]}}+(1-\\beta) \\frac{\\partial J}{\\partial W^{[\\ell]}}\\     W^{[\\ell]}=W^{[\\ell]}-\\alpha v_{d W[\\ell]}     \\end{array}     $$     </p> <p>Take past updates into consideration (Look at the past update and take the average of it) in order to find the right way to go.</p> </li> <li> <p>RMS prop (CS230)</p> </li> <li> <p>Atom (CS230)</p> </li> <li> <p>Regularization</p> </li>"},{"location":"CS/ML/CS229/#lecture-13-debugging-ml-models-and-error-analysis","title":"Lecture 13 Debugging ML Models and Error Analysis","text":"<p>Outline</p> <ol> <li>Diagnostics for debugging learning algorithms</li> <li>Error analyses and ablative analysis</li> <li>How to get started in a machine learning  problem.</li> <li>Premature (statistical) optimization</li> </ol>"},{"location":"CS/ML/CS229/#diagnostics-for-debugging-learning-algorithms","title":"Diagnostics for debugging learning algorithms","text":"<p>Workflow to figure out what's the problem</p> <ul> <li> <p>Bias &amp; Variance Diagnostics</p> </li> <li> <p>Optimization Diagnostics</p> </li> <li> <p>Is the algorithm converging?</p> </li> <li>Is there something wrong with the numerical model?</li> <li> <p>Did we use the right cost function \\(J(\\theta)\\)</p> </li> <li> <p>Error analysis</p> </li> </ul> <p>Figure the differences between where you are and perfect performance.</p> <p>When you have a  complex machine learning pipeline, error analysis helps you break down the error (i.e. attribute the error to different components), which let you focus on what to work on.</p> <ul> <li>Ablative Analysis</li> </ul> <p>Figure the differences between where you are and something much worse.</p>"},{"location":"CS/ML/CS229/#lecture-14-expectation-maximization-algorithms","title":"Lecture 14 Expectation-Maximization Algorithms","text":"<p>EM implements a softer way of assigning points to the different cluster centroids.</p> <ul> <li> <p>To do EM, we need a concave function</p> </li> <li> <p>E-step    $$    Q _i(z^{(i)}) = P(z ^{(i)} \\mid x ^{(i)};\\theta)    $$    \\(Q _i(z^{(i)})\\) is \\(w _j^{(i)}\\)</p> </li> <li> <p>M-step    $$    \\theta := \\arg \\max \\theta \\sum_i \\sum    $$}} Q _i(z^{(i)}) \\log \\frac {P(x ^{(i)}, z^{(i)} ; \\theta)}{Q _i (z^{(i)})</p> </li> </ul> <p>Iterative E-step and M-step, the algorithm should converge to a local optima.</p> <p>Question: Why don't we just maximize \\(\\max _\\theta  l (\\theta)\\) ?</p> <p>Because there's no known way to solve that.</p>"},{"location":"CS/ML/CS229/#clustering-k-means","title":"Clustering (K-means)","text":"<p>How to choose \\(K\\) ?</p> <p>Issue: <code>#clusters</code> might be ambiguous.</p> <p>Solution: </p> <ol> <li>AIC or BIC criteria for automatically choosing <code>#clusters</code></li> <li>Choose manually</li> </ol> <p>What to do when K-means stuck in local minima?</p> <p>Run K-means on different iteration times and different initializations of cluster centroids. Pick the lowest cost function \\(J(c,\\mu)\\) run.</p>"},{"location":"CS/ML/CS229/#density-estimation","title":"Density Estimation","text":"<p>EM algorithm visualization:</p> <ul> <li>At E-step, constructing a lower bound (green curve) for the log-likelihood.</li> </ul> <p>Green curve has two properties</p> <ol> <li> <p>Green curve is a lower bound (i.e. green curve lies blow the blue curve)</p> </li> <li> <p>Its value is equal to the blue curve at the current value of \\(\\theta\\). (This property guarantees that when you optimize the green function, you can improving blue function too.)      $$      \\log {E_{z^{(i)} \\sim Q_i}\\left[\\frac {P(x ^{(i)}, z^{(i)} ; \\theta)}{Q i (z^{(i)})}\\right]} = E\\right]      $$      To ensure this property, we need      $$      Q _i(z^{(i)}) = P(z ^{(i)} \\mid x ^{(i)};\\theta)      $$} \\sim Q_i}\\left[\\log {\\frac {P(x ^{(i)}, z^{(i)} ; \\theta)}{Q _i (z^{(i)})}</p> </li> <li> <p>At M-step, take the green curve and find its maximum</p> </li> </ol> <p>Move \\(\\theta\\) from green value to the red value.</p>"},{"location":"CS/ML/CS229/#mixture-of-gaussian-model","title":"Mixture of Gaussian Model","text":"<p>Gaussian Discrimination Analysis(GDA) vs Mixture of Gaussians</p> <ol> <li>Dataset (supervised or unsupervised)</li> </ol> <p>For a given dataset, GDA has example \\((x^{(i)}, y^{(i)})\\) where \\(y^{(i)}\\) is observed, MG doesn't.</p> <ol> <li>Probability Distribution</li> </ol> <p>For GDA, \\(y^{(i)} \\sim Bernoulli(\\phi)\\)</p> <p>For MG, \\(z^{(i)} \\sim Multinomial(\\phi)\\)</p> <ol> <li>Variance for Gaussian Distribution \\(\\Sigma\\)</li> </ol> <p>For GDA, \\(x^{(i)} \\mid y^{(i)}_j \\sim \\mathcal N (\\mu _j ,\\Sigma)\\)</p> <p>For MG, \\(x^{(i)} \\mid z^{(i)}_j \\sim \\mathcal N (\\mu _j ,\\Sigma _j)\\)</p>"},{"location":"CS/ML/CS229/#jensens-inequality","title":"Jensen's Inequality","text":"<p>Let \\(f\\) be a convex function (i.e. \\(f''(x) \\gt 0\\)), let \\(x\\) be a random variable, then \\(f(EX) \\le E[f(x)]\\)</p> <p>Addendum</p> <ul> <li>If \\(f''(x) \\gt 0\\) (i.e. \\(f\\) is strictly convex), then \\(f(EX) = E[f(x)] \\Longleftrightarrow X = E[X] \\text { with probablity 1 }(i.e. x \\text{ is a constant})\\) </li> <li>Jensen's Equality in concave form: Let \\(f\\) be a concave function (i.e. \\(f''(x) \\lt 0\\)), let \\(x\\) be a random variable, then \\(f(EX) \\ge E[f(x)]\\)</li> </ul>"},{"location":"CS/ML/CS229/#lecture-15-em-algorithms-factor-analysis","title":"Lecture 15 EM Algorithms &amp; Factor Analysis","text":"<p>Outline :</p> <ul> <li>EM convergence</li> </ul> <p>How to monitor if EM is converging</p> <ul> <li>Gaussian Properties</li> </ul> <p>Map the EM equations back to mixture of Gaussian models \\(Q _i(z^{(i)}) \\Rightarrow w _j^{(i)}\\)</p> <ul> <li>Factor Analysis :star:</li> </ul> <p>A useful model for datasets which is very high-dimensional but very few training examples</p> <ul> <li> <p>Gaussian Marginals &amp; Conditionals</p> </li> <li> <p>EM Steps</p> </li> </ul> <p>Derive for the Factor Analysis model</p>"},{"location":"CS/ML/CS229/#em-convergence","title":"EM convergence","text":"<p>Different \\(Q _i(z^{(i)})\\) means different choices of lower bounds. In algorithm perspective, we write code to compute \\(w ^{(i)}_j\\)</p>"},{"location":"CS/ML/CS229/#factor-analysis-zi-sim-mathcal-n","title":"Factor Analysis \\(z^{(i)} \\sim \\mathcal N\\)","text":"<p>Factor analysis can take very high dimensional data and model them to a lower dimensional subspace with a little bit of fuzz.</p> <p>$\\color{WildStrawberry} \\text {If the data doesn't lie in a  subspace, model may not be the best model. } $</p> <p>\\(\\color{NavyBlue} \\text{But it's still a reasonable way to fit a Factor Analysis to n&gt;&gt;m dataset. }\\) [Time Label]</p> <p>A continuous \\(z\\) EM model</p> <ul> <li> <p>One Other View of EM</p> </li> <li> <p>Coordinate Ascent \\(J(\\theta, Q)\\)</p> </li> <li> <p>Comparison between Mixture of Gaussian &amp; Factor Analysis</p> </li> <li> <p>When \\(m \\gg n\\), use MG</p> <p></p> </li> <li> <p>else FA</p> </li> </ul> <p>If <code>#Training Examples</code> &lt; the dimension of the data, usual MLE function of covariance \\(\\Sigma\\) will be singular(non-invertible)</p>"},{"location":"CS/ML/CS229/#gaussian-marginals-conditionals","title":"Gaussian Marginals &amp; Conditionals","text":"<p>Marginal : calculate \\(P(x_1)\\) and we have \\(x_1 \\sim \\mathcal N (\\mu _1, \\Sigma _{11})\\)</p> <p>Conditional : calculate \\(P(x_1 \\mid x_2)\\) we have \\(x_1 \\mid x_2 \\sim \\mathcal N (\\mu _{1 \\mid 2}, \\Sigma _{1 \\mid 2})\\) $$ \\begin{aligned} \\mu {1 \\mid 2} &amp; = \\mu_1 + \\Sigma (x_2 - \\mu } \\Sigma_{22}^{-12)  \\ \\ \\Sigma  \\end{aligned} $$} &amp; = \\Sigma_{11} - \\Sigma_{12} \\Sigma_{22}^{-1} \\Sigma_{21</p>"},{"location":"CS/ML/CS229/#em-steps","title":"EM Steps","text":"<ol> <li>Derive \\(P(x, z)\\), \\(z\\) and \\(x\\) has a joint Gaussian distribution    $$    \\begin{aligned}    \\begin{pmatrix}    z \\    x     \\end{pmatrix} &amp; \\sim \\mathcal N (\\mu _{x,z}, \\Sigma) \\    z &amp; \\sim  \\mathcal N (0,I) \\    x &amp; = \\mu + \\Lambda z + \\epsilon    \\end{aligned}    $$    </li> </ol> <p>Simplify above equations. Finally we have    $$    \\begin{aligned}    \\mu _{x,z} &amp; = \\begin{pmatrix}    0 \\    \\mu     \\end{pmatrix} \\    \\Sigma &amp; = \\begin{pmatrix}    I &amp; \\Lambda ^T \\    \\Lambda &amp; \\Lambda \\Lambda ^T + \\Psi    \\end{pmatrix}</p> <p>\\end{aligned}    $$    Putting everything together, we have    $$    \\left[\\begin{array}{l}    z \\    x    \\end{array}\\right] \\sim \\mathcal{N}\\left(\\left[\\begin{array}{l}    \\vec{0} \\    \\mu    \\end{array}\\right],\\left[\\begin{array}{lc}    I &amp; \\Lambda^{T} \\    \\Lambda &amp; \\Lambda \\Lambda^{T}+\\Psi    \\end{array}\\right]\\right)    $$    There's no parameters' closed form when solve derivatives of \\(\\color{DarkBlue} P(x^{(i)})\\)'s log likelihood.</p> <p>So, we use EM to solve these parameters.</p> <ol> <li> <p>EM for Factor Analysis</p> </li> <li> <p>E-Step      $$      \\begin{aligned}      Q i(z^{(i)}) &amp; = P(z ^{(i)} \\mid x ^{(i)};\\theta) \\      z^{(i)} \\mid x^{(i)} &amp; \\sim \\mathcal N (\\mu , \\Sigma } \\mid x^{(i)}{z^{(i)} \\mid x^{(i)}})      \\end{aligned}      $$      Where      $$      \\begin{aligned}      \\mu  - \\mu) \\      \\Sigma _{z^{(i)} \\mid x^{(i)}} &amp; = I - \\Lambda ^T (\\Lambda \\Lambda^T + \\Psi) ^ {-1} \\Lambda      \\end{aligned}      $$} \\mid x^{(i)}} &amp; = \\vec 0 + \\Lambda ^T (\\Lambda \\Lambda^T + \\Psi) ^ {-1} (x^{(i)</p> </li> <li> <p>M-step      $$      \\begin{aligned}      \\Lambda &amp; =\\left(\\sum_{i=1}^{m}\\left(x^{(i)}-\\mu\\right) \\mu_{z^{(i)} \\mid x^{(i)}}^{T}\\right)\\left(\\sum_{i=1}^{m} \\mu_{z^{(i)} \\mid x^{(i)}} \\mu_{z^{(i)} \\mid x^{(i)}}^{T}+\\Sigma_{z^{(i)} \\mid x^{(i)}}\\right)^{-1} \\      \\mu &amp; = \\frac{1}{m} \\sum_{i=1}^{m} x^{(i)} \\      \\Psi &amp;= \\frac{1}{m} \\sum_{i=1}^{m} x^{(i)} x^{(i)^{T}}-x^{(i)} \\mu_{z^{(i)} \\mid x^{(i)}}^{T} \\Lambda^{T}-\\Lambda \\mu_{z^{(i)} \\mid x^{(i)}} x^{(i)^{T}}+\\Lambda\\left(\\mu_{z^{(i)} \\mid x^{(i)}} \\mu_{z^{(i)} \\mid x^{(i)}}^{T}+\\Sigma_{z^{(i)} \\mid x^{(i)}}\\right) \\Lambda^{T}      \\end{aligned}      $$</p> </li> </ol> <p>Tips:</p> <p>Simplify integrals by   $$   \\underset {z^{(i)}} \\int {Q_i(z^{(i)}) {z^{(i)}}} d{z^{(i)}} = E[z^{(i)}] = \\mu _{z^{(i)} \\mid x^{(i)}}   $$</p>"},{"location":"CS/ML/CS229/#lecture-16-independent-components-analysis-reinforced-learning","title":"Lecture 16 Independent Components Analysis &amp; Reinforced Learning","text":"<p>Outline :</p> <ul> <li>Independent Components Analysis</li> <li>CDF (cumulative distribution functions) </li> <li>ICA model</li> <li>Reinforcement Learning</li> <li>MDP (Markov decision processes)</li> </ul>"},{"location":"CS/ML/CS229/#independent-components-analysis","title":"Independent Components Analysis","text":"<ul> <li>If the data is Gaussian, ICA is not possible. Because Gaussian distribution is rotational symmetric, so there will be a rotational ambiguity (i.e. Any axis can be your component.).</li> </ul> <p>Gaussian density is the only distribution that is rotationally symmetric.</p> <ol> <li>Compute density \\(x\\)</li> </ol> <p>\\(P_x (X) = P_s(WX) |W|\\)</p> <ol> <li>Choose the density of s \\(\\longleftrightarrow\\) \\(P_s (S) = ?\\)</li> </ol> <p>Common choice of \\(P_s (S)\\)</p> <ul> <li>Laplacian Distribution (Double-Sided Exponential Function)</li> <li>Derivation of SIgmoid</li> </ul> <p>To summarize, we have $$ \\begin{aligned} P_x(X) &amp; = P_s(WX) |W| \\ &amp; = \\left(\\prod_{j=1}^n P_s\\left(w_j^T x \\right)\\right) |W| \\end{aligned} $$ MLE of this is $$ l(w) = \\sum _{i=1}^m \\log \\left(\\prod_j P_s(w_j ^T x^{(i)}) |W| \\right) $$</p>"},{"location":"CS/ML/CS229/#reinforcement-learning","title":"Reinforcement Learning","text":"<p>When you don't have a mapping from X to Y, you can't use supervised learning.</p> <p>Reinforcement Learning specify a reward function.</p> <p>Our goal is to write a cost function of a reward function give good results a high reward.</p> <p>Challenge :  Credit Assignment</p>"},{"location":"CS/ML/CS229/#mdp-markov-decision-processes","title":"MDP (Markov decision processes)","text":"<p>MDP is a five-element tuple, (S, A, {\\(P_{Sa}\\)}, \\(\\gamma\\), R). MDP provides the formalism in which RL problems are usually posed.</p> <ul> <li> <p>\\(S\\) --- set of states</p> </li> <li> <p>\\(A\\) --- set of actions</p> </li> <li> <p>\\(P_{Sa}\\) --- state transition probabilities (\\(\\sum _{s'} P_{Sa}(s') = 1\\))</p> </li> <li> <p>\\(\\gamma\\) --- discount factor (\\(\\gamma \\in [0,1)\\) usually to be chosen slightly less than 1)</p> </li> </ul> <p>\\(\\gamma\\) is the power of the time that reward is multiplied by. (e.g. encourages the robot to get deposited rewards faster or postpone the negative rewards.)</p> <p>\\(\\gamma\\) guarantees that total payoff is a bounded value.</p> <ul> <li>\\(R\\) --- reward function</li> </ul> <p>Goal of RL : Choose actions over time to maximize the expected value of the total payoff. \\(E\\left[\\sum_i \\gamma ^i R(s_i) \\right]\\)</p> <p>Output a optimal policy/controller \\(\\pi (s)\\) / \\(a = \\pi (s)\\) that maps states to actions (i.e. \\(\\pi : S \\rightarrow A\\))</p> <p>Value Function: \\(V ^{\\pi} (s)\\) is simply the expected sum of discounted rewards upon starting in state s, and taking actions according to \\(\\pi\\) $$ V^{\\pi}(s)=\\mathrm{E}\\left[R\\left(s_{0}\\right)+\\gamma R\\left(s_{1}\\right)+\\gamma^{2} R\\left(s_{2}\\right)+\\cdots \\mid s_{0}=s, \\pi\\right] $$</p>"},{"location":"CS/ML/CS229/#bellman-equations","title":"Bellman equations","text":"<p>Bellman equation can be used to efficiently solve for \\(V^\\pi\\) $$ \\begin{aligned} V^{\\pi}(s) &amp;= R(s)+\\gamma \\sum_{s^{\\prime} \\in S} P_{s \\pi(s)}\\left(s^{\\prime}\\right) V^{\\pi}\\left(s^{\\prime}\\right) \\ &amp;= R(s)+\\gamma E_{s^{\\prime} \\sim P_{s \\pi(s)}}\\left[V^{\\pi}\\left(s^{\\prime}\\right)\\right] \\end{aligned} $$</p> <p>Bellman equation sets up a solvable system of linear equations.</p>"},{"location":"CS/ML/CS229/#lecture-17-mdps-valuepolicy-iteration","title":"Lecture 17 MDPs &amp; Value/Policy Iteration","text":"<p>Challenge of finding a  policy: There's exponentially large number of possible policies.</p> <p>So how to compute the optimal policy?</p> <ul> <li>finding \\(V^*\\)</li> <li>finding \\(\\pi ^ *\\)</li> </ul> <p>What if don't know \\(P_{sa}\\) ?</p> <p>Can perform Laplace smoothing to avoid \\(\\frac{0}{0}\\) evaluation, but not necessary. Because unlike Naive Bayes, Reinforcement learning is not that sensitive for 0 values.</p> <p>MDP with unknown state transition probabilities</p> <pre><code>1.Initialize \u03c0 randomly\n2.Repeat{\n        (a)Execute \u03c0 in the MDP for some number of trials.\n        (b)Using the accumulated experience in the MDP,update our esti-\n        mates for $P_{sa}$ (and R,if applicable).\n        (c)Apply value iteration with the estimated state transition probabil-\n        ities and rewards to get a new estimated value function V.\n        (d)Update \u03c0 to be the greedy policy with respect to V.\n        }\n</code></pre>"},{"location":"CS/ML/CS229/#value-iteration","title":"Value Iteration","text":"<ul> <li>Focus on finding \\(V^*\\)</li> </ul> <p>For absorbing state, set its \\(P_{sa}\\) to 0</p> <p>Value iteration converges very quickly. (Due to \\(\\gamma\\), converges exponentially quickly) </p>"},{"location":"CS/ML/CS229/#policy-iteration","title":"Policy Iteration","text":"<ul> <li>Focus on finding \\(\\pi ^ *\\) </li> </ul>"},{"location":"CS/ML/CS229/#comparison","title":"Comparison","text":"<p>Value iteration VS Policy Iteration</p> <ul> <li>Policy iteration works good for relatively small <code>#states</code> problems, poor on large.</li> </ul> <p>For small problem, policy iteration converge faster than value iteration.</p> <ul> <li>Value Iteration will converge to \\(V^*\\), but won't ever get to exactly \\(V^*\\)</li> </ul>"},{"location":"CS/ML/CS229/#exploration-vs-exploitation-problem","title":"Exploration VS Exploitation Problem","text":"<p>When you acting a MDP, how aggressively of how greedy should you be at just taking actions to maximize your rewards?</p> <p>Using epsilon greedy to solve this problem.</p> <pre><code>1.Initialize \u03c0 randomly\n2.Repeat{\n        (a)Execute EPSILON-GREEDY \u03c0 in the MDP for some number of trials.\n        (b)Using the accumulated experience in the MDP,update our esti-\n        mates for $P_{sa}$ (and R,if applicable).\n        (c)Apply value iteration with the estimated state transition probabil-\n        ities and rewards to get a new estimated value function V.\n        (d)Update \u03c0 to be the greedy policy with respect to V.\n        }\n</code></pre> <p>Questions</p> <ul> <li>Is \\(\\epsilon\\) in epsilon greedy have to be constant?</li> </ul> <p>No, it doesn't have to be. *Boltzmann exploration</p> <ul> <li>Can you get a reward for reaching states you've never seen before?</li> </ul> <p>Intrinsic reinforcement learning / Intrinsic Motivation.</p> <ul> <li>How many actions should you take before updating \\(\\pi\\)?</li> </ul> <p>Run as frequently as it can.</p>"},{"location":"CS/ML/CS229/#lecture-18-continuous-state-mdp-model-simulation","title":"Lecture 18 Continuous State MDP &amp; Model Simulation","text":"<p>Outline</p> <ul> <li> <p>Discretization</p> </li> <li> <p>Models/Simulation</p> </li> <li> <p>Fitted Value Iteration</p> </li> </ul> <p>Fitted Value Iteration works best with a model/simulator of the MDP</p>"},{"location":"CS/ML/CS229/#discretization","title":"Discretization","text":"<p>CONS</p> <ol> <li> <p>Not smooth</p> </li> <li> <p>Curse of dimensionality</p> </li> </ol> <p>If state space is in \\(s \\in \\mathbb R ^n\\), and discretize each into \\(k\\) values, get \\(k^n\\) discrete states.</p> <p>Besides, large \\(n\\) will also cause computational complexity problem,</p>"},{"location":"CS/ML/CS229/#modelssimulation-of-mdp","title":"Models/Simulation of MDP","text":"<p>How to build a model?</p> <ul> <li> <p>Physics simulator</p> </li> <li> <p>Learn model from data</p> </li> <li> <p>Get \\(m\\) examples      $$      \\begin{align}      &amp; s_{0}^{(1)} \\stackrel{a_{0}^{(1)}}{\\longrightarrow} s_{1}^{(1)} \\stackrel{a_{1}^{(1)}}{\\longrightarrow} s_{2}^{(1)} \\stackrel{a_{2}^{(1)}}{\\longrightarrow} \\cdots \\cdot \\stackrel{a_{T-1}^{(1)}}{\\longrightarrow} s_{T}^{(1)} \\      &amp; s_{0}^{(2)} \\stackrel{a_{0}^{(2)}}{\\longrightarrow} s_{1}^{(2)} \\stackrel{a_{1}^{(2)}}{\\longrightarrow} s_{2}^{(2)} \\stackrel{a_{2}^{(2)}}{\\longrightarrow} \\cdots \\stackrel{a_{T-1}^{(2)}}{\\longrightarrow} s_{T}^{(2)} \\      &amp; s_{0}^{(m)} \\stackrel{a_{0}^{(m)}}{\\longrightarrow} s_{1}^{(m)} \\stackrel{a_{1}^{(m)}}{\\longrightarrow} s_{2}^{(m)} \\stackrel{a_{2}^{(m)}}{\\longrightarrow} \\cdots \\stackrel{a_{T-1}^{(m)}}{\\longrightarrow} s_{T}^{(m)} \\      \\end{align}      $$</p> </li> <li> <p>Apply supervised learning to estimate \\(s_{t+1}\\) as function of \\(s_t, a_t\\).</p> </li> </ul> <p>e.g. linear regression version </p> <ul> <li> <p>Deterministic model</p> <p>\\(s_{t+1} = A s_t + B a_t\\)</p> <p>A lot of deterministic model will learn a  brittle model.</p> <p>Exception: LQR and LQG will use deterministic models.</p> </li> <li> <p>Stochastic model</p> <p>\\(s_{t+1} = A s_t + B a_t + \\epsilon_t\\) where \\(\\epsilon_t \\sim \\mathcal N (0, \\sigma ^2 I)\\) </p> </li> </ul> <p>$$      \\arg \\min {A, B} \\sum    $$}^{m} \\sum_{t=0}^{T-1}\\left|s_{t+1}^{(i)}-\\left(A s_{t}^{(i)}+B a_{t}^{(i)}\\right)\\right|^{2</p> <p>==Model-based reinforcement learning==: build a model of the robot and train the RL algorithm in a simulator, and take the learned policy and apply it back on your real robot.</p> <p>**Choose feature \\(\\phi (s)\\) of state \\(s\\) **</p> <p>When you designing features, pick a bunch of features that you think hope convey how well is your robot doing. $$ V(s) = \\theta ^T \\phi(s) $$</p>"},{"location":"CS/ML/CS229/#fitted-value-iteration","title":"Fitted Value Iteration","text":"<p>Tricks</p> <p></p> <p></p> <ul> <li>Important to have noise in the simulator in model based RL, but when you're deploying, set noise \\(\\epsilon = 0\\) and \\(k=1\\)</li> </ul>"},{"location":"CS/ML/CS229/#lecture-19-reward-model-linear-dynamical-system","title":"Lecture 19 Reward Model &amp; Linear Dynamical System","text":"<p>Outline</p> <ul> <li> <p>State-action rewards</p> </li> <li> <p>Finite horizon MDP</p> </li> <li> <p>Linear dynamical systems</p> </li> </ul> <p>Can compute the exact value function (without approximation) even though the state space is continuous.</p> <ul> <li>Model</li> <li>LQR (Linear Quadratic Regulation)</li> </ul>"},{"location":"CS/ML/CS229/#state-action-rewards","title":"State-action rewards","text":"<p>Rewards is a function mapping from states and actions to the rewards (i.e. \\(R(s, a) : S \\times A  \\longmapsto \\mathbb R\\))</p> <p>In this case, Bellman's equation is $$ V^(s) = \\max a \\left(R(s, a) + \\gamma \\sum (s^\\prime)V^} P_{sa(s^\\prime)\\right) $$ Can use value iteration to solve \\(V^*(s)\\), then you can get optimal policy, which is $$ \\pi^ (s)  = \\arg \\max a \\left(R(s, a) + \\gamma \\sum (s^\\prime)V^} P_{sa(s^\\prime)\\right) $$</p>"},{"location":"CS/ML/CS229/#finite-horizon-mdp","title":"Finite horizon MDP","text":"<p>\\(\\left(\\mathcal{S}, \\mathcal{A}, P_{s a}^{(t)}, T, R^{(t)}\\right)\\)</p> <p>Replace discount factor \\(\\gamma\\) with a horizon time \\(T\\), MDP will run a finite number of \\(T\\)  steps.</p> <ul> <li>Action you take might depend on what time it is on the clock.</li> </ul> <p>Thus \\(\\pi\\) should be time dependent (i.e. \\(\\pi _t^*(s)\\)) ==Non-stationary policy==</p> <p>Non-stationary state transitions \\(s_{t+1} \\sim P_{s_t a_t}^{(t)}\\)</p> <p>Non-stationary Reward \\(R ^{(t)} (s,a)\\)</p> <p>Examples</p> <ul> <li>Changing dynamics</li> <li>Weather forecasts</li> <li>Industrial automation</li> </ul> <p>How to solve for a finite horizon MDP</p> <ol> <li> <p>Define the optimal value function    $$    V_{t}(s)=\\mathbb{E}\\left[R^{(t)}\\left(s_{t}, a_{t}\\right)+\\cdots+R^{(T)}\\left(s_{T}, a_{T}\\right) \\mid s_{t}=s, \\pi\\right]    $$    \\(V_{t}(s)\\) it the total payoff start on state \\(s\\) at time \\(t\\) execute \\(\\pi\\)</p> </li> <li> <p>Value iteration (a dynamic programming problem in this case)    $$    \\forall t&lt;T, s \\in \\mathcal{S}: \\quad V_{t}^{}(s):=\\max {a \\in \\mathcal{A}}\\left[R^{(t)}(s, a)+\\mathbb{E}^{} \\sim P_{s a}^{(t)}}\\left[V_{t+1}\\left(s^{\\prime}\\right)\\right]\\right]    $$    Base case (the final step)    $$    V^_T (s) = \\max _a R(s,a)    $$    Get the optimal policy    $$    \\pi_t^(s) =  \\arg \\max a \\left(R(s, a) + \\sum ^} P_{sa}(s^\\prime)V_{t+1(s^\\prime)\\right)    $$     Make \\(R, P_{sa}\\) be \\(R^{(t)}, P_{sa}^{(t)}\\) for non-stationary problems*</p> </li> <li> <p>If make \\(T\\) infinite, value function \\(V_t^* (s)\\) will be unbounded, thus simply make \\(T\\) infinite wouldn't get a discounted MDP formalism (traditional MDP) value iteration, we also need discount \\(\\gamma\\) to ensure a value function bound.</p> </li> </ol>"},{"location":"CS/ML/CS229/#linear-quadratic-regulation-lqr","title":"Linear Quadratic Regulation (LQR)","text":"<p>Convenient to develop with the finite horizon setting \\(\\left(\\mathcal{S}, \\mathcal{A}, P_{s a}^{(t)}, T, R^{(t)}\\right)\\), but also works with discounted MDP formalism  \\(\\left(\\mathcal{S}, \\mathcal{A}, P_{s a}, \\gamma, R \\right)\\)</p> <p>Where to get \\(A,B\\) ?</p> <ul> <li>Learn from data</li> </ul> <p>Linear regression on m examples</p> <ul> <li>Linearize a non-linear model</li> </ul> <p>A remarkable property</p> <ul> <li>Using LQR make the value function a quadratic function, can solve \\(V^\\star\\) exactly.</li> </ul>"},{"location":"CS/ML/CS229/#dynamic-programming-for-lqr","title":"Dynamic programming for LQR","text":"<ol> <li> <p>Base case    $$    \\begin{aligned}    V_T^{\\star}(s_T) &amp;= \\max_{a_T} \\space R\\left(s_T,a_T\\right) \\    &amp;= - s_T^T U s_T \\    \\pi^{\\star}_T (s_T) &amp;= \\vec 0    \\end{aligned}    $$    ==Initialize \\(\\Phi _T = -U, \\Psi_T = \\vec 0\\)==</p> </li> <li> <p>The key step</p> </li> </ol> <p>It's can be show that    $$    \\begin{align}    \\text {if } \\space V_{t+1}^{}\\left(s_{t+1}\\right) &amp; = s_{t+1}^{\\top} \\Phi_{t+1} s_{t+1}+\\Psi_{t+1} \\    \\text {then }  \\space V_{t}^{}\\left(s_{t}\\right) &amp; = s_{t}^{\\top} \\Phi_{t} s_{t}+\\Psi_{t}     \\end{align}    $$    Then we can solve LQR recursively.</p> <p>==Recursive calculate \\(\\Phi_t, \\Psi_t\\)   using \\(\\Phi_{t+1}, \\Psi_{t+1}\\) for \\(t=T-1,T-2,...,0\\)==    $$    \\begin{array}{l}    \\Phi_{t}=A_{t}^{\\top}\\left(\\Phi_{t+1}-\\Phi_{t+1} B_{t}\\left(B_{t}^{\\top} \\Phi_{t+1} B_{t}-W_{t}\\right)^{-1} B_{t} \\Phi_{t+1}\\right) A_{t}-U_{t} \\    \\Psi_{t}=-\\operatorname{tr}\\left(\\Sigma_{t} \\Phi_{t+1}\\right)+\\Psi_{t+1}    \\end{array}    $$    ==Calculate \\(L_t\\)==</p> <p>==\\(\\pi ^\\star (s_t) = L_t s_t\\)==    $$    \\begin{aligned}    L_{t} &amp;=\\left[\\left(B_{t}^{\\top} \\Phi_{t+1} B_{t}-W_{t}\\right)^{-1} B_{t} \\Phi_{t+1} A_{t}\\right]\\    \\pi ^\\star(s_t) &amp;= a_{t}^{} =\\left[\\left(B_{t}^{\\top} \\Phi_{t+1} B_{t}-V_{t}\\right)^{-1} B_{t} \\Phi_{t+1} A_{t}\\right] \\cdot s_{t} \\    &amp;=L_{t} \\cdot s_{t} \\    \\end{aligned}    $$    Takeaway: Optimal action is a linear function of state \\(s_t\\)*</p> <p>Fun Fact about LQR</p> <ul> <li>\\(L_t\\) depends on \\(\\Phi_{t+1}\\) but not \\(\\Psi_{t+1}\\), means that in order to take action, constant item for the quadratic function doesn't matter. :warning: ==So it's NOT NECESSARY to calculate \\(\\Psi\\) in the LQR algorithm==</li> </ul> <p>Thus \\(\\pi ^\\star, L_t\\) don't depend on \\(\\Sigma_w\\), but \\(V ^\\star\\) does.</p>"},{"location":"CS/ML/CS229/#lecture-20-rl-debugging-and-diagnostics","title":"Lecture 20 RL Debugging and Diagnostics","text":"<p>Outline :</p> <ul> <li>RL debugging/diagnostics</li> <li>Policy search</li> <li>Conclusion</li> </ul>"},{"location":"CS/ML/CS229/#rl-debuggingdiagnostics","title":"RL debugging/diagnostics","text":"<ol> <li>Improving simulator</li> <li>Modify value function \\(V_\\pi (s)\\)</li> <li>Modify RL algorithms \\(R(s)\\)</li> </ol>"},{"location":"CS/ML/CS229/#direct-policy-search","title":"(Direct) Policy Search","text":"<p>Solve \\(\\pi ^ \\star\\) directly (instead of using \\(V^\\star\\) to solve \\(\\pi ^ \\star\\))</p> <p>DPS focus on coming up with the class of policies you'll entertain or come up with the set of functions you use to approximate the policy.</p> <p>New definition: A stochastic policy is a function \\(\\pi : S \\times A \\longmapsto \\mathbb R\\) when \\(\\pi(s,a)\\) is the probability of taking action \\(a\\) in state \\(s\\) (\\(\\sum_a \\pi(s,a)=1\\))</p> <p>Goal : Find \\(\\theta\\) so that when we execute \\(\\pi_\\theta (s,a)\\), we maximize total payoff (expected sum of rewards), i.e. $$ \\max_ \\theta E\\left[R\\left(s_0,a_0\\right)+...+R\\left(s_T,a_T\\right)\\mid \\pi_\\theta \\right] $$  How to do that Derive a stochastic gradient ascent algorithm as a function of \\(\\theta\\) solve the equation above.</p> <p>==Reinforce Algorithm== (Very inefficient)</p> <pre><code>loop{\n/*Sample*/\ns_0, a_0, s_1,a_1 ... s_T,a_T\n/* Compute payoff*/\n$R(s_0,a_0),...,R(s_T,a_T)$\n/*Update $\\theta$ (using gradient ascent)*/\nImplement $\\theta$ update rule\n}\n</code></pre> <p>\\(\\theta\\) update rule $$ \\theta := \\theta + \\alpha \\left[\\frac{\\nabla \\pi_\\theta(s_0 ,a_0)}{\\pi_\\theta(s_0 ,a_0)} +\\frac{\\nabla \\pi_\\theta(s_1 ,a_1)}{\\pi_\\theta(s_1 ,a_1)}+...+\\frac{\\nabla \\pi_\\theta(s_T ,a_T)}{\\pi_\\theta(s_T ,a_T)} \\right] \\left(R(s_0,a_0),...,R(s_T,a_T)\\right) $$</p> <p>Note: One difference between policy search and estimated value function is that in Direct policy search \\(s_0\\) is a fixed initial state \\(s_0\\) or there's a fixed distribution over initial state \\(s_0\\).</p> <p>Direct policy search also works for continuous value function, in that situation, we have \\(a = \\theta^T + \\text{Gaussian noise}\\).</p>"},{"location":"CS/ML/CS229/#direct-policy-search-vs-value-function-based-approach","title":"Direct Policy Search vs Value-function-based Approach","text":"<p>When to use DPS:</p> <ol> <li>POMDP (Partially Observable MDP)</li> </ol> <p>At each step, get a partial (and potentially noisy) measurement of the state. Have to choose an action \\(a\\) using that.</p> <p>If we just have partially observed value of the state, even if we know \\(V^\\star (s), \\pi ^\\star (s)\\),  because we can't ensure what the state is, we still can not apply then. So we can only use DPS</p> <p>Can apply Kalman filter to estimate full state vector using partial observation state vectors, and plug them as features into policy search.</p> <ol> <li>When \\(\\pi^\\star\\) is simpler than \\(V^\\star\\)</li> </ol> <p>For low-level control task, we often have simple map form \\(\\mathcal S \\longmapsto \\mathcal A\\) i.e. simpler policy \\(\\pi^\\star\\). For multi-step reasoning problems, we should prefer to choose value function based approaches.</p> <p>CONS for Reinforce Algorithm</p> <ul> <li>Very inefficient</li> </ul> <p>Gradient estimate for reinforcement algorithm turns out to be very noisy, even though the expected value is right.</p>"},{"location":"CS/ML/CS229/#andrew-ng-rocks","title":"Andrew Ng Rocks\uff01","text":""},{"location":"CS/ML/CS229_Problem_Sets/","title":"PS 1","text":""},{"location":"CS/ML/CS229_Problem_Sets/#problem-1-linear-classifiers-logistic-regression-and-gda","title":"Problem 1 Linear Classifiers (logistic regression and GDA)","text":""},{"location":"CS/ML/CS229_Problem_Sets/#b-newtons-method","title":"b) Newton's Method","text":"<pre><code>import numpy as np\nimport util\n\nfrom linear_model import LinearModel\n\n\ndef main(train_path, eval_path, pred_path):\n    \"\"\"Problem 1(b): Logistic regression with Newton's Method.\n\n    Args:\n        train_path: Path to CSV file containing dataset for training.\n        eval_path: Path to CSV file containing dataset for evaluation.\n        pred_path: Path to save predictions.\n    \"\"\"\n    x_train, y_train = util.load_dataset(train_path, add_intercept=True)\n\n    # *** START CODE HERE ***\n    # Train Logistic Regression\n    model = LogisticRegression(eps=1e-5)\n    model.fit(x_train, y_train)\n\n    # Plot data and decision boundary\n    util.plot(x_train, y_train, model.theta, 'output/p01b.png'.format(pred_path[-5])) #TODO: Why -5?\n\n    # Save Predictions\n    x_eval, y_eval = util.load_dataset(eval_path, add_intercept=True)\n    y_pred = model.predict(x_eval)\n    np.savetxt(pred_path, y_pred &gt; 0.5, fmt='%d')\n    # *** END CODE HERE ***\n\n\nclass LogisticRegression(LinearModel):\n    \"\"\"Logistic regression with Newton's Method as the solver.\n\n    Example usage:\n        &gt; clf = LogisticRegression()\n        &gt; clf.fit(x_train, y_train)\n        &gt; clf.predict(x_eval)\n    \"\"\"\n\n    def fit(self, x, y):\n        \"\"\"Run Newton's Method to minimize J(theta) for logistic regression.\n\n        Args:\n            x: Training example inputs. Shape (m, n).\n            y: Training example labels. Shape (m,).\n        \"\"\"\n        # *** START CODE HERE ***\n        # Initialization\n        m, n = x.shape\n        self.theta = np.zeros(n)\n        # Newton's method\n        while True:\n            # save precious theta\n            old_theta = np.copy(self.theta)\n            # Sigmoid function\n            h_x = 1 / (1 + np.exp(-x.dot(self.theta)))\n            # Calculate Hessians\n            H = (x.T * h_x * (1-h_x)).dot(x) / m\n            gradient_J_theta = x.T.dot(h_x-y) / m\n\n            # Update theta\n            self.theta -= np.linalg.inv(H).dot(gradient_J_theta)\n            if np.linalg.norm(self.theta - old_theta,ord=1) &lt; self.eps:\n                break\n        # *** END CODE HERE ***\n\n    def predict(self, x):\n        \"\"\"Make a prediction given new inputs x.\n\n        Args:\n            x: Inputs of shape (m, n).\n\n        Returns:\n            Outputs of shape (m,).\n        \"\"\"\n        # *** START CODE HERE ***\n        return 1 / (1 + np.exp(-x.dot(self.theta)))\n        # *** END CODE HERE ***\n</code></pre>"},{"location":"CS/ML/CS229_Problem_Sets/#e-gda","title":"e) GDA","text":"<pre><code>import numpy as np\nimport util\n\nfrom linear_model import LinearModel\n\n\ndef main(train_path, eval_path, pred_path):\n    \"\"\"Problem 1(e): Gaussian discriminant analysis (GDA)\n\n    Args:\n        train_path: Path to CSV file containing dataset for training.\n        eval_path: Path to CSV file containing dataset for evaluation.\n        pred_path: Path to save predictions.\n    \"\"\"\n    # Load dataset\n    x_train, y_train = util.load_dataset(train_path, add_intercept=False)\n\n    # *** START CODE HERE ***\n    # Train Logistic Regression\n    model = GDA()\n    model.fit(x_train, y_train)\n\n    # Plot data and decision boundary\n    util.plot(x_train, y_train, model.theta, 'output/p01e_{}.png'.format(pred_path[-5]))\n\n    # Save Predictions\n    x_eval, y_eval = util.load_dataset(eval_path, add_intercept=True)\n    y_pred = model.predict(x_eval)\n    np.savetxt(pred_path, y_pred &gt; 0.5, fmt='%d')\n    # *** END CODE HERE ***\n\n\nclass GDA(LinearModel):\n    \"\"\"Gaussian Discriminant Analysis.\n\n    Example usage:\n        &gt; clf = GDA()\n        &gt; clf.fit(x_train, y_train)\n        &gt; clf.predict(x_eval)\n    \"\"\"\n\n    def fit(self, x, y):\n        \"\"\"Fit a GDA model to training set given by x and y.\n\n        Args:\n            x: Training example inputs. Shape (m, n).\n            y: Training example labels. Shape (m,).\n\n        Returns:\n            theta: GDA model parameters.\n        \"\"\"\n        # *** START CODE HERE ***\n        # Initialization\n        m, n = x.shape\n        self.theta = np.zeros(n + 1)\n        sum_y_indicator = np.sum(y)\n\n        # GDA Parameters\n        phi = sum_y_indicator/m\n        mu_0 = x.T.dot(1 - y) / (m - sum_y_indicator)\n        mu_1 = x.T.dot(y) / sum_y_indicator / sum_y_indicator\n        sigma = ((x[y == 0] - mu_0).T.dot(x[y == 0] - mu_0) + (x[y == 1] - mu_1).T.dot(x[y == 1] - mu_1)) / m\n        sigma_inv = np.linalg.inv(sigma)\n        self.theta[0] = 0.5*(mu_0+mu_1).T.dot(sigma_inv).dot(mu_0-mu_1) - np.log((1 - phi) / phi)\n        self.theta[1:] = sigma_inv.dot(mu_1-mu_0)\n\n        return self.theta\n        # *** END CODE HERE ***\n\n    def predict(self, x):\n        \"\"\"Make a prediction given new inputs x.\n\n        Args:\n            x: Inputs of shape (m, n).\n\n        Returns:\n            Outputs of shape (m,).\n        \"\"\"\n        # *** START CODE HERE ***\n        return 1 / (1 + np.exp(-x.dot(self.theta)))\n        # *** END CODE HERE\n</code></pre>"},{"location":"CS/ML/CS229_Problem_Sets/#gb-vs-e","title":"g\uff09(b) vs. (e)","text":"<p>Dataset 1 is fitted worse by GDA compare with Newton. </p> <p>Possible reason : \\(x|y\\) might not Gaussian distributed, so the fitted line performed worse.</p>"},{"location":"CS/ML/CS229_Problem_Sets/#h","title":"h)","text":"<p>Box-Cox transformation.</p> <p>~~Because of the skewed distribution of the data. Try another distribution for dataset 1, (e.g. Poisson, parameter is \\(\\lambda\\)). (img pasted from wikipedia)~~</p> <p></p>"},{"location":"CS/ML/CS229_Problem_Sets/#problem-2-incomplete-positive-only-labels","title":"Problem 2 Incomplete, Positive-Only Labels","text":""},{"location":"CS/ML/CS229_Problem_Sets/#ps-2","title":"PS 2","text":""},{"location":"CS/ML/CS229_Problem_Sets/#problem-1","title":"Problem 1","text":""},{"location":"CS/ML/CS229_Problem_Sets/#a","title":"a)","text":"<p>Dataset A can converge, while Dataset B cannot.</p>"},{"location":"CS/ML/CS229_Problem_Sets/#b","title":"b)","text":"<p>Dataset B is linearly separable, Dataset A isn't.</p> <p>Because \\(y \\in \\{1,-1\\}\\), we can see that the gradient of the cost function is</p> <p>$$ \\nabla_\\theta J(\\theta) = - \\frac{1}{m} \\displaystyle \\sum_{i = 1}^{m} \\frac{y^{(i)} x^{(i)}}{1 + \\exp (y^{(i)} \\theta^T x^{(i)})} $$ which means that the gradient descent algorithm is trying to minimize</p> <p>$$ \\ell (\\theta) = - \\frac{1}{m}  \\displaystyle  \\sum_{i = 1}^{m} \\log \\frac{1}{1 + \\exp (-y^{(i)} \\theta^T x^{(i)})} $$ If a dataset is completely linearly separable, i.e. \\(\\forall i \\in \\{1, \\dots, m \\}, \\ y^{(i)} \\theta^T x^{(i)} &gt; 0\\), then, by multiplying a larger positive scalar, there will always be a new \\(\\theta\\) that makes \\(\\ell (\\theta)\\) even smaller, which prevents the algorithm from converging. However, if the dataset is not linearly separable, \\(\\theta\\) cannot be generated in such way while minimizing \\(\\ell (\\theta)\\).</p>"},{"location":"CS/ML/CS229_Problem_Sets/#c","title":"c)","text":""},{"location":"CS/ML/CS229_Problem_Sets/#problem-3","title":"Problem 3","text":""},{"location":"CS/ML/CS229_Problem_Sets/#a_1","title":"a)","text":"<p>Prob: Show that \\(\\(\\theta_{\\mathrm{MAP}}=\\operatorname{argmax}_{\\theta} p(y \\mid x, \\theta) p(\\theta)\\)\\), while \\(\\(p(\\theta) = p(\\theta \\mid x)\\)\\) $$ \\begin{aligned} p(\\theta \\mid x,y) &amp; = \\frac{p(x,y,\\theta)}{p(x,y)} \\ &amp; = \\frac{p(y \\mid x,\\theta)p(x,\\theta)}{p(x,y)} \\ &amp; = \\frac{p(y \\mid x,\\theta)p(\\theta \\mid x)p(x)}{p(x,y)} \\ \\end{aligned} $$ Because \\(p(\\theta) = p(\\theta \\mid x)\\), thus $$ \\begin{aligned} p(\\theta \\mid x,y) &amp; = \\frac{p(y \\mid x,\\theta)p(\\theta \\mid x)p(x)}{p(x,y)} \\ &amp; = \\frac{p(y \\mid x,\\theta)p(\\theta)p(x)}{p(x,y)} \\end{aligned} $$ then $$ \\begin{aligned} \\theta_{MAP} &amp; = \\underset{\\theta}{argmax} \\space p(\\theta \\mid x,y) \\ &amp; = \\underset{\\theta}{argmax} \\space p(y \\mid x,\\theta)p(\\theta)\\frac{p(x)}{p(x,y)}\\ &amp; = \\underset{\\theta}{argmax} \\space p(y \\mid x,\\theta)p(\\theta) \\end{aligned} $$"},{"location":"CS/ML/CS229_Problem_Sets/#b_1","title":"b)","text":"<p>Prob: Show that MAP estimation with a zero-mean Gaussian prior over \\(\\theta\\) (i.e. \\(\\theta \\sim \\mathcal{N}\\left(0, \\eta^{2} I\\right)\\)) is equivalent to applying L2 regularization with MLE estimation. <p>Solution:</p> <p>From (a), we know that $$ \\begin{aligned}\\theta_{MAP} &amp; = \\underset{\\theta}{argmax} \\space p(\\theta \\mid x,y) \\ &amp; = \\underset{\\theta}{argmax} \\space p(y \\mid x,\\theta)p(\\theta) \\end{aligned} $$ Because \\(\\theta \\sim \\mathcal{N}\\left(0, \\eta^{2} I\\right)\\), we have \\(p(\\theta) = \\frac{1}{\\eta \\sqrt{2 \\pi}} e^{-\\frac{1}{2}\\left(\\frac{\\theta}{\\eta}\\right)^{2}}\\) $$ \\begin{aligned} \\theta_{MAP}&amp; = \\underset{\\theta}{argmax} \\space p(y \\mid x,\\theta)p(\\theta) \\ &amp; = \\underset{\\theta}{argmax} \\space p(y \\mid x,\\theta) \\frac{1}{\\eta \\sqrt{2 \\pi}} e^{-\\frac{1}{2}\\left(\\frac{\\theta}{\\eta}\\right)^{2}} \\ &amp; =\\underset{\\theta}{argmax} \\space log \\space {p(y \\mid x,\\theta)} - \\frac{1}{2}\\left(\\frac{\\theta}{\\eta}\\right)^{2}  \\end{aligned} $$ While \\(\\eta &gt; 0\\) and we do \\(arg\\) operation, thus $$ \\begin{aligned} \\theta_{MAP}&amp; = arg \\space \\underset{\\theta}{max} \\space p(y \\mid x,\\theta)p(\\theta) \\ &amp; =arg \\space \\underset{\\theta}{min} \\space - log \\space {p(y \\mid x,\\theta)} + \\frac{1}{2{\\eta}^2}{| \\theta |} ^{2}_2  \\end{aligned} $$ so we have \\(\\lambda =  \\frac{1}{2{\\eta}^2}\\)</p>"},{"location":"CS/ML/CS229_Problem_Sets/#c_1","title":"c)","text":"<p>Prob: For linear regression model. Come up with a closed form expression for \\(\\theta_{MAP}\\) $$ \\begin{array}{c} \\epsilon^{(i)} \\sim \\mathcal{N}\\left(0, \\sigma^{2}\\right) \\ y^{(i)}=\\theta^{T} x^{(i)}+\\epsilon^{(i)} \\ y^{(i)} \\mid x^{(i)}, \\theta \\sim \\mathcal{N}\\left(\\theta^{T} x^{(i)}, \\sigma^{2}\\right) \\ p\\left(y^{(i)} \\mid x^{(i)}, \\theta\\right)=\\frac{1}{\\sqrt{2 \\pi} \\sigma} \\exp \\left{-\\frac{1}{2 \\sigma^{2}}\\left(y^{(i)}-\\theta^{T} x^{(i)}\\right)^{2}\\right} \\ p(\\vec{y} \\mid X, \\theta)=\\prod_{i=1}^{m} p\\left(y^{(i)} \\mid x^{(i)}, \\theta\\right) \\ =\\prod_{i=1}^{m} \\frac{1}{\\sqrt{2 \\pi} \\sigma} \\exp \\left{-\\frac{1}{2 \\sigma^{2}}\\left(y^{(i)}-\\theta^{T} x^{(i)}\\right)^{2}\\right} \\ =\\frac{1}{(2 \\pi)^{m / 2} \\sigma^{m}} \\exp \\left{-\\frac{1}{2 \\sigma^{2}} \\sum_{i=1}^{m}\\left(y^{(i)}-\\theta^{T} x^{(i)}\\right)^{2}\\right} \\ =\\frac{1}{(2 \\pi)^{m / 2} \\sigma^{m}} \\exp \\left{-\\frac{1}{2 \\sigma^{2}}|X \\theta-\\vec{y}|{2}^{2}\\right} \\ \\log p(\\vec{y} \\mid X, \\theta)=-\\frac{m}{2} \\log (2 \\pi)-m \\log \\sigma-\\frac{1}{2 \\sigma^{2}}|X \\theta-\\vec{y}| \\ \\theta_{\\mathrm{MAP}}=\\arg \\min }^{2{\\theta}-\\log p(y \\mid x, \\theta)+\\frac{1}{2 \\eta^{2}}|\\theta| \\ =\\arg \\min }^{2{\\theta} \\frac{1}{2 \\sigma^{2}}|X \\theta-\\vec{y}||\\theta|}^{2}+\\frac{1}{2 \\eta^{2}{2}^{2} \\ J(\\theta)=\\frac{1}{2 \\sigma^{2}}|X \\theta-\\vec{y}||\\theta|}^{2}+\\frac{1}{2 \\eta^{2}{2}^{2} \\ \\nabla \\theta=0 \\ \\theta_{\\mathrm{MAP}}=\\arg \\min _{\\theta} J(\\theta)=\\left(X^{T} X+\\frac{\\sigma^{2}}{\\eta^{2}} I\\right)^{-1} X^{T} \\vec{y} \\end{array} $$} J(\\theta)=\\frac{1}{\\sigma^{2}}\\left(X^{T} X \\theta-X^{T} \\vec{y}\\right)+\\frac{1}{\\eta^{2}"},{"location":"CS/ML/CS229_Problem_Sets/#d","title":"d)","text":"<p>Prob: Show that \\(\\theta_{MAP}\\) in this case is equivalent to the solution of linear regression with \\(L1\\) regularization, $$ \\begin{aligned} \\epsilon^{(i)} &amp;\\sim \\mathcal{N}\\left(0, \\sigma^{2}\\right) \\ \\theta &amp;\\sim \\mathcal{L}(0, b I) \\ y^{(i)}&amp;=\\theta^{T} x^{(i)}+\\epsilon^{(i)} \\ f_{\\mathcal{L}}(z \\mid \\mu, b) &amp;=\\frac{1}{2 b} \\exp \\left(-\\frac{|z-\\mu|}{b}\\right) \\end{aligned} $$ Thus  $$ \\begin{aligned} p(\\theta) &amp; = \\frac{1}{(2b)^n} exp(-\\frac{|\\theta|1}{b}) \\ \\log p(\\theta) &amp;= -n\\log(2b)-\\frac{|\\theta|_1}{b} \\ \\theta &amp; = \\arg \\max_\\theta p(y \\ \\vert \\ x, \\theta) \\ p(\\theta) \\ &amp; = \\arg \\min_\\theta - \\sum_{i = 1}^{m} \\log \\frac{1}{\\sqrt{2 \\pi} \\sigma} \\exp \\big( - \\frac{(y^{(i)} - \\theta^T x^{(i)})^2}{2 \\sigma^2} \\big)- \\sum_{i = 1}^{n} \\log \\frac{1}{2 b} \\exp \\big( - \\frac{\\vert \\theta_i - 0 \\vert}{b} \\big) \\                       &amp; = \\arg \\min_\\theta \\frac{1}{2 \\sigma^2} \\sum_{i = 1}^{m} (y^{(i)} - \\theta^T x^{(i)})^2 + \\sum_{i = 1}^{n} \\frac{1}{b} \\vert \\theta_i \\vert \\                       &amp; = \\arg \\min_\\theta \\frac{1}{2 \\sigma^2} \\Vert X \\theta - \\vec{y} \\Vert_2^2 + \\frac{1}{b} \\Vert \\theta \\Vert_1 \\                       &amp; = \\arg \\min_\\theta \\Vert X \\theta - \\vec{y} \\Vert_2^2 + \\frac{2 \\sigma^2}{b} \\Vert \\theta \\Vert_1 \\end{aligned} $$}"},{"location":"CS/ML/CS229_Problem_Sets/#problem-4","title":"Problem 4","text":""},{"location":"CS/ML/CS229_Problem_Sets/#a_2","title":"a)","text":"<p>True. Because \\(K_1\\) and \\(K_2\\) are kernel in same dimension, thus they are symmetric and positive semidefinite matrix in n-th dimension, so \\(K_1 + K_2\\) is also a (Mercer) kernel in that dimension.</p>"},{"location":"CS/ML/CS229_Problem_Sets/#b_2","title":"b)","text":"<p>Not necessarily. Because \\(K\\) isn't necessarily  a positive semidefinite matrix, thus not necessarily a kernel in n-th dimension.</p>"},{"location":"CS/ML/CS229_Problem_Sets/#c-d","title":"c) &amp; d)","text":"<p>(c) True (d) False.</p> <p>\\(a\\) is a positive real number, thus \\(aK\\) is a positive semidefinite matrix, while \\(-aK\\) isn't.</p>"},{"location":"CS/ML/CS229_Problem_Sets/#e","title":"e)","text":"<p>\\(K(x, z) = K_1 (x, z) K_2 (x, z)\\) is a valid kernel.</p> <p>Because for any \\(z \\in \\mathbb{R}^n\\),</p> \\[ \\begin{align*} z^T K z &amp; = \\sum_i \\sum_j z_i K_{ij} z_j \\\\         &amp; = \\sum_i \\sum_j z_i K(x^{(i)}, x^{(j)}) z_j \\\\         &amp; = \\sum_i \\sum_j z_i K_1 (x^{(i)}, x^{(j)}) K_2 (x^{(i)}, x^{(j)}) z_j \\\\         &amp; = \\sum_i \\sum_j z_i \\phi_1 (x^{(i)})^T \\phi_1 (x^{(j)}) \\phi_2 (x^{(i)})^T \\phi_2 (x^{(j)}) z_j \\\\         &amp; = \\sum_i \\sum_j z_i \\sum_k \\phi_{1k} (x^{(i)}) \\phi_{1k} (x^{(j)}) \\sum_l \\phi_{2l} (x^{(i)}) \\phi_{2l} (x^{(j)}) z_j \\\\         &amp; = \\sum_k \\sum_l \\sum_i \\sum_j z_i \\phi_{1k} (x^{(i)}) \\phi_{2l} (x^{(i)}) z_j \\phi_{1k} (x^{(j)}) \\phi_{2l} (x^{(j)}) \\\\         &amp; = \\sum_k \\sum_l \\big( \\sum_i z_i \\phi_{1k} (x^{(i)}) \\phi_{2l} (x^{(i)}) \\big)^2 \\\\         &amp; \\geq 0 \\end{align*} \\]"},{"location":"CS/ML/CS229_Problem_Sets/#f","title":"f)","text":"<p>Yes. \\(K\\) is PSD </p> \\[ \\begin{aligned} z^T K z &amp; = \\sum_i\\sum_j z_i f(x_i)f(x_j) z_j \\\\ &amp; = \\sum_i (z_i f(x_i))^2 \\ge 0 \\end{aligned} \\] <p>~~Not necessarily. When \\(f(x)f(z)&lt;0\\), then for \\(z^T K z\\) we have a negative value multiply \\(z^T z\\),  so \\(z^T K z \\le 0\\). \\(K\\) isn't PSD.~~</p>"},{"location":"CS/ML/CS229_Problem_Sets/#g","title":"g)","text":"<p>\\(K\\) is PSD. Because \\(K_3\\) is a kernel no matter what the inputs are.</p>"},{"location":"CS/ML/CS229_Problem_Sets/#h_1","title":"h)","text":"<p>\\(K\\) is a kernel. Because \\(p(x) = \\displaystyle \\sum_k c_k x^k\\), thus \\(p(K_1 (x,z)) = \\displaystyle \\sum_k c_k (K_1 (x,z))^k\\).</p> <p>From (e), we know \\(K(x,z)=K_1(x,z) K_2(x,z)\\) is a kernel.</p> <p>From (c), we know \\(K(x,z) = a K_1(x,z), a \\in \\mathbb R^+\\) is a kernel.</p> <p>thus \\(K\\) is a kernel. </p>"},{"location":"CS/ML/CS229_Problem_Sets/#problem-5","title":"Problem 5","text":""},{"location":"CS/ML/CS229_Problem_Sets/#a_3","title":"a)","text":"<ol> <li>How to represent \\(\\theta^{(i)}\\)</li> </ol> <p>Because \\(\\theta^{(i+1)}:=\\theta^{(i)}+\\alpha\\left(y^{(i+1)}-h_{\\theta^{(i)}}\\left(\\phi (x^{(i+1))}\\right)\\right) \\phi (x^{(i+1)})\\), so \\(\\theta^{(i)}\\) is a linear function of \\(\\phi (x^{(0)}), \\phi (x^{(1)})...\\phi (x^{(i)})\\)</p> <p>i.e.    $$    \\begin{aligned}    \\theta^{(i)} &amp;= \\sum_{j=1}^i \\space \\beta_i \\phi (x^{(i)}) = \\beta^T \\Phi(x)\\    \\theta^{(0)} &amp;= \\sum_{j=1}^0 \\space  \\beta_i \\phi (x^{(i)}) = 0    \\end{aligned}    $$</p> <ol> <li> <p>How to represent \\(h_{\\theta^{(i)}}\\left(x^{(i+1)}\\right)\\)    $$    \\begin{aligned}    h_{\\theta^{(i)}}\\left(x^{(i+1)}\\right) &amp;= g\\left(\\theta^{(i)^{T}} \\phi\\left(x^{(i+1)}\\right)\\right) \\    &amp; = g\\left(\\sum_{j=1}^i \\space \\beta_i \\phi \\left(x^{(i)}\\right) \\phi\\left(x^{(i+1)}\\right)\\right) \\    &amp; = g\\left(\\sum_{j=1}^i \\space \\beta_i K\\left(x,z\\right)\\right) \\    &amp; = g\\left(\\beta^T K\\right) \\    &amp; = \\mathbb I{\\beta^T K = 1}    \\end{aligned}    $$</p> </li> <li> <p>How to modify the update rule?    $$    \\begin{aligned}    \\theta^{(i+1)} &amp;= \\theta^{(i)} + \\alpha \\left(\\overset {\\rightarrow}{y}X - \\mathbb I \\left{\\beta^T K =1\\right}^T X \\right)    \\end{aligned}    $$    \\(\\(\\begin{align*}    \\theta^{(i + 1)} : &amp; = \\theta^{(i)} + \\alpha \\big( y^{(i + 1)} - h_{\\theta^{(i)}} (\\phi (x^{(i + 1)})) \\big) \\phi (x^{(i + 1)}) \\\\                       &amp; = \\sum_{j = 1}^{i} \\beta_j \\phi (x^{(j)}) + \\underbrace{\\alpha ( y^{(i + 1)} - \\mathrm{sign} \\big( \\sum_{j = 1}^{i} \\beta_j K(x^{(j)}, x^{(i + 1)}) \\big) )}_{\\beta_{i + 1}} \\phi (x^{(i + 1)}) \\\\                       &amp; = \\sum_{j = 1}^{i + 1} \\beta_j \\phi (x^{(j)})    \\end{align*}\\)\\)</p> </li> </ol> <p>Therefore, the new update rule is:</p> <p>\\(\\(\\beta_{i + 1} := \\alpha ( y^{(i + 1)} - \\mathrm{sign} \\big( \\sum_{j = 1}^{i} \\beta_j K(x^{(j)}, x^{(i + 1)}) \\big) )\\)\\)</p>"},{"location":"CS/ML/CS229_Problem_Sets/#b_3","title":"b)","text":"<pre><code>def initial_state():\n    \"\"\"Return the initial state for the perceptron.\n\n    This function computes and then returns the initial state of the perceptron.\n    Feel free to use any data type (dicts, lists, tuples, or custom classes) to\n    contain the state of the perceptron.\n\n    \"\"\"\n\n    # *** START CODE HERE ***\n    return []\n    # *** END CODE HERE ***\n\n\ndef predict(state, kernel, x_i):\n    \"\"\"Perform a prediction on a given instance x_i given the current state\n    and the kernel.\n\n    Args:\n        state: The state returned from initial_state()\n        kernel: A binary function that takes two vectors as input and returns\n            the result of a kernel\n        x_i: A vector containing the features for a single instance\n\n    Returns:\n        Returns the prediction (i.e 0 or 1)\n    \"\"\"\n    # *** START CODE HERE ***\n    return sign(sum(beta * kernel(x, x_i) for x, beta in state))\n    # *** END CODE HERE ***\n\n\ndef update_state(state, kernel, learning_rate, x_i, y_i):\n    \"\"\"Updates the state of the perceptron.\n\n    Args:\n        state: The state returned from initial_state()\n        kernel: A binary function that takes two vectors as input and returns the result of a kernel\n        learning_rate: The learning rate for the update\n        x_i: A vector containing the features for a single instance\n        y_i: A 0 or 1 indicating the label for a single instance\n    \"\"\"\n    # *** START CODE HERE ***\n    beta_i = learning_rate*(y_i - sign(sum(beta * kernel(x, x_i) for x, beta in state)))\n    state.append((x_i, beta_i))\n    # *** END CODE HERE ***\n</code></pre>"},{"location":"CS/ML/CS229_Problem_Sets/#c_2","title":"c)","text":"<p>Dot kernel</p> <p></p> <p>Radial basis kernel</p> <p></p> <p>Dot kernel perform badly, because dataset isn't linearly separable. Because dot product kernel doesn't have feature mapping, thus the model is still linear after applying the product kernel.</p>"},{"location":"CS/ML/CS229_Problem_Sets/#problem-6-spam-classification","title":"Problem 6 Spam Classification","text":"<pre><code>import collections\n\nimport numpy as np\n\nimport util\nimport svm\n</code></pre>"},{"location":"CS/ML/CS229_Problem_Sets/#a-processing-the-the-spam-messages-into-numpy-arrays","title":"a) processing the the spam messages into numpy arrays","text":"<pre><code>def get_words(message):\n    \"\"\"Get the normalized list of words from a message string.\n\n    This function should split a message into words, normalize them, and return\n    the resulting list. For splitting, you should split on spaces. For normalization,\n    you should convert everything to lowercase.\n\n    Args:\n        message: A string containing an SMS message\n\n    Returns:\n       The list of normalized words from the message.\n    \"\"\"\n\n    # *** START CODE HERE ***\n    return message.lower().split()\n    # *** END CODE HERE ***\n\n\ndef create_dictionary(messages):\n    \"\"\"Create a dictionary mapping words to integer indices.\n\n    This function should create a dictionary of word to indices using the provided\n    training messages. Use get_words to process each message. \n\n    Rare words are often not useful for modeling. Please only add words to the dictionary\n    if they occur in at least five messages.\n\n    Args:\n        messages: A list of strings containing SMS messages\n\n    Returns:\n        A python dict mapping words to integers.\n    \"\"\"\n\n    # *** START CODE HERE ***\n    word_dict = dict()\n    index = 0\n    for message in messages:\n        word_list = get_words(message)\n        for word in word_list:\n            if word not in word_dict:\n                word_dict[word] = 1\n            else:\n                word_dict[word] += 1\n    # Delete rare words\n    index = 0\n    for word in list(word_dict.keys()):\n        if word_dict[word] &gt;= 5:\n            word_dict[word] = index\n            index += 1\n        else:\n            del word_dict[word]\n    return word_dict\n    # *** END CODE HERE ***\n\n\ndef transform_text(messages, word_dictionary):\n    \"\"\"Transform a list of text messages into a numpy array for further processing.\n\n    This function should create a numpy array that contains the number of times each word\n    appears in each message. Each row in the resulting array should correspond to each \n    message and each column should correspond to a word.\n\n    Use the provided word dictionary to map words to column indices. Ignore words that \n    are not present in the dictionary. Use get_words to get the words for a message.\n\n    Args:\n        messages: A list of strings where each string is an SMS message.\n        word_dictionary: A python dict mapping words to integers.\n\n    Returns:\n        A numpy array marking the words present in each message.\n    \"\"\"\n    # *** START CODE HERE ***\n    m, n = len(messages), len(word_dictionary)\n    # Initialize word matrix\n    word_matrix = np.zeros([m, n], dtype=int)\n    # for word in word_dictionary:\n    #     word_index = word_dictionary[word]\n    #     word_matrix[0, word_index] = word_index\n    # Record occurrence of words in messages\n    msg_index = 0\n    for message in messages:\n        word_list = get_words(message)\n        for word in word_list:\n            if word in word_dictionary:\n                word_matrix[msg_index, word_dictionary[word]] += 1\n        msg_index += 1\n    return word_matrix\n    # *** END CODE HERE ***\n</code></pre>"},{"location":"CS/ML/CS229_Problem_Sets/#b-fit-predict","title":"b) Fit &amp; Predict","text":"<p>Prediction accuracy: <code>Naive Bayes had an accuracy of 0.8870967741935484 on the testing set</code></p> <p>If possibility is <code>0.28</code> instead of <code>0.5</code>, the accuracy on the test set is <code>0.9301075268817204</code></p> <pre><code>def fit_naive_bayes_model(matrix, labels):\n    \"\"\"Fit a naive bayes model.\n\n    This function should fit a Naive Bayes model given a training matrix and labels.\n\n    The function should return the state of that model.\n\n    Feel free to use whatever datatype you wish for the state of the model.\n\n    Args:\n        matrix: A numpy array containing word counts for the training data\n        labels: The binary (0 or 1) labels for that training data\n\n    Returns: The trained model\n    \"\"\"\n    # *** START CODE HERE ***\n    m, n = matrix.shape\n    num_spam = sum(labels)\n    phi_1 = (matrix.T.dot(labels) + 1) / (num_spam + 2)\n    phi_0 = (matrix.T.dot(1-labels) + 1) / (m - num_spam + 2)\n    phi_y = num_spam / m\n    return phi_1, phi_0, phi_y\n    # *** END CODE HERE ***\n\n\ndef predict_from_naive_bayes_model(model, matrix):\n    \"\"\"Use a Naive Bayes model to compute predictions for a target matrix.\n\n    This function should be able to predict on the models that fit_naive_bayes_model\n    outputs.\n\n    Args:\n        model: A trained model from fit_naive_bayes_model\n        matrix: A numpy array containing word counts\n\n    Returns: A numpy array contains the predictions from the model\n    \"\"\"\n    # *** START CODE HERE ***\n    phi_1, phi_0, phi_y = model\n    log_y1 = np.log(matrix.dot(phi_1)) + np.log(phi_y)\n    log_y0 = np.log(matrix.dot(phi_0)) + np.log(1-phi_y)\n    possibility_y1 = np.exp(log_y1) / (np.exp(log_y0) + np.exp(log_y1))\n    return [possibility_y1 &gt; 0.5]\n    # *** END CODE HERE ***\n</code></pre>"},{"location":"CS/ML/CS229_Problem_Sets/#c-get-spam-indicators","title":"c) Get Spam Indicators","text":"<p><code>The top 5 indicative words for Naive Bayes are:  ['claim', 'won', 'prize', 'tone', 'urgent!']</code></p> <pre><code>def get_top_five_naive_bayes_words(model, dictionary):\n    \"\"\"Compute the top five words that are most indicative of the spam (i.e positive) class.\n\n    Ues the metric given in 6c as a measure of how indicative a word is.\n    Return the words in sorted form, with the most indicative word first.\n\n    Args:\n        model: The Naive Bayes model returned from fit_naive_bayes_model\n        dictionary: A mapping of word to integer ids\n\n    Returns: The top five most indicative words in sorted order with the most indicative first\n    \"\"\"\n    # *** START CODE HERE ***\n    phi_1, phi_0, phi_y = model\n    spam_coeff = np.log(phi_1) - np.log(phi_0)\n    hot5word_index = spam_coeff.argsort()[-5:]\n    hot5word = list()\n    for index in hot5word_index:\n        for key in dictionary:\n            if dictionary[key] == index:\n                hot5word.append(key)\n                break\n    hot5word.reverse()\n    return hot5word\n    # *** END CODE HERE ***\n</code></pre>"},{"location":"CS/ML/CS229_Problem_Sets/#d-compute-best-svm-radius","title":"d) Compute Best SVM Radius","text":"<p><code>The optimal SVM radius was 0.1 The SVM model had an accuracy of 0.9695340501792115 on the testing set</code></p> <pre><code>def compute_best_svm_radius(train_matrix, train_labels, val_matrix, val_labels, radius_to_consider):\n    \"\"\"Compute the optimal SVM radius using the provided training and evaluation datasets.\n\n    You should only consider radius values within the radius_to_consider list.\n    You should use accuracy as a metric for comparing the different radius values.\n\n    Args:\n        train_matrix: The word counts for the training data\n        train_labels: The spam or not spam labels for the training data\n        val_matrix: The word counts for the validation data\n        val_labels: The spam or not spam labels for the validation data\n        radius_to_consider: The radius values to consider\n\n    Returns:\n        The best radius which maximizes SVM accuracy.\n    \"\"\"\n    # *** START CODE HERE ***\n    rad_acc_dict = {}\n    for radius in radius_to_consider:\n        predict_label = svm.train_and_predict_svm(train_matrix, train_labels, val_matrix, radius)\n        accuracy = np.mean(predict_label == val_labels)\n        rad_acc_dict[radius] = accuracy\n    maxaccuracy = max(rad_acc_dict.values())\n    for key in rad_acc_dict:\n        if rad_acc_dict[key] == maxaccuracy:\n            return key\n    # return rad_acc_dict.popitem()\n    # *** END CODE HERE ***\n</code></pre>"},{"location":"CS/ML/CS229_Problem_Sets/#ps-3","title":"PS 3","text":""},{"location":"CS/ML/CS229_Problem_Sets/#problem-1-a-simple-neural-network","title":"Problem 1 A Simple Neural Network","text":""},{"location":"CS/ML/CS229_Problem_Sets/#a-calculate-gradient-descent-update-to-w-_12i","title":"a) Calculate gradient descent update to \\(w _{1,2}^{[i]}\\)","text":"\\[ \\begin{aligned} \\frac{\\part l}{\\part w _{1,2}^{[i]}} &amp; = \\frac{\\part l}{\\part o^{[i]}} \\frac{\\part o^{[i]}}{\\part h_2^{[i]}} \\frac{\\part h_2^{[i]}}{\\part w _{1,2}^{[i]}}\\\\ &amp; = \\frac{2}{m} \\sum _{i=1}^m \\left(o^{[i]}-y^{[i]}\\right) o^{[i]} \\left(1-o^{[i]}\\right) w_2^{[2]} h_2^{[i]} \\left(1-h_2^{[i]}\\right) x_1^{[i]} \\\\ &amp; = \\frac{2}{m}  w_2^{[2]}  \\sum _{i=1}^m \\left(o^{[i]}-y^{[i]}\\right) o^{[i]} \\left(1-o^{[i]}\\right) h_2^{[i]} \\left(1-h_2^{[i]}\\right) x_1^{[i]} \\\\ \\text{Update rule for } w _{1,2}^{[i]} \\text{ is:}\\\\ w _{1,2}^{[i]} &amp;= w _{1,2}^{[i]} - \\alpha \\frac{\\part l}{\\part w _{1,2}^{[i]}}\\\\ &amp; = w _{1,2}^{[i]} - \\alpha \\frac{2}{m}  w_2^{[2]}  \\sum _{i=1}^m \\left(o^{[i]}-y^{[i]}\\right) o^{[i]} \\left(1-o^{[i]}\\right) h_2^{[i]} \\left(1-h_2^{[i]}\\right) x_1^{[i]}  \\end{aligned} \\]"},{"location":"CS/ML/CS229_Problem_Sets/#b-modify-activation-function-as-step-function-and-prove-its-accuracy","title":"b) Modify activation function as step function and prove its accuracy","text":"<p>After using step function as activation function to achieve 100% accuracy.</p> <p>Because we CAN use three line(which form a triangle area) to separate different category samples for the given dataset.</p> <pre><code>def optimal_step_weights():\n    \"\"\"Return the optimal weights for the neural network with a step activation function.\n\n    This function will not be graded if there are no optimal weights.\n    See the PDF for instructions on what each weight represents.\n\n    The hidden layer weights are notated by [1] on the problem set and \n    the output layer weights are notated by [2].\n\n    This function should return a dict with elements for each weight, see example_weights above.\n\n    \"\"\"\n    w = example_weights()\n\n    # *** START CODE HERE ***\n    w['hidden_layer_0_1'] = -1\n    w['hidden_layer_1_1'] = 2\n    w['hidden_layer_2_1'] = 0\n    w['hidden_layer_0_2'] = -1\n    w['hidden_layer_1_2'] = 0\n    w['hidden_layer_2_2'] = 2\n    w['hidden_layer_0_3'] = -4\n    w['hidden_layer_1_3'] = 1\n    w['hidden_layer_2_3'] = 1\n\n    w['output_layer_0'] = -1\n    w['output_layer_1'] = 2\n    w['output_layer_2'] = 2\n    w['output_layer_3'] = 2\n    # *** END CODE HERE ***\n\n    return w\n</code></pre>"},{"location":"CS/ML/CS229_Problem_Sets/#c-modify-activation-function-as-step-function-and-prove-its-accuracy","title":"c) Modify activation function as step function and prove its accuracy","text":"<p>It's not possible, when we use identity(linear) function as activation function and output layer activation is step function, the neural network can be viewed as a linear classifier. But given dataset is not linearly separable.</p>"},{"location":"CS/ML/CS229_Problem_Sets/#problem-2-kl-divergence-and-maximum-likelihood","title":"Problem 2 KL divergence and Maximum Likelihood","text":""},{"location":"CS/ML/CS229_Problem_Sets/#a-non-negativity","title":"a) Non-negativity","text":"\\[ \\begin{aligned} D_{\\mathrm{KL}}(P \\| Q) &amp;=\\sum_{x \\in \\mathcal{X}} P(x) \\log \\frac{P(x)}{Q(x)} \\\\ &amp;=-\\sum_{x \\in \\mathcal{X}} P(x) \\log \\frac{Q(x)}{P(x)} \\\\ &amp;=E\\left[-\\log \\frac{Q(x)}{P(x)}\\right]\\\\ &amp;\\ge -\\log E\\left[ \\frac{Q(x)}{P(x)}\\right]\\\\ &amp;=- \\log  \\sum_{x \\in \\mathcal{X}}P(x) \\frac{Q(x)}{P(x)}\\\\ &amp;=- \\log  \\sum_{x \\in \\mathcal{X}} Q(x) \\\\ &amp;=- \\log 1 \\\\ &amp;=0 \\end{aligned} \\] <p>Further,</p> <ul> <li> <p>If \\(P = Q\\), then \\(D_{\\mathrm{KL}}(P \\| Q) =\\sum_{x \\in \\mathcal{X}} P(x) \\log 1 = 0\\)</p> </li> <li> <p>Because \\(-\\log\\) is strictly convex, so if \\(D_{\\mathrm{KL}}(P \\| Q) =0\\) while \\(\\frac{Q(x)}{P(x)}\\) is a constant for all \\(x \\in \\mathcal{X}\\), which is \\(Q(x)=P(x)\\)</p> </li> </ul> <p>Thus, \\(D_{\\mathrm{KL}}(P \\| Q) =0\\) if and only if \\(P=Q\\)</p>"},{"location":"CS/ML/CS229_Problem_Sets/#b-chain-rule-for-kl-divergence","title":"b) Chain rule for KL divergence","text":"\\[ \\begin{aligned} D_{\\mathrm{KL}}(P(X) \\| Q(X)) &amp;= \\sum_{x \\in \\mathcal{X}} P(x) \\log \\frac{P(x)}{Q(x)} \\\\ D_{\\mathrm{KL}}(P(Y \\mid X) \\| Q(Y \\mid X)) &amp;= \\sum_{x} P(x)\\left(\\sum_{y} P(y \\mid x) \\log \\frac{P(y \\mid x)}{Q(y \\mid x)}\\right) \\end{aligned} \\] <p>According to Bayes, we have $$ P(X,Y) = P(Y \\mid X) P(X) $$</p> \\[ \\begin{aligned} D_{\\mathrm{KL}}(P(X, Y) \\| Q(X, Y)) &amp;= D_{\\mathrm{KL}}(P(Y \\mid X) P(X) \\| Q(Y \\mid X) Q(X)) \\\\ &amp;= \\sum_{x}\\sum_{y} P(y \\mid x)P(x) \\log {\\frac {P(y \\mid x)P(x)}{Q(y \\mid x)Q(x)}} \\\\ &amp;= \\sum_{x}\\sum_{y} P(y \\mid x)P(x)  \\left(\\log {\\frac{P(y \\mid x)}{Q(y \\mid x)}}+\\log \\frac {P(x)}{Q(x)} \\right) \\\\ &amp;= \\sum_{y}P(y \\mid x)\\sum_{x}P(x)\\log \\frac {P(x)}{Q(x)} + \\sum_{x} P(x)\\left(\\sum_{y} P(y \\mid x) \\log \\frac{P(y \\mid x)}{Q(y \\mid x)}\\right) \\\\ &amp;= 1* \\sum_{x}P(x)\\log \\frac {P(x)}{Q(x)} + \\sum_{x} P(x)\\left(\\sum_{y} P(y \\mid x) \\log \\frac{P(y \\mid x)}{Q(y \\mid x)}\\right) \\\\ &amp;=D_{\\mathrm{KL}}(P(X) \\| Q(X))+D_{\\mathrm{KL}}(P(Y \\mid X) \\| Q(Y \\mid X)) \\end{aligned} \\]"},{"location":"CS/ML/CS229_Problem_Sets/#c-kl-and-maximum-likelihood","title":"c) KL and maximum likelihood","text":"\\[ D_{\\mathrm{KL}}\\left(\\hat{P} \\| P_{\\theta}\\right) = \\sum_{x} \\hat{P}(x) \\log \\frac{\\hat{P}(x)}{P_{\\theta}(x)} \\] <p>Thus, $$ \\begin{aligned} \\arg \\min {\\theta} D\\right)&amp;= \\arg \\min }}\\left(\\hat{P} | P_{\\theta{\\theta} \\sum\\right) \\ &amp;= \\arg \\max } \\hat{P}(x) \\log \\left({\\hat{P}(x)}-{P_{\\theta}(x){\\theta}\\sum\\ &amp;= \\arg \\max } \\hat{P}(x) \\log {P_{\\theta}(x){\\theta}\\sum(x)\\ &amp;= \\arg \\max } \\left(\\frac{1}{m} \\sum_{i=1}^m 1\\left{x^{(i)} = x \\right}\\right)P_{\\theta{\\theta} \\sum\\right) \\ \\end{aligned} $$}^{m} \\log P_{\\theta}\\left(x^{(i)</p>"},{"location":"CS/ML/CS229_Problem_Sets/#problem-3-kl-divergence-fisher-information-and-the-natural-gradient","title":"Problem 3 KL Divergence, Fisher Information, and the Natural Gradient","text":""},{"location":"CS/ML/CS229_Problem_Sets/#a-score-function","title":"a) Score Function","text":"<p>signifies the sensitivity of the likelihood function with respect to the parameters. $$ \\begin{aligned} \\nabla_{\\theta} \\log p\\left(y ; \\theta\\right) &amp;= \\frac {\\nabla_{\\theta} p(y ; \\theta)}{p(y ; \\theta)} \\ \\ \\mathbb{E}{y \\sim p(y ; \\theta)}\\left[\\left.\\nabla\\right)\\right|}} \\log p\\left(y ; \\theta^{\\prime{\\theta^{\\prime}=\\theta}\\right] &amp;= \\int\\right)\\right|}^{\\infty} p(y) \\left.\\nabla_{\\theta^{\\prime}} \\log p\\left(y ; \\theta^{\\prime{\\theta^{\\prime}=\\theta} d y\\ &amp;= \\int d y \\ &amp;= \\int_{-\\infty}^{\\infty} \\nabla_{\\theta} p(y ; \\theta) d y\\ &amp;= 0 \\ \\end{aligned} $$}^{\\infty} p(y ; \\theta)\\frac {\\nabla_{\\theta} p(y ; \\theta)}{p(y ; \\theta)</p>"},{"location":"CS/ML/CS229_Problem_Sets/#b-fisher-information","title":"b) Fisher information","text":"<p>Fisher information represents the amount of information that a random variable Y carries about a parameter \u03b8 of interest.</p> <p>From score function, we know that, $$ \\begin{aligned} \\mathbb{E}{y \\sim p(y ; \\theta)}\\left[\\left.\\nabla\\right)\\right|}} \\log p\\left(y ; \\theta^{\\prime{\\theta^{\\prime}=\\theta}\\right] = 0  \\end{aligned} $$ So we can derive the following: $$ \\begin{aligned} \\mathcal{I}(\\theta)&amp;= \\operatorname{Cov}\\right)\\right|}\\left[\\left.\\nabla_{\\theta^{\\prime}} \\log p\\left(y ; \\theta^{\\prime{\\theta^{\\prime}=\\theta}\\right]\\ &amp;= \\mathbb{E}\\right|}\\left[\\left.\\nabla_{\\theta^{\\prime}} \\log p\\left(y ; \\theta^{\\prime}\\right) \\nabla_{\\theta^{\\prime}} \\log p\\left(y ; \\theta^{\\prime}\\right)^{T{\\theta^{\\prime} = \\theta}\\right] -  \\mathbb{E}\\right) \\right|}\\left[\\left.\\nabla_{\\theta^{\\prime}} \\log p\\left(y;\\theta^{\\prime{\\theta^{\\prime} = \\theta}\\right] \\mathbb{E}\\right|} \\left[\\left.\\nabla_{\\theta^{\\prime}} \\log p\\left(y ; \\theta^{\\prime}\\right)^{T{\\theta^{\\prime}=\\theta}\\right]\\ &amp;=\\mathbb{E}\\right|}\\left[\\left.\\nabla_{\\theta^{\\prime}} \\log p\\left(y ; \\theta^{\\prime}\\right) \\nabla_{\\theta^{\\prime}} \\log p\\left(y ; \\theta^{\\prime}\\right)^{T{\\theta^{\\prime}=\\theta}\\right]  -0 \\ &amp;=\\mathbb{E}\\right] \\ \\end{aligned} $$}\\left[\\left.\\nabla_{\\theta^{\\prime}} \\log p\\left(y ; \\theta^{\\prime}\\right) \\nabla_{\\theta^{\\prime}} \\log p\\left(y ; \\theta^{\\prime}\\right)^{T}\\right|_{\\theta^{\\prime}=\\theta</p>"},{"location":"CS/ML/CS229_Problem_Sets/#c-fisher-information-alternate-form","title":"c)  ==Fisher Information (alternate form)==","text":"\\[ \\begin{aligned} \\mathbb{E}_{y \\sim p(y ; \\theta)}\\left[-\\left.\\nabla_{\\theta^{\\prime}}^{2} \\log p\\left(y ; \\theta^{\\prime}\\right)\\right|_{\\theta^{\\prime}=\\theta}\\right]=\\mathcal{I}(\\theta) \\end{aligned} \\]"},{"location":"CS/ML/CS229_Problem_Sets/#d-approximating-d_kl-with-fisher-information","title":"d) Approximating \\(D_{KL}\\) with Fisher Information","text":"<p>Make \\(\\overset \\sim\\theta = \\theta +d\\), Then Taylor expansion for \\(\\log p\\left(\\overset \\sim\\theta \\right)\\) is  $$ \\log p\\left(\\overset \\sim\\theta \\right) \\approx \\log p\\left(\\theta \\right)+d^T \\left.\\nabla_{\\theta ^\\prime} \\log p(\\theta ^\\prime) \\right|{\\theta ^\\prime = \\theta}  +\\frac{1}{2}d^T  \\left.\\nablad $$}^2 \\log p(\\theta ^\\prime) \\right|_{\\theta ^\\prime = \\theta</p> <p>So we have $$ \\begin{align} D_{\\mathrm{KL}}\\left(p_{\\theta} | p_{\\theta+d}\\right) &amp;= \\sum_y p_\\theta \\log p_\\theta -\\sum_y p_\\theta \\log p_{\\theta+d}\\ &amp;\\approx \\sum_y p_\\theta \\log p_\\theta -\\sum_y p_\\theta \\left(\\log p\\left(\\theta \\right)+d^T \\left.\\nabla_{\\theta ^\\prime} \\log p(\\theta ^\\prime) \\right|{\\theta ^\\prime = \\theta}  +\\frac{1}{2}d^T  \\left.\\nabla^2 \\log p(\\theta ^\\prime) \\right|{\\theta ^\\prime = \\theta}d\\right) \\ &amp;= -\\sum_y p\\theta d^T \\left.\\nabla_{\\theta ^\\prime} \\log p(\\theta ^\\prime) \\right|{\\theta ^\\prime = \\theta} -\\sum_y p\\theta  \\frac{1}{2}d^T  \\left.\\nabla_{\\theta ^\\prime}^2 \\log p(\\theta ^\\prime) \\right|{\\theta ^\\prime = \\theta}d \\ &amp;= - d^T \\sum_y p\\theta \\left.\\nabla_{\\theta ^\\prime} \\log p(\\theta ^\\prime) \\right|{\\theta ^\\prime = \\theta} +  \\frac{1}{2}d^T  \\left(\\sum_y p\\theta \\left.\\nabla_{\\theta ^\\prime}^2  -\\log p(\\theta ^\\prime) \\right|{\\theta ^\\prime = \\theta} \\right)d \\ &amp;= 0 + \\frac{1}{2}d^T \\mathbb{E}\\right)\\right|}\\left[-\\left.\\nabla_{\\theta^{\\prime}}^{2} \\log p\\left(y ; \\theta^{\\prime{\\theta^{\\prime}=\\theta}\\right] d\\ &amp;= \\frac{1}{2} d^{T} \\mathcal{I}(\\theta) d \\ \\ D(\\theta) d  \\end{align} $$}}\\left(p_{\\theta} | p_{\\theta+d}\\right) &amp;\\approx  \\frac{1}{2} d^{T} \\mathcal{I</p>"},{"location":"CS/ML/CS229_Problem_Sets/#e-natural-gradient","title":"e) ==Natural Gradient==","text":"\\[ \\begin{array}{l} \\ell(\\theta+d) \\approx \\ell(\\theta)+\\left.d^{T} \\nabla_{\\theta^{\\prime}} \\ell\\left(\\theta^{\\prime}\\right)\\right|_{\\theta^{\\prime}=\\theta}\\\\ =\\log p(y ; \\theta)+\\left.d^{T} \\nabla_{\\theta^{\\prime}} \\log p\\left(y ; \\theta^{\\prime}\\right)\\right|_{\\theta^{\\prime}=\\theta}\\\\ =\\log p(y ; \\theta)+d^{T} \\frac{\\left.\\nabla_{\\theta^{\\prime}} p\\left(y ; \\theta^{\\prime}\\right)\\right|_{\\theta^{\\prime}=\\theta}}{p(y ; \\theta)}\\\\ D_{\\mathrm{KL}}\\left(p_{\\theta} \\| p_{\\theta+d}\\right) \\approx \\frac{1}{2} d^{T} \\mathcal{I}(\\theta) d\\\\ \\mathcal{L}(d, \\lambda)=\\ell(\\theta+d)-\\lambda\\left[D_{\\mathrm{KL}}\\left(p_{\\theta} \\| p_{\\theta+d}\\right)-c\\right]\\\\ \\approx \\log p(y ; \\theta)+d^{T} \\frac{\\left.\\nabla_{\\theta^{\\prime}} p\\left(y ; \\theta^{\\prime}\\right)\\right|_{\\theta^{\\prime}=\\theta}}{p(y ; \\theta)}-\\lambda\\left[\\frac{1}{2} d^{T} \\mathcal{I}(\\theta) d-c\\right]\\\\ \\nabla_{d} \\mathcal{L}(d, \\lambda) \\approx \\frac{\\left.\\nabla_{\\theta^{\\prime}} p\\left(y ; \\theta^{\\prime}\\right)\\right|_{\\theta^{\\prime}=\\theta}}{p(y ; \\theta)}-\\lambda \\mathcal{I}(\\theta) d=0\\\\ \\tilde{d}=\\frac{1}{\\lambda} \\mathcal{I}(\\theta)^{-1} \\frac{\\left.\\nabla_{\\theta^{\\prime}} p\\left(y ; \\theta^{\\prime}\\right)\\right|_{\\theta^{\\prime}=\\theta}}{p(y ; \\theta)}\\\\ \\nabla_{\\lambda} \\mathcal{L}(d, \\lambda) \\approx c-\\frac{1}{2} d^{T} \\mathcal{I}(\\theta) d\\\\ =c-\\frac{1}{2} \\cdot \\frac{1}{\\lambda} \\frac{\\left.\\nabla_{\\theta^{\\prime}} p\\left(y ; \\theta^{\\prime}\\right)\\right|_{\\theta^{\\prime}=\\theta}{ }^{T}}{p(y ; \\theta)} \\mathcal{I}(\\theta)^{-1} \\cdot \\mathcal{I}(\\theta) \\cdot \\frac{1}{\\lambda} \\mathcal{I}(\\theta)^{-1} \\frac{\\left.\\nabla_{\\theta^{\\prime}} p\\left(y ; \\theta^{\\prime}\\right)\\right|_{\\theta^{\\prime}=\\theta}}{p(y ; \\theta)}\\\\ =c-\\left.\\left.\\frac{1}{2 \\lambda^{2}(p(y ; \\theta))^{2}} \\nabla_{\\theta^{\\prime}} p\\left(y ; \\theta^{\\prime}\\right)\\right|_{\\theta^{\\prime}=\\theta}{ }^{T} \\mathcal{I}(\\theta)^{-1} \\nabla_{\\theta^{\\prime}} p\\left(y ; \\theta^{\\prime}\\right)\\right|_{\\theta^{\\prime}=\\theta}\\\\ =0\\\\ \\lambda=\\sqrt{\\left.\\left.\\frac{1}{2 c(p(y ; \\theta))^{2}} \\nabla_{\\theta^{\\prime}} p\\left(y ; \\theta^{\\prime}\\right)\\right|_{\\theta^{\\prime}=\\theta} ^{T} \\mathcal{I}(\\theta)^{-1} \\nabla_{\\theta^{\\prime}} p\\left(y ; \\theta^{\\prime}\\right)\\right|_{\\theta^{\\prime}=\\theta}}\\\\ d^{*}=\\sqrt{\\frac{2 c(p(y ; \\theta))^{2}}{\\left.\\left.\\nabla_{\\theta^{\\prime}} p\\left(y ; \\theta^{\\prime}\\right)\\right|_{\\theta^{\\prime}=\\theta} ^{T} \\mathcal{I}(\\theta)^{-1} \\nabla_{\\theta^{\\prime}} p\\left(y ; \\theta^{\\prime}\\right)\\right|_{\\theta^{\\prime}=\\theta}}} \\mathcal{I}(\\theta)^{-1} \\frac{\\left.\\nabla_{\\theta^{\\prime}} p\\left(y ; \\theta^{\\prime}\\right)\\right|_{\\theta^{\\prime}=\\theta}}{p(y ; \\theta)}\\\\ =\\left.\\sqrt{\\frac{2 c}{\\left.\\left.\\nabla_{\\theta^{\\prime}} p\\left(y ; \\theta^{\\prime}\\right)\\right|_{\\theta^{\\prime}=\\theta} ^{T} \\mathcal{I}(\\theta)^{-1} \\nabla_{\\theta^{\\prime}} p\\left(y ; \\theta^{\\prime}\\right)\\right|_{\\theta^{\\prime}=\\theta}}} \\mathcal{I}(\\theta)^{-1} \\nabla_{\\theta^{\\prime}} p\\left(y ; \\theta^{\\prime}\\right)\\right|_{\\theta^{\\prime}=\\theta} \\end{array} \\]"},{"location":"CS/ML/CS229_Problem_Sets/#f-relation-to-newtons-method","title":"f) ==Relation to Newton\u2019s Method==","text":"<p>For natural gradient $$ \\tilde{d}=\\frac{1}{\\lambda} \\mathcal{I}(\\theta)^{-1} \\frac{\\left.\\nabla_{\\theta^{\\prime}} p\\left(y ; \\theta^{\\prime}\\right)\\right|{\\theta^{\\prime}=\\theta}}{p(y ; \\theta)} $$ For Newton\u2019s Method $$ \\begin{align} \\theta: &amp; = \\theta-H^{-1} \\nabla \\ell(\\theta) \\end{align} $$ Then, $$ \\begin{aligned} \\mathcal{I}(\\theta) &amp;=\\mathbb{E}{y \\sim p(y ; \\theta)}\\left[-\\left.\\nabla\\right)\\right|}}^{2} \\log p\\left(y ; \\theta^{\\prime{\\theta^{\\prime}=\\theta}\\right] \\ &amp;=\\mathbb{E} \\ell(\\theta)\\right] \\ &amp;=-\\mathbb{E}}\\left[-\\nabla_{\\theta}^{2{y \\sim p(y ; \\theta)}[H] \\ \\theta: &amp;=\\theta+\\tilde{d} \\ &amp;=\\theta+\\frac{1}{\\lambda} \\mathcal{I}(\\theta)^{-1} \\nabla \\ell(\\theta) \\ &amp;=\\theta-\\frac{1}{\\lambda} \\mathbb{E}{y \\sim p(y ; \\theta)}[H]^{-1} \\nabla \\ell(\\theta) \\end{aligned} $$</p>"},{"location":"CS/ML/CS229_Problem_Sets/#problem-4-semi-supervised-em","title":"Problem 4 Semi-supervised EM","text":""},{"location":"CS/ML/CS229_Problem_Sets/#a_4","title":"a)","text":"\\[ \\begin{aligned} \\ell_{\\text {semi-sup }}\\left(\\theta^{(t+1)}\\right) &amp;=\\ell_{\\text {unsup }}\\left(\\theta^{(t+1)}\\right)+\\alpha \\ell_{\\text {sup }}\\left(\\theta^{(t+1)}\\right) \\\\ &amp; \\geq \\sum_{i=1}^{m}\\left(\\sum_{z^{(i)}} Q_{i}^{(t)}\\left(z^{(i)}\\right) \\log \\frac{p\\left(x^{(i)}, z^{(i)} ; \\theta^{(t+1)}\\right)}{Q_{i}^{(t)}\\left(z^{(i)}\\right)}\\right)+\\alpha\\left(\\sum_{i=1}^{\\tilde{m}} \\log p\\left(\\tilde{x}^{(i)}, \\tilde{z}^{(i)} ; \\theta^{(t+1)}\\right)\\right) \\\\ &amp; \\geq \\sum_{i=1}^{m}\\left(\\sum_{z^{(i)}} Q_{i}^{(t)}\\left(z^{(i)}\\right) \\log \\frac{p\\left(x^{(i)}, z^{(i)} ; \\theta^{(t)}\\right)}{Q_{i}^{(t)}\\left(z^{(i)}\\right)}\\right)+\\alpha\\left(\\sum_{i=1}^{\\tilde{m}} \\log p\\left(\\tilde{x}^{(i)}, \\tilde{z}^{(i)} ; \\theta^{(t)}\\right)\\right) \\\\ &amp;=\\ell_{\\text {unsup }}\\left(\\theta^{(t)}\\right)+\\alpha \\ell_{\\text {sup }}\\left(\\theta^{(t)}\\right) \\\\ &amp;=\\ell_{\\text {semi-sup }}\\left(\\theta^{(t)}\\right) \\end{aligned} \\]"},{"location":"CS/ML/CS229_Problem_Sets/#b-semi-supervised-e-step","title":"b) Semi-supervised E-step","text":"\\[ \\begin{aligned} w_{j}^{(i)}&amp;=p\\left(z^{(i)}=j \\mid x^{(i)} ; \\phi, \\mu, \\Sigma\\right) \\\\ &amp;= \\frac{p\\left(x^{(i)} \\mid z^{(i)}=j ; \\mu, \\Sigma\\right) p\\left(z^{(i)}=j ; \\phi\\right)}{\\sum_{l=1}^{k} p\\left(x^{(i)} \\mid z^{(i)}=l ; \\mu, \\Sigma\\right) p\\left(z^{(i)}=l ; \\phi\\right)} \\\\ &amp;= \\frac {\\frac{1}{(2 \\pi)^{n / 2}|\\Sigma_j|^{1 / 2}} \\exp \\left(-\\frac{1}{2}(x-\\mu_j)^{T} \\Sigma_j^{-1}(x-\\mu_j)\\right) \\phi_j} {\\sum_{l=1}^{k} \\frac {1}{(2 \\pi)^{n / 2}|\\Sigma_l|^{1 / 2}} \\exp \\left(-\\frac{1}{2}(x-\\mu_l)^{T} \\Sigma_l^{-1}(x-\\mu_l)\\right) \\phi_l} \\\\ \\end{aligned} \\] <p>Appendix: $$ \\begin{aligned} p\\left(x^{(i)} \\mid z^{(i)}=j ; \\mu, \\Sigma\\right) &amp;= \\frac{1}{(2 \\pi)^{n / 2}|\\Sigma|^{1 / 2}} \\exp \\left(-\\frac{1}{2}(x-\\mu)^{T} \\Sigma^{-1}(x-\\mu)\\right)\\ p\\left(z^{(i)}=j ; \\phi\\right) &amp;= \\phi _j \\end{aligned} $$</p>"},{"location":"CS/ML/CS229_Problem_Sets/#c-semi-supervised-m-step","title":"c) ==Semi-supervised M-step==","text":"\\[ \\begin{aligned} \\phi_{j} &amp; = \\frac{\\sum_{i = 1}^{m} w_{j}^{(i)}+\\alpha \\sum_{i = 1}^{\\tilde{m}} \\mathbb I\\left\\{\\tilde{z}^{(i)}  = j\\right\\}}{m+\\alpha \\tilde{m}} \\\\ \\mu_{j} &amp;=\\frac{\\sum_{i=1}^{m} w_{j}^{(i)} x^{(i)} + \\alpha \\sum_{i=1}^{\\tilde m} \\mathbb I\\{\\tilde z^{(i)} = j \\} \\tilde x^{(i)}}{\\sum_{i=1}^{m} w_{j}^{(i)} + \\alpha \\sum_{i=1}^{\\tilde m} \\mathbb I\\{\\tilde z^{(i)} = j \\}} \\\\ \\Sigma_{j} &amp;=\\frac{\\sum_{i=1}^{m} w_{j}^{(i)}\\left(x^{(i)}-\\mu_{j}\\right)\\left(x^{(i)}-\\mu_{j}\\right)^{T} + \\alpha \\sum_{i=1}^{\\tilde m} \\mathbb I\\{\\tilde z^{(i)} = j \\} \\left(\\tilde x^{(i)}-\\mu_{j}\\right)\\left(\\tilde x^{(i)}-\\mu_{j}\\right)^{T}}{\\sum_{i=1}^{m} w_{j}^{(i)}+\\alpha \\sum_{i=1}^{\\tilde m} \\mathbb I\\{\\tilde z^{(i)} = j \\}}\\\\  \\end{aligned} \\]"},{"location":"CS/ML/CS229_Problem_Sets/#d-classical-unsupervised-em-implementation","title":"d) Classical (Unsupervised) EM Implementation","text":"<p>Initialize Dataset</p> <pre><code>def main(is_semi_supervised, trial_num):\n    \"\"\"Problem 3: EM for Gaussian Mixture Models (unsupervised and semi-supervised)\"\"\"\n    print('Running {} EM algorithm...'\n          .format('semi-supervised' if is_semi_supervised else 'unsupervised'))\n\n    # Load dataset\n    train_path = os.path.join('..', 'data', 'ds3_train.csv')\n    x, z = load_gmm_dataset(train_path)\n    x_tilde = None\n\n    if is_semi_supervised:\n        # Split into labeled and unlabeled examples\n        labeled_idxs = (z != UNLABELED).squeeze()\n        x_tilde = x[labeled_idxs, :]   # Labeled examples\n        z = z[labeled_idxs, :]         # Corresponding labels\n        x = x[~labeled_idxs, :]        # Unlabeled examples\n\n    # *** START CODE HERE ***\n    # (1) Initialize mu and sigma by splitting the m data points uniformly at random\n    # into K groups, then calculating the sample mean and covariance for each group\n    # Split data\n    m, n = x.shape\n    row_indices = np.random.permutation(m)\n    partitionLen = int(m / K)\n    # initialize mu and sigma\n    mu, sigma = np.zeros((K, n)), np.zeros((K, n, n))\n    for i in range(K):\n        x_samp = x[row_indices[i * partitionLen:(i + 1) * partitionLen]]\n        mu[i] = np.mean(x_samp, axis=0)\n        sigma[i] = np.cov(x_samp.T)\n    # (2) Initialize phi to place equal probability on each Gaussian\n    # phi should be a numpy array of shape (K,)\n    phi = 1 / K * np.ones(K)\n    # (3) Initialize the w values to place equal probability on each Gaussian\n    # w should be a numpy array of shape (m, K)\n    w = 1 / K * np.ones((m, K))\n    # *** END CODE HERE ***\n\n    if is_semi_supervised:\n        w = run_semi_supervised_em(x, x_tilde, z, w, phi, mu, sigma)\n    else:\n        w = run_em(x, w, phi, mu, sigma)\n\n    # Plot your predictions\n    z_pred = np.zeros(m)\n    if w is not None:  # Just a placeholder for the starter code\n        for i in range(m):\n            z_pred[i] = np.argmax(w[i])\n\n    plot_gmm_preds(x, z_pred, is_semi_supervised, plot_id=trial_num)\n</code></pre> <p>EM Algorithm</p> <pre><code>def run_em(x, w, phi, mu, sigma):\n    \"\"\"Problem 3(d): EM Algorithm (unsupervised).\n\n    See inline comments for instructions.\n\n    Args:\n        x: Design matrix of shape (m, n).\n        w: Initial weight matrix of shape (m, k).\n        phi: Initial mixture prior, of shape (k,).\n        mu: Initial cluster means, list of k arrays of shape (n,).\n        sigma: Initial cluster covariances, list of k arrays of shape (n, n).\n\n    Returns:\n        Updated weight matrix of shape (m, k) resulting from EM algorithm.\n        More specifically, w[i, j] should contain the probability of\n        example x^(i) belonging to the j-th Gaussian in the mixture.\n    \"\"\"\n    m, n = x.shape\n    # No need to change any of these parameters\n    eps = 1e-3  # Convergence threshold\n    max_iter = 1000\n\n    # Stop when the absolute change in log-likelihood is &lt; eps\n    # See below for explanation of the convergence criterion\n    it = 0\n    ll = prev_ll = None\n    while it &lt; max_iter and (prev_ll is None or np.abs(ll - prev_ll) &gt;= eps):\n        # Just a placeholder for the starter code\n        # *** START CODE HERE\n        # (1) E-step: Update your estimates in w\n        # (2) M-step: Update the model parameters phi, mu, and sigma\n        # (3) Compute the log-likelihood of the data to check for convergence.\n        # By log-likelihood, we mean `ll = sum_x[log(sum_z[p(x|z) * p(z)])]`.\n        # We define convergence by the first iteration where abs(ll - prev_ll) &lt; eps.\n        # Hint: For debugging, recall part (a). We showed that ll should be monotonically increasing.\n\n        # (1) E-step:\n        # it += 1\n        prev_ll = ll\n        for i in range(K):\n            w[:, i] = np.exp(-0.5 * ((x - mu[i]).dot(np.linalg.inv(sigma[i]))*(x - mu[i])).sum(axis=1))*phi[i] / np.sqrt(np.linalg.det(sigma[i]))\n        ll = np.sum(np.log(w))\n        w /= np.sum(w, axis=1)[:, None]\n        # (2) M-step\n        phi = np.sum(w, axis=0)/m\n        for i in range(K):\n            mu[i] = x.T.dot(w[:, i]) / np.sum(w[:, i])\n            sigma[i] = ((x-mu[i]).T*w[:, i]).dot(x-mu[i]) / np.sum(w[:, i])\n        it += 1\n        # *** END CODE HERE ***\n    print(f'Number of iterations:{it}')\n    return w\n</code></pre>"},{"location":"CS/ML/CS229_Problem_Sets/#e-semi-supervised-em","title":"e) Semi-supervised EM","text":"<pre><code>def run_semi_supervised_em(x, x_tilde, z, w, phi, mu, sigma):\n    \"\"\"Problem 3(e): Semi-Supervised EM Algorithm.\n\n    See inline comments for instructions.\n\n    Args:\n        x: Design matrix of unlabeled examples of shape (m, n).\n        x_tilde: Design matrix of labeled examples of shape (m_tilde, n).\n        z: Array of labels of shape (m_tilde, 1).\n        w: Initial weight matrix of shape (m, k).\n        phi: Initial mixture prior, of shape (k,).\n        mu: Initial cluster means, list of k arrays of shape (n,).\n        sigma: Initial cluster covariances, list of k arrays of shape (n, n).\n\n    Returns:\n        Updated weight matrix of shape (m, k) resulting from semi-supervised EM algorithm.\n        More specifically, w[i, j] should contain the probability of\n        example x^(i) belonging to the j-th Gaussian in the mixture.\n    \"\"\"\n    # No need to change any of these parameters\n    alpha = 20.  # Weight for the labeled examples\n    eps = 1e-3   # Convergence threshold\n    max_iter = 1000\n\n    m, n = x.shape\n    m_tilde = x_tilde.shape[0]\n    # Stop when the absolute change in log-likelihood is &lt; eps\n    # See below for explanation of the convergence criterion\n    it = 0\n    ll = prev_ll = None\n    while it &lt; max_iter and (prev_ll is None or np.abs(ll - prev_ll) &gt;= eps):\n        pass  # Just a placeholder for the starter code\n        # *** START CODE HERE ***\n        # (1) E-step: Update your estimates in w\n        # (2) M-step: Update the model parameters phi, mu, and sigma\n        # (3) Compute the log-likelihood of the data to check for convergence.\n        # Hint: Make sure to include alpha in your calculation of ll.\n        # Hint: For debugging, recall part (a). We showed that ll should be monotonically increasing.\n        # (1) E-step:\n        # it += 1\n        prev_ll = ll\n        for i in range(K):\n            w[:, i] = np.exp(-0.5 * ((x - mu[i]).dot(np.linalg.inv(sigma[i])) * (x - mu[i])).sum(axis=1)) * phi[\n                i] / np.sqrt(np.linalg.det(sigma[i]))\n        ll = np.sum(np.log(w))\n        w /= np.sum(w, axis=1)[:, None]\n        # (2) M-step\n        z_tilde_indi = np.zeros((m_tilde, K))\n        for i in range(K):\n            z_tilde_indi[:, i] = [1. if z_ele == i else 0 for z_ele in z]\n        phi = (np.sum(w, axis=0) + alpha*z_tilde_indi.sum(axis=0)) / (m+alpha*m_tilde)\n        for i in range(K):\n            mu[i] = (x.T.dot(w[:, i]) + alpha*x_tilde.T.dot(z_tilde_indi[:, i])) / (np.sum(w[:, i])+alpha*np.sum(z_tilde_indi[:, i]))\n            sigma[i] = (((x - mu[i]).T * w[:, i]).dot(x - mu[i]) + alpha*((x_tilde - mu[i]).T * z_tilde_indi[:, i]).dot(x_tilde - mu[i])) \\\n                       / (np.sum(w[:, i])+alpha*np.sum(z_tilde_indi[:, i]))\n        it += 1\n\n    print(f'Number of iterations:{it}')\n    # *** END CODE HERE ***\n\n    return w\n</code></pre>"},{"location":"CS/ML/CS229_Problem_Sets/#f-comparison","title":"f) Comparison","text":"<p><code>Running unsupervised EM algorithm...    Number of iterations:1000    Running semi-supervised EM algorithm...    Number of iterations:52    Running unsupervised EM algorithm...    Number of iterations:1000    Running semi-supervised EM algorithm...    Number of iterations:58    Running unsupervised EM algorithm...    Number of iterations:1000    Running semi-supervised EM algorithm...    Number of iterations:53</code></p> <ol> <li>Stability</li> </ol> <p>~~Semi-supervised EM is perfectly stable, whereas classical EM isn't so satisfactory.~~</p> <p>Semi-supervised EM are more stable than unsupervised EM. The assignments by unsupervised EM are random with different random initializations. But the assignments by semi-supervised EM are the same.</p> <ol> <li>Overall quality of assignments</li> </ol> <p>~~Classical EM performs poorly while high-variance Gaussian is mixed into dataset.~~</p> <p>The overall quality of assignments by semi-supervised EM are higher than unsupervised EM. </p> <p>In the pictures of semi-supervised EM, there are three nearly the same low-variance Gaussian distributions, and a high-variance Gaussian distribution.</p> <p>In the pictures of unsupervised EM, there are four Gaussian distributions which variances are different.</p> <p>p03_gmm.py</p> <pre><code>import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nPLOT_COLORS = ['red', 'green', 'blue', 'orange']  # Colors for your plots\nK = 4           # Number of Gaussians in the mixture model\nNUM_TRIALS = 3  # Number of trials to run (can be adjusted for debugging)\nUNLABELED = -1  # Cluster label for unlabeled data points (do not change)\n\n\ndef main(is_semi_supervised, trial_num):\n    \"\"\"Problem 3: EM for Gaussian Mixture Models (unsupervised and semi-supervised)\"\"\"\n    print('Running {} EM algorithm...'\n          .format('semi-supervised' if is_semi_supervised else 'unsupervised'))\n\n    # Load dataset\n    train_path = os.path.join('..', 'data', 'ds3_train.csv')\n    x, z = load_gmm_dataset(train_path)\n    x_tilde = None\n\n    if is_semi_supervised:\n        # Split into labeled and unlabeled examples\n        labeled_idxs = (z != UNLABELED).squeeze()\n        x_tilde = x[labeled_idxs, :]   # Labeled examples\n        z = z[labeled_idxs, :]         # Corresponding labels\n        x = x[~labeled_idxs, :]        # Unlabeled examples\n\n    # *** START CODE HERE ***\n    # (1) Initialize mu and sigma by splitting the m data points uniformly at random\n    # into K groups, then calculating the sample mean and covariance for each group\n    # Split data\n    m, n = x.shape\n    row_indices = np.random.permutation(m)\n    partitionLen = int(m / K)\n    # initialize mu and sigma\n    mu, sigma = np.zeros((K, n)), np.zeros((K, n, n))\n    for i in range(K):\n        x_samp = x[row_indices[i * partitionLen:(i + 1) * partitionLen]]\n        mu[i] = np.mean(x_samp, axis=0)\n        sigma[i] = np.cov(x_samp.T)\n    # (2) Initialize phi to place equal probability on each Gaussian\n    # phi should be a numpy array of shape (K,)\n    phi = 1 / K * np.ones(K)\n    # (3) Initialize the w values to place equal probability on each Gaussian\n    # w should be a numpy array of shape (m, K)\n    w = 1 / K * np.ones((m, K))\n    # *** END CODE HERE ***\n\n    if is_semi_supervised:\n        w = run_semi_supervised_em(x, x_tilde, z, w, phi, mu, sigma)\n    else:\n        w = run_em(x, w, phi, mu, sigma)\n\n    # Plot your predictions\n    z_pred = np.zeros(m)\n    if w is not None:  # Just a placeholder for the starter code\n        for i in range(m):\n            z_pred[i] = np.argmax(w[i])\n\n    plot_gmm_preds(x, z_pred, is_semi_supervised, plot_id=trial_num)\n\n\ndef run_em(x, w, phi, mu, sigma):\n    \"\"\"Problem 3(d): EM Algorithm (unsupervised).\n\n    See inline comments for instructions.\n\n    Args:\n        x: Design matrix of shape (m, n).\n        w: Initial weight matrix of shape (m, k).\n        phi: Initial mixture prior, of shape (k,).\n        mu: Initial cluster means, list of k arrays of shape (n,).\n        sigma: Initial cluster covariances, list of k arrays of shape (n, n).\n\n    Returns:\n        Updated weight matrix of shape (m, k) resulting from EM algorithm.\n        More specifically, w[i, j] should contain the probability of\n        example x^(i) belonging to the j-th Gaussian in the mixture.\n    \"\"\"\n    m, n = x.shape\n    # No need to change any of these parameters\n    eps = 1e-3  # Convergence threshold\n    max_iter = 1000\n\n    # Stop when the absolute change in log-likelihood is &lt; eps\n    # See below for explanation of the convergence criterion\n    it = 0\n    ll = prev_ll = None\n    while it &lt; max_iter and (prev_ll is None or np.abs(ll - prev_ll) &gt;= eps):\n        # Just a placeholder for the starter code\n        # *** START CODE HERE\n        # (1) E-step: Update your estimates in w\n        # (2) M-step: Update the model parameters phi, mu, and sigma\n        # (3) Compute the log-likelihood of the data to check for convergence.\n        # By log-likelihood, we mean `ll = sum_x[log(sum_z[p(x|z) * p(z)])]`.\n        # We define convergence by the first iteration where abs(ll - prev_ll) &lt; eps.\n        # Hint: For debugging, recall part (a). We showed that ll should be monotonically increasing.\n\n        # (1) E-step:\n        # it += 1\n        prev_ll = ll\n        for i in range(K):\n            w[:, i] = np.exp(-0.5 * ((x - mu[i]).dot(np.linalg.inv(sigma[i]))*(x - mu[i])).sum(axis=1))*phi[i] / np.sqrt(np.linalg.det(sigma[i]))\n        ll = np.sum(np.log(w))\n        w /= np.sum(w, axis=1)[:, None]\n        # (2) M-step\n        phi = np.sum(w, axis=0)/m\n        for i in range(K):\n            mu[i] = x.T.dot(w[:, i]) / np.sum(w[:, i])\n            sigma[i] = ((x-mu[i]).T*w[:, i]).dot(x-mu[i]) / np.sum(w[:, i])\n        it += 1\n        # *** END CODE HERE ***\n    print(f'Number of iterations:{it}')\n    return w\n\n\ndef run_semi_supervised_em(x, x_tilde, z, w, phi, mu, sigma):\n    \"\"\"Problem 3(e): Semi-Supervised EM Algorithm.\n\n    See inline comments for instructions.\n\n    Args:\n        x: Design matrix of unlabeled examples of shape (m, n).\n        x_tilde: Design matrix of labeled examples of shape (m_tilde, n).\n        z: Array of labels of shape (m_tilde, 1).\n        w: Initial weight matrix of shape (m, k).\n        phi: Initial mixture prior, of shape (k,).\n        mu: Initial cluster means, list of k arrays of shape (n,).\n        sigma: Initial cluster covariances, list of k arrays of shape (n, n).\n\n    Returns:\n        Updated weight matrix of shape (m, k) resulting from semi-supervised EM algorithm.\n        More specifically, w[i, j] should contain the probability of\n        example x^(i) belonging to the j-th Gaussian in the mixture.\n    \"\"\"\n    # No need to change any of these parameters\n    alpha = 20.  # Weight for the labeled examples\n    eps = 1e-3   # Convergence threshold\n    max_iter = 1000\n\n    m, n = x.shape\n    m_tilde = x_tilde.shape[0]\n    # Stop when the absolute change in log-likelihood is &lt; eps\n    # See below for explanation of the convergence criterion\n    it = 0\n    ll = prev_ll = None\n    while it &lt; max_iter and (prev_ll is None or np.abs(ll - prev_ll) &gt;= eps):\n        pass  # Just a placeholder for the starter code\n        # *** START CODE HERE ***\n        # (1) E-step: Update your estimates in w\n        # (2) M-step: Update the model parameters phi, mu, and sigma\n        # (3) Compute the log-likelihood of the data to check for convergence.\n        # Hint: Make sure to include alpha in your calculation of ll.\n        # Hint: For debugging, recall part (a). We showed that ll should be monotonically increasing.\n        # (1) E-step:\n        # it += 1\n        prev_ll = ll\n        for i in range(K):\n            w[:, i] = np.exp(-0.5 * ((x - mu[i]).dot(np.linalg.inv(sigma[i])) * (x - mu[i])).sum(axis=1)) * phi[\n                i] / np.sqrt(np.linalg.det(sigma[i]))\n        ll = np.sum(np.log(w))\n        w /= np.sum(w, axis=1)[:, None]\n        # (2) M-step\n        z_tilde_indi = np.zeros((m_tilde, K))\n        for i in range(K):\n            z_tilde_indi[:, i] = [1. if z_ele == i else 0 for z_ele in z]\n        phi = (np.sum(w, axis=0) + alpha*z_tilde_indi.sum(axis=0)) / (m+alpha*m_tilde)\n        for i in range(K):\n            mu[i] = (x.T.dot(w[:, i]) + alpha*x_tilde.T.dot(z_tilde_indi[:, i])) / (np.sum(w[:, i])+alpha*np.sum(z_tilde_indi[:, i]))\n            sigma[i] = (((x - mu[i]).T * w[:, i]).dot(x - mu[i]) + alpha*((x_tilde - mu[i]).T * z_tilde_indi[:, i]).dot(x_tilde - mu[i])) \\\n                       / (np.sum(w[:, i])+alpha*np.sum(z_tilde_indi[:, i]))\n        it += 1\n\n    print(f'Number of iterations:{it}')\n    # *** END CODE HERE ***\n\n    return w\n\n\n# *** START CODE HERE ***\n# Helper functions\n# *** END CODE HERE ***\n\n\ndef plot_gmm_preds(x, z, with_supervision, plot_id):\n    \"\"\"Plot GMM predictions on a 2D dataset `x` with labels `z`.\n\n    Write to the output directory, including `plot_id`\n    in the name, and appending 'ss' if the GMM had supervision.\n\n    NOTE: You do not need to edit this function.\n    \"\"\"\n    plt.figure(figsize=(12, 8))\n    plt.title('{} GMM Predictions'.format('Semi-supervised' if with_supervision else 'Unsupervised'))\n    plt.xlabel('x_1')\n    plt.ylabel('x_2')\n\n    for x_1, x_2, z_ in zip(x[:, 0], x[:, 1], z):\n        color = 'gray' if z_ &lt; 0 else PLOT_COLORS[int(z_)]\n        alpha = 0.25 if z_ &lt; 0 else 0.75\n        plt.scatter(x_1, x_2, marker='.', c=color, alpha=alpha)\n\n    file_name = 'p03_pred{}_{}.pdf'.format('_ss' if with_supervision else '', plot_id)\n    save_path = os.path.join('output', file_name)\n    plt.savefig(save_path)\n\n\ndef load_gmm_dataset(csv_path):\n    \"\"\"Load dataset for Gaussian Mixture Model (problem 3).\n\n    Args:\n         csv_path: Path to CSV file containing dataset.\n\n    Returns:\n        x: NumPy array shape (m, n)\n        z: NumPy array shape (m, 1)\n\n    NOTE: You do not need to edit this function.\n    \"\"\"\n\n    # Load headers\n    with open(csv_path, 'r') as csv_fh:\n        headers = csv_fh.readline().strip().split(',')\n\n    # Load features and labels\n    x_cols = [i for i in range(len(headers)) if headers[i].startswith('x')]\n    z_cols = [i for i in range(len(headers)) if headers[i] == 'z']\n\n    x = np.loadtxt(csv_path, delimiter=',', skiprows=1, usecols=x_cols, dtype=float)\n    z = np.loadtxt(csv_path, delimiter=',', skiprows=1, usecols=z_cols, dtype=float)\n\n    if z.ndim == 1:\n        z = np.expand_dims(z, axis=-1)\n\n    return x, z\n\n\nif __name__ == '__main__':\n    np.random.seed(229)\n    # Run NUM_TRIALS trials to see how different initializations\n    # affect the final predictions with and without supervision\n    for t in range(NUM_TRIALS):\n        main(is_semi_supervised=False, trial_num=t)\n\n        # *** START CODE HERE ***\n        # Once you've implemented the semi-supervised version,\n        # uncomment the following line.\n        # You do not need to add any other lines in this code block.\n        # main(with_supervision=True, trial_num=t)\n        main(is_semi_supervised=True, trial_num=t)\n        # *** END CODE HERE ***\n</code></pre>"},{"location":"CS/ML/CS229_Problem_Sets/#converge_iterations","title":"converge_iterations","text":""},{"location":"CS/ML/CS229_Problem_Sets/#problem-5-k-means-for-compression","title":"Problem 5 K-means for compression","text":""},{"location":"CS/ML/CS229_Problem_Sets/#a-k-means-compression-implementation","title":"a) K-Means Compression Implementation","text":"<p>compressed image</p>"},{"location":"CS/ML/CS229_Problem_Sets/#b-compression-factor","title":"b) Compression factor","text":"<p>~~Compression factor is approximately <code>2</code>~~</p> <p>In the original image, we need 3 8=24 bits to represent a pixel. </p> <p>In the compressed image, we only need 4 bits (16 colors) to represent a pixel. </p> <p>So the image are compressed by factor 6.</p> <p>p05_kmeans.py</p> <pre><code>from matplotlib.image import imread\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nK = 16\n\n\n# Helper functions\ndef rgb_img_vectorization(img_matrix):\n    \"\"\"\n\n    Args:\n        img_matrix: a list(size: [l,l,3]) represent a square image\n\n    Returns: vectorized numpy array (size : [l**2,3] ) represent the image\n\n    \"\"\"\n    return np.array(img_matrix).reshape(len(img_matrix) ** 2, 3)\n\n\n# K-means\ndef k_means_rgbimg(x, mu):\n    \"\"\"\n    Compress an image (use K colors instead).\n    Args:\n        x: unlabelled dataset of size (m,3)\n        mu: cluster centroids size (K, 3)\n\n    Returns: updated mu\n\n    \"\"\"\n    m, n = x.shape\n    k = mu.shape[0]\n    max_iter = 1000\n\n    prev_c = None\n    it = 0\n    norm_mu_x = np.zeros((m, k))\n    c = np.zeros(m)\n    while it &lt; max_iter and (not (prev_c == c).all()):\n        prev_c = c\n        for i in range(K):\n            norm_mu_x[:, i] = np.linalg.norm(mu[i] - x, axis=1)\n        c = np.argmin(norm_mu_x, axis=1)\n        c_indi = np.zeros((m, K))\n        for i in range(K):\n            c_indi[:, i] = [1. if c_ele == i else 0 for c_ele in c]\n            mu[i] = x.T.dot(c_indi[:, i]) / np.sum(c_indi[:, i])\n        it += 1\n    print(f'Number of iterations:{it}')\n    new_img = np.zeros(x.shape)\n    for i in range(m):\n        new_img[i] = mu[c[i]]\n    img_array = new_img.reshape((int(np.sqrt(m + 1)), int(np.sqrt(m + 1)), 3))\n    plt.imshow(img_array.astype(int))\n    plt.show()\n    return None\n\n\n# Initialize mu to randomly chosen pixel in the image\nA = imread('../data/peppers-large.tiff')\nmu = np.zeros((K, 3))\nx_t = rgb_img_vectorization(A)\n\nidx = np.random.randint(0, len(A)**2, size=K)\nfor i in range(K):\n    mu[i] = x_t[idx[i], :]\n\nk_means_rgbimg(x_t, mu)\n</code></pre>"},{"location":"CS/ML/CS229_Problem_Sets/#ps-4","title":"PS 4","text":""},{"location":"CS/ML/CS229_Problem_Sets/#problem-1-cnn-for-mnist","title":"Problem 1 CNN for MNIST","text":""},{"location":"CS/ML/CS229_Problem_Sets/#a-backward-functions","title":"a) Backward functions","text":"<p>FYI :</p> <p>Softmax in neural network</p> <ul> <li>Softmax is responsible for properly backpropagating the loss gradient so that upstream layers can learn from it.</li> </ul> <p>backward softmax</p> <pre><code>def backward_softmax(x, grad_outputs):\n    \"\"\"\n    Compute the gradient of the loss with respect to x.\n\n    grad_outputs is the gradient of the loss with respect to the outputs of the softmax.\n\n    Args:\n        x: A 1d numpy float array of shape number_of_classes\n        grad_outputs: A 1d numpy float array of shape number_of_classes\n\n    Returns:\n        A 1d numpy float array of the same shape as x with the derivative of the loss with respect to x\n    \"\"\"\n\n    # *** START CODE HERE ***\n    # FYI: the input gradient is the output gradient multiplied by the softmax derivative\n    # Calculate Softmax derivative\n    s = forward_softmax(x)\n    soft_deri = np.diag(s) - np.outer(s, s)\n    return soft_deri.dot(grad_outputs)\n    # *** END CODE HERE ***\n</code></pre> <p>backward ReLU</p> <pre><code>def backward_relu(x, grad_outputs):\n    \"\"\"\n    Compute the gradient of the loss with respect to x\n\n    Args:\n        x: A numpy array of arbitrary shape containing the input.\n        grad_outputs: A numpy array of the same shape of x containing the gradient of the loss with respect\n            to the output of relu\n\n    Returns:\n        A numpy array of the same shape as x containing the gradients with respect to x.\n    \"\"\"\n\n    # *** START CODE HERE ***\n    return (x &gt; 0) * grad_outputs\n    # *** END CODE HERE ***\n</code></pre> <p>backward cross entropy loss</p> <pre><code>def backward_cross_entropy_loss(probabilities, labels):\n    \"\"\"\n    Compute the gradient of the cross entropy loss with respect to the probabilities.\n\n    probabilities is of the shape (# classes)\n    labels is of the shape (# classes)\n\n    The output should be the gradient with respect to the probabilities.\n\n    Returns:\n        The gradient of the loss with respect to the probabilities.\n    \"\"\"\n\n    # *** START CODE HERE ***\n    # loss_grad = np.zeros(probabilities.shape)\n    # for i, label in enumerate(labels):\n    #     if label == 1:\n    #         loss_grad[i] = -1/probabilities[i]\n    # return loss_grad\n    return - labels/probabilities\n    # *** END CODE HERE ***\n</code></pre> <p>backward linear</p> <pre><code>def backward_linear(weights, bias, data, output_grad):\n    \"\"\"\n    Compute the gradients of the loss with respect to the parameters of a linear layer.\n\n    See forward_linear for information about the shapes of the variables.\n\n    output_grad is the gradient of the loss with respect to the output of this layer.\n\n    This should return a tuple with three elements:\n    - The gradient of the loss with respect to the weights\n    - The gradient of the loss with respect to the bias\n    - The gradient of the loss with respect to the data\n    \"\"\"\n\n    # *** START CODE HERE ***\n    return np.outer(data, output_grad), output_grad, weights.dot(output_grad)\n    # *** END CODE HERE ***\n</code></pre> <p>backward convolution</p> <pre><code>def backward_convolution(conv_W, conv_b, data, output_grad):\n    \"\"\"\n    Compute the gradient of the loss with respect to the parameters of the convolution.\n\n    See forward_convolution for the sizes of the arguments.\n    output_grad is the gradient of the loss with respect to the output of the convolution.\n\n    Returns:\n        A tuple containing 3 gradients.\n        The first element is the gradient of the loss with respect to the convolution weights\n        The second element is the gradient of the loss with respect to the convolution bias\n        The third element is the gradient of the loss with respect to the input data\n    \"\"\"\n\n    # *** START CODE HERE ***\n    conv_channels, _, conv_width, conv_height = conv_W.shape\n\n    input_channels, input_width, input_height = data.shape\n\n    grad_bias = output_grad.sum(axis=(1, 2))\n    grad_weight = np.zeros(conv_W.shape)\n    grad_data = np.zeros(data.shape)\n\n    for x in range(input_width - conv_width + 1):\n        for y in range(input_height - conv_height + 1):\n            for output_channel in range(conv_channels):\n                grad_weight[output_channel, :, :, :] += data[:, x:(x + conv_width), y:(y + conv_height)] * output_grad[output_channel, x, y]\n                grad_data[:, x:(x + conv_width), y:(y + conv_height)] += conv_W[output_channel, :, :, :] * output_grad[output_channel, x, y]\n\n    return grad_weight, grad_bias, grad_data\n    # *** END CODE HERE ***\n</code></pre> <p>backward max pool</p> <pre><code>def backward_max_pool(data, pool_width, pool_height, output_grad):\n    \"\"\"\n    Compute the gradient of the loss with respect to the data in the max pooling layer.\n\n    data is of the shape (# channels, width, height)\n    output_grad is of shape (# channels, width // pool_width, height // pool_height)\n\n    output_grad is the gradient of the loss with respect to the output of the backward max\n    pool layer.\n\n    Returns:\n        The gradient of the loss with respect to the data (of same shape as data)\n    \"\"\"\n\n    # *** START CODE HERE ***\n    input_channels, input_width, input_height = data.shape\n    grad_data = np.zeros(data.shape)\n\n    for i in range(input_channels):\n        for x in range(0, input_width, pool_width):\n            for y in range(0, input_height, pool_height):\n                # Need the index of the max entry\n                # Solution 1\n                temp_index = data[i, x:(x + pool_width), y:(y + pool_height)].argmax()\n                grad_data[i, x:(x + pool_width), y:(y + pool_height)].flat[temp_index] += output_grad[\n                    i, x // pool_width, y // pool_height]\n                # Solution 2\n                # temp_matrix = data[i, x:(x + pool_width), y:(y + pool_height)]\n                # grad_data[i, x:(x + pool_width), y:(y + pool_height)][i, np.unravel_index(temp_matrix.argmax(), temp_matrix.shape)] += output_grad[\n                #     i, x // pool_width, y // pool_height]\n\n    return grad_data\n    # *** END CODE HERE ***\n</code></pre>"},{"location":"CS/ML/CS229_Problem_Sets/#b-backward-propagation","title":"b) Backward Propagation","text":"<pre><code>def backward_prop(data, labels, params):\n    \"\"\"\n    Implement the backward propagation gradient computation step for a neural network\n\n    Args:\n        data: A numpy array containing the input for a single example\n        labels: A 1d numpy array containing the labels for a single example\n        params: A dictionary mapping parameter names to numpy arrays with the parameters.\n            This numpy array will contain W1, b1, W2, and b2\n            W1 and b1 represent the weights and bias for the convolutional layer\n            W2 and b2 represent the weights and bias for the output layer of the network\n\n    Returns:\n        A dictionary of strings to numpy arrays where each key represents the name of a weight\n        and the values represent the gradient of the loss with respect to that weight.\n\n        In particular, it should have 4 elements:\n            W1, W2, b1, and b2\n    \"\"\"\n\n    # *** START CODE HERE ***\n    W1 = params['W1']\n    b1 = params['b1']\n    W2 = params['W2']\n    b2 = params['b2']\n\n    first_convolution = forward_convolution(W1, b1, data)\n    first_max_pool = forward_max_pool(first_convolution, MAX_POOL_SIZE, MAX_POOL_SIZE)\n    first_after_relu = forward_relu(first_max_pool)\n    flattened = np.reshape(first_after_relu, (-1))\n    logits = forward_linear(W2, b2, flattened)\n    y = forward_softmax(logits)\n\n    dc_dp = backward_cross_entropy_loss(y, labels)\n    d_softmax = backward_softmax(logits, dc_dp)\n    d_W2, d_b2, dlin_dlinear = backward_linear(W2, b2, flattened, d_softmax)\n    d_relu = backward_relu(first_max_pool, dlin_dlinear.reshape(first_max_pool.shape))\n    d_maxpool = backward_max_pool(first_convolution, MAX_POOL_SIZE, MAX_POOL_SIZE, d_relu)\n    d_W1, d_b1, _ = backward_convolution(W1, b1, data, d_maxpool)\n\n    grad_dict = {'W1': d_W1,\n                 'b1': d_b1,\n                 'W2': d_W2,\n                 'b2': d_b2}\n\n    return grad_dict\n    # *** END CODE HERE ***\n</code></pre>"},{"location":"CS/ML/CS229_Problem_Sets/#problem-2-off-policy-evaluation-for-mdps","title":"Problem 2 Off-policy evaluation for MDPs","text":""},{"location":"CS/ML/CS229_Problem_Sets/#a-importance-sampling-estimator","title":"a) Importance Sampling Estimator","text":"<p>Wanted equation is equal to</p> \\[ \\begin{aligned} \\mathbb{E}_{\\substack{s \\sim p(s) \\\\ a \\sim \\pi_{0}(s, a)}} \\frac{\\pi_{1}(s, a)}{\\hat{\\pi}_{0}(s, a)} R(s, a) &amp;= \\sum_{(s,a)} p(s,a) \\frac{\\pi_{1}(s, a)}{\\hat{\\pi}_{0}(s, a)} R(s, a)\\\\ &amp;= \\sum_{(s,a)} p(s) \\pi_{0}(s, a) \\frac{\\pi_{1}(s, a)}{\\hat{\\pi}_{0}(s, a)} R(s, a) \\end{aligned} \\] <p>Besides, we have \\(\\hat \\pi _0 = \\pi _0\\), thus $$ \\begin{aligned} \\mathbb{E}{\\substack{s \\sim p(s) \\ a \\sim \\pi}(s, a)}} \\frac{\\pi_{1}(s, a)}{\\hat{\\pi{0}(s, a)} R(s, a) &amp;= \\sum} p(s) \\pi_{0}(s, a) \\frac{\\pi_{1}(s, a)}{{\\pi{0}(s, a)} R(s, a) \\ &amp;= \\sum R(s, a)\\ &amp;= \\mathbb{E}} p(s)  {\\pi_{1}(s, a){\\substack{s \\sim p(s) \\ a \\sim \\pi R(s, a) \\end{aligned} $$}(s, a)}</p>"},{"location":"CS/ML/CS229_Problem_Sets/#b-weighted-importance-sampling","title":"b) Weighted Importance Sampling","text":"<p>When \\(\\hat \\pi _0 = \\pi _0\\), we have $$ \\begin{aligned} \\frac{\\mathbb{E}{\\substack{s \\sim p(s) \\ a \\sim \\pi}(s, a)}} \\frac{\\pi_{1}(s, a)}{\\hat{\\pi{0}(s, a)} R(s, a)}{\\mathbb{E}}(s, a)}} \\frac{\\pi_{1}(s, a)}{\\hat{\\pi{0}(s, a)}} &amp;= \\frac{\\sum \\ &amp;= \\frac{\\sum_{(s,a)} p(s)  {\\pi_{1}(s, a)} R(s, a)}{\\sum_{(s,a)} p(s,a)} \\ &amp;= \\sum_{(s,a)} p(s)  {\\pi_{1}(s, a)} R(s, a) \\ &amp;= \\mathbb{E}} p(s)  {\\pi_{1}(s, a)} R(s, a)}{\\sum_{(s,a)} p(s)  {\\pi_{1}(s, a)}{\\substack{s \\sim p(s) \\ a \\sim \\pi R(s, a) \\end{aligned} $$}(s, a)}</p>"},{"location":"CS/ML/CS229_Problem_Sets/#c-weighted-importance-sampling-estimator-is-biased","title":"c) Weighted importance sampling estimator is biased","text":"<p>In our assumption, we have $$ \\begin{aligned} \\frac{\\mathbb{E}{\\substack{s \\sim p(s) \\ a \\sim \\pi}(s, a)}} \\frac{\\pi_{1}(s, a)}{\\hat{\\pi{0}(s, a)} R(s, a)}{\\mathbb{E}}(s, a)}} \\frac{\\pi_{1}(s, a)}{\\hat{\\pi{0}(s, a)}} &amp;= \\mathbb{E} R(s, a) \\end{aligned} $$ If there's only one data element in observational dataset, we have $$ \\begin{aligned} \\frac{\\mathbb{E}}(s, a)}{\\substack{s \\sim p(s) \\ a \\sim \\pi}(s, a)}} \\frac{\\pi_{1}(s, a)}{\\hat{\\pi{0}(s, a)} R(s, a)}{\\mathbb{E}}(s, a)}} \\frac{\\pi_{1}(s, a)}{\\hat{\\pi{0}(s, a)}} &amp;= \\frac{\\sum} p(s,a) \\frac{\\pi_{1}(s, a)}{\\hat{\\pi{0}(s, a)} R(s, a)}{\\sum} p(s,a) \\frac{\\pi_{1}(s, a)}{\\hat{\\pi{0}(s, a)}} \\ &amp;= \\frac{p(s,a) \\frac{\\pi}(s, a)}{\\hat{\\pi{0}(s, a)} R(s, a)}{p(s,a) \\frac{\\pi}(s, a)}{\\hat{\\pi{0}(s, a)}} \\ &amp;= R(s, a) \\ &amp;= \\mathbb{E} R(s, a) \\end{aligned} $$ If }(s, a)}\\(\\pi_1 \\ne \\pi_0\\), $$ \\mathbb{E}{\\substack{s \\sim p(s) \\ a \\sim \\pi}(s, a)}} R(s, a) \\ne \\mathbb{E{\\substack{s \\sim p(s) \\ a \\sim \\pi R(s, a) $$}(s, a)}</p>"},{"location":"CS/ML/CS229_Problem_Sets/#d-doubly-robust","title":"d) Doubly Robust","text":"<p>1) When \\(\\hat \\pi _0 = \\pi _0\\), we have \\(\\hat R (s,a) = R (s,a)\\), thus $$ \\begin{aligned} \\mathbb{E}{\\substack{s \\sim p(s) \\ a \\sim \\pi}(s, a)}}\\left(\\left(\\mathbb{E{a \\sim \\pi}(s, a)} \\hat{R}(s, a)\\right)+\\frac{\\pi_{1}(s, a)}{\\hat{\\pi{0}(s, a)}(R(s, a)-\\hat{R}(s, a))\\right)  &amp;= {\\mathbb{E}}(s, a)}} \\left(\\mathbb{E{a \\sim \\pi}(s, a)} \\hat R(s, a)\\right)}+{\\mathbb{E{\\substack{s \\sim p(s) \\ a \\sim \\pi}(s, a)}} \\frac{\\pi_{1}(s, a)}{{\\pi{0}(s, a)}R(s, a)}\\ &amp;-{\\mathbb{E}}(s, a)}} \\frac{\\pi_{1}(s, a)}{{\\pi{0}(s, a)}\\hat R(s, a)} \\ &amp;= {\\mathbb{E}}(s, a)}} \\hat R(s, a)} + {\\mathbb{E{\\substack{s \\sim p(s) \\ a \\sim \\pi}(s, a)}} \\frac{\\pi_{1}(s, a)}{{\\pi{0}(s, a)}R(s, a)} \\ &amp;- {\\mathbb{E} \\ &amp;= {\\mathbb{E}}(s, a)}} \\hat R(s, a){\\substack{s \\sim p(s) \\ a \\sim \\pi}(s, a)}} \\frac{\\pi_{1}(s, a)}{{\\pi{0}(s, a)}R(s, a)}\\ &amp;= \\mathbb{E} R(s, a) \\end{aligned} $$ }(s, a)}2) When \\(\\hat R (s,a) = R (s,a)\\), thus $$ \\begin{aligned} \\mathbb{E}{\\substack{s \\sim p(s) \\ a \\sim \\pi}(s, a)}}\\left(\\left(\\mathbb{E{a \\sim \\pi}(s, a)} \\hat{R}(s, a)\\right)+\\frac{\\pi_{1}(s, a)}{\\hat{\\pi{0}(s, a)}(R(s, a)-\\hat{R}(s, a))\\right)  &amp;= \\mathbb{E}}(s, a)}} \\left(\\mathbb{E{a \\sim \\pi R(s, a)\\right)\\ &amp;= \\frac{\\mathbb{E}}(s, a){\\substack{s \\sim p(s) \\ a \\sim \\pi}(s, a)}} \\frac{\\pi_{1}(s, a)}{\\hat{\\pi{0}(s, a)} R(s, a)}{\\mathbb{E}}(s, a)}} \\frac{\\pi_{1}(s, a)}{\\hat{\\pi{0}(s, a)}}\\ &amp;= \\mathbb{E} R(s, a)\\ \\end{aligned} $$}(s, a)}</p>"},{"location":"CS/ML/CS229_Problem_Sets/#e-choice-between-estimator","title":"e) Choice between estimator","text":"<p>i.  Use importance sampling estimator. Because the interaction \\(R(s,a)\\) is very complicated, and importance sampling estimator only need to model drug assignment policy \\(\\pi _0\\) using \\(R(s,a)\\) as observation data. </p> <p>ii. Use regression estimator. Because interaction \\(R(s,a)\\) is very simple. And for regression estimator, if \\(\\hat R(s,a) = R(s,a)\\), the estimator is trivially correct.</p>"},{"location":"CS/ML/CS229_Problem_Sets/#problem-3-pca","title":"Problem 3 ==PCA==","text":"\\[ \\begin{aligned} f_{u}(x) &amp; = \\arg \\min _{v \\in \\mathcal{V}}\\|x-v\\|^{2} =\\frac{uu^T x}{u^T u} = uu^T x \\\\ \\\\ \\arg \\min _{u: u^{T} u=1} \\sum_{i=1}^{m}\\left\\|x^{(i)}-f_{u}\\left(x^{(i)}\\right)\\right\\|_{2}^{2} &amp;=\\arg \\min _{u: u^{T} u=1} \\sum_{i=1}^{m}\\left\\|x^{(i)}-u u^{T} x^{(i)}\\right\\|_{2}^{2} \\\\ &amp;=\\arg \\min _{u: u^{T} u=1} \\sum_{i=1}^{m}\\left(x^{(i)}-u u^{T} x^{(i)}\\right)^{T}\\left(x^{(i)}-u u^{T} x^{(i)}\\right) \\\\ &amp;=\\arg \\min _{u: u^{T} u=1} \\sum_{i=1}^{m} x^{(i)^{T}} x^{(i)}-x^{(i)^{T}} u u^{T} x^{(i)} \\\\ &amp;=\\arg \\max _{u: u^{T} u=1} \\sum_{i=1}^{m} x^{(i)^{T}} u u^{T} x^{(i)} \\\\ &amp;=\\arg \\max _{u: u^{T} u=1} \\sum_{i=1}^{m} u^{T} x^{(i)} x^{(i)^{T}} u \\\\ &amp;=\\arg \\max _{u: u^{T} u=1} u^{T}\\left(\\sum_{i=1}^{m} x^{(i)} x^{(i)^{T}}\\right) u \\end{aligned} \\]"},{"location":"CS/ML/CS229_Problem_Sets/#problem-4-independent-components-analysis","title":"Problem 4  Independent Components Analysis","text":""},{"location":"CS/ML/CS229_Problem_Sets/#a-gaussian-source","title":"a) Gaussian source","text":"<p>When \\(g\\prime\\) is standard normal distribution, we have $$ \\begin{aligned} \\ell (W) &amp;= \\sum_{i=1}^n \\left(\\log |W| + \\sum_{j=1}^d \\log {\\frac{1}{\\sqrt{2\\pi}} \\exp \\frac{- \\left(w_j^T x^{(i)} \\right)^2}{2}} \\right) \\ \\nabla_W \\ell &amp;= n{(W^{-1})^T} + \\left(\\sum_{i=1}^n \\nabla_W\\sum_{j=1}^d \\frac{- \\left(w_j^T x^{(i)} \\right)^2}{2} \\right) \\ &amp;=  n{(W^{-1})^T} + \\sum_{i=1}^n W x^{(i)} x^{(i)T} \\ &amp;=  n{(W^{-1})^T} - W X X^T \\ &amp;= 0 \\ \\</p> <p>n{(W^{-1})^T} &amp;= W X X^T \\ W^T W &amp;= n X^T X \\end{aligned} $$ Let \\(u\\) be an arbitrary orthogonal matrix, and let \\(W ^\\prime = RW\\). Then $$ {W ^\\prime}^T W ^\\prime =  W^T R^T R W = W^T W $$ i.e. if \\(W\\) is a solution, any \\(W^ \\prime\\) is also a solution.</p>"},{"location":"CS/ML/CS229_Problem_Sets/#b-laplace-source","title":"b) ==Laplace source==","text":"<p>Derive update for \\(W\\) when \\(s_i \\sim \\mathcal L (0,1)\\) $$ \\begin{aligned} \\nabla_{W} \\ell(W) &amp;=\\nabla_{W}\\left(\\log |W|+\\sum_{j=1}^{d} \\log \\frac{1}{2} \\exp \\left(-\\left|w_{j}^{T} x^{(i)}\\right|\\right)\\right) \\ &amp;=\\left(W^{-1}\\right)^{T}-\\nabla_{W} \\sum_{j=1}^{d}\\left|w_{j}^{T} x^{(i)}\\right| \\ &amp;=\\left(W^{T}\\right)^{-1}-\\operatorname{sign}\\left(W x^{(i)}\\right) x^{(i)^{T}} \\ W &amp;:=W+\\alpha\\left(\\left(W^{T}\\right)^{-1}-\\operatorname{sign}\\left(W x^{(i)}\\right) x^{(i)^{T}}\\right) \\end{aligned} $$</p> <p>Wrong Solution $$ \\begin{aligned} \\ell (W) &amp;= \\sum_{i=1}^n \\left(\\log |W| + \\sum_{j=1}^d \\log {\\frac{1}{2} \\exp \\left({- w_j^T x^{(i)}} \\right) }\\right) \\ \\nabla_W \\ell &amp;= n{(W^{-1})^T} + \\left(\\sum_{i=1}^n \\nabla_W\\sum_{j=1}^d {- w_j^T x^{(i)}} \\right) \\ &amp;= n{(W^{-1})^T} - nX \\ \\ W &amp;:= W + \\alpha n \\left({(W^{-1})^T} - X \\right) \\end{aligned} $$</p>"},{"location":"CS/ML/CS229_Problem_Sets/#c-cocktail-party-problem","title":"c) Cocktail Party Problem","text":"<pre><code>def update_W(W, x, learning_rate):\n    \"\"\"\n    Perform a gradient ascent update on W using data element x and the provided learning rate.\n\n    This function should return the updated W.\n\n    Use the laplace distribiution in this problem.\n\n    Args:\n        W: The W matrix for ICA\n        x: A single data element\n        learning_rate: The learning rate to use\n\n    Returns:\n        The updated W\n    \"\"\"\n\n    # *** START CODE HERE ***\n    updated_W = W + learning_rate * (np.linalg.inv(W.T) - np.outer(np.sign(W.dot(x)), x.T))\n    # *** END CODE HERE ***\n\n    return updated_W\n\n\ndef unmix(X, W):\n    \"\"\"\n    Unmix an X matrix according to W using ICA.\n\n    Args:\n        X: The data matrix\n        W: The W for ICA\n\n    Returns:\n        A numpy array S containing the split data\n    \"\"\"\n\n    S = np.zeros(X.shape)\n\n\n    # *** START CODE HERE ***\n    S = X.dot(W.T)\n    # *** END CODE HERE ***\n\n    return S\n</code></pre>"},{"location":"CS/ML/CS229_Problem_Sets/#problem-5-markov-decision-processes","title":"Problem 5 Markov decision processes","text":""},{"location":"CS/ML/CS229_Problem_Sets/#a_5","title":"a)","text":"\\[ \\begin{aligned} \\left\\|B\\left(V_{1}\\right)-B\\left(V_{2}\\right)\\right\\|_{\\infty} &amp;=\\gamma\\left\\|\\max _{a \\in A} \\sum_{s^{\\prime} \\in S} P_{s a}\\left(s^{\\prime}\\right)\\left[V_{1}\\left(s^{\\prime}\\right)-V_{2}\\left(s^{\\prime}\\right)\\right]\\right\\|_{\\infty} \\\\ &amp;=\\gamma \\max _{s^{\\prime} \\in S}\\left|\\max _{a \\in A} \\sum_{s^{\\prime} \\in S} P_{s a}\\left(s^{\\prime}\\right)\\left[V_{1}\\left(s^{\\prime}\\right)-V_{2}\\left(s^{\\prime}\\right)\\right]\\right| \\\\ &amp; \\leq \\gamma\\left\\|V_{1}-V_{2}\\right\\|_{\\infty} \\end{aligned} \\] <p>The inequality holds because for any \\(\\alpha,x \\in \\mathbb R ^n\\), if \\(\\sum_i \\alpha_i = 1\\) and \\(\\alpha_i \\ge 0\\), then \\(\\sum_i \\alpha_i x_i \\le \\max_i x_i\\) </p>"},{"location":"CS/ML/CS229_Problem_Sets/#b_4","title":"b)","text":"<p>Assume there are two fixed points \\(V_1,V_2\\), i.e. \\(B\\left(V_{1}\\right)=V_{1}, B\\left(V_{2}\\right)=V_{2}\\) $$ \\begin{array}{c} \\left|V_{1}-V_{2}\\right|{\\infty} = \\left|B\\left(V\\right)\\right|}\\right)-B\\left(V_{2{\\infty} \\leq \\gamma\\left|V\\right|}-V_{2{\\infty} \\ \\left|V\\right|}-V_{2{\\infty} = 0 \\ V \\end{array} $$ So } = V_{2\\(B\\) have at most one fixed point.</p>"},{"location":"CS/ML/CS229_Problem_Sets/#problem-6-reinforcement-learning-the-inverted-pendulum","title":"Problem 6 Reinforcement Learning: The inverted pendulum","text":"<ul> <li><code>simulate()</code> function for simulating the pole dynamics</li> <li><code>get_state()</code> for discretizing</li> <li><code>show_cart()</code> for display</li> <li><code>NUM_STATES</code> </li> <li><code>time_steps_to_failure</code>  records the time for which the pole was balanced before each failure is in memory</li> <li><code>num_failures</code>  stores the number of failures (pole drops / cart out of bounds) till now.</li> </ul> <p>:warning: </p> <ul> <li>Update the transition counts and rewards observed after each simulation cycle</li> </ul> <pre><code>def choose_action(state, mdp_data):\n    \"\"\"\n    Choose the next action (0 or 1) that is optimal according to your current\n    mdp_data. When there is no optimal action, return a random action.\n\n    Args:\n        state: The current state in the MDP\n        mdp_data: The parameters for your MDP. See initialize_mdp_data.\n\n    Returns:\n        0 or 1 that is optimal according to your current MDP\n    \"\"\"\n\n    # *** START CODE HERE ***\n    expected_value = mdp_data['value'].dot(mdp_data['transition_probs'][state])\n    if expected_value[0] == expected_value[1]:\n        return np.random.randint(2)\n    elif expected_value[0] &gt; expected_value[1]:\n        return 0\n    else:\n        return 1\n\n    # Plan B\n\n    # else:\n    #     return np.argmax(expect_value)\n\n    # *** END CODE HERE ***\n\ndef update_mdp_transition_counts_reward_counts(mdp_data, state, action, new_state, reward):\n    \"\"\"\n    Update the transition count and reward count information in your mdp_data.\n    Do not change the other MDP parameters (those get changed later).\n\n    Record the number of times `state, action, new_state` occurs.\n    Record the rewards for every `new_state`\n    (since rewards are -1 or 0, you just need to record number of times reward -1 is seen in 'reward_counts' index new_state,0)\n    Record the number of time `new_state` was reached (in 'reward_counts' index new_state,1)\n\n    Args:\n        mdp_data: The parameters of your MDP. See initialize_mdp_data.\n        state: The state that was observed at the start.\n        action: The action you performed.\n        new_state: The state after your action.\n        reward: The reward after your action (i.e. reward corresponding to new_state).\n\n    Returns:\n        Nothing\n    \"\"\"\n\n    # *** START CODE HERE ***\n    mdp_data['transition_counts'][state, new_state, action] += 1\n\n    mdp_data['reward_counts'][new_state, 1] += 1\n\n    if reward != 0:\n        mdp_data['reward_counts'][new_state, 0] += 1\n    # *** END CODE HERE ***\n\n    # This function does not return anything\n    return\n\n\ndef update_mdp_transition_probs_reward(mdp_data):\n    \"\"\"\n    Update the estimated transition probabilities and reward values in your MDP.\n\n    Make sure you account for the case when a state-action pair has never\n    been tried before, or the state has never been visited before. In that\n    case, you must not change that component (and thus keep it at the\n    initialized uniform distribution).\n\n    Args:\n        mdp_data: The data for your MDP. See initialize_mdp_data.\n\n    Returns:\n        Nothing\n\n    \"\"\"\n\n    # *** START CODE HERE ***\n    transition_counts = mdp_data['transition_counts']\n    num_counts = transition_counts.sum(axis=1)\n    for i in range(mdp_data['num_states']):\n        for a in range(2):\n            if num_counts[i, a] != 0:\n                mdp_data['transition_probs'][i, :, a] = transition_counts[i, :, a] / num_counts[i, a]\n\n    for state in range(mdp_data['num_states']):\n        if mdp_data['reward_counts'][state][1] != 0:\n            mdp_data['reward'][state] = - mdp_data['reward_counts'][state][0] / mdp_data['reward_counts'][state][1]\n    # *** END CODE HERE ***\n\n    # This function does not return anything\n    return\n\ndef update_mdp_value(mdp_data, tolerance, gamma):\n    \"\"\"\n    Update the estimated values in your MDP.\n\n    Perform value iteration using the new estimated model for the MDP.\n    The convergence criterion should be based on `TOLERANCE` as described\n    at the top of the file.\n\n    Return true if it converges within one iteration.\n\n    Args:\n        mdp_data: The data for your MDP. See initialize_mdp_data.\n        tolerance: The tolerance to use for the convergence criterion.\n        gamma: Your discount factor.\n\n    Returns:\n        True if the value iteration converged in one iteration\n\n    \"\"\"\n\n    # *** START CODE HERE ***\n    iters = 0\n\n    while True:\n        iters += 1\n\n        value = mdp_data['value']\n        new_value = mdp_data['reward'] + gamma * value.dot(mdp_data['transition_probs']).max(axis=1)\n        mdp_data['value'] = new_value\n\n        if np.max(np.abs(value - new_value)) &lt; tolerance:\n            break\n\n    return iters == 1\n    # *** END CODE HERE ***\n</code></pre>"},{"location":"CS/ML/CS231n/","title":"Standford CS231n 2017 Summary","text":"<p>After watching all the videos of the famous Standford's CS231n course that took place in 2017, i decided to take summary of the whole course to help me to remember and to anyone who would like to know about it. I've skipped some contents in some lectures as it wasn't important to me.</p>"},{"location":"CS/ML/CS231n/#table-of-contents","title":"Table of contents","text":"<ul> <li>Standford CS231n 2017 Summary</li> <li>Table of contents</li> <li>Course Info</li> <li>01. Introduction to CNN for visual recognition</li> <li>02. Image classification</li> <li>03. Loss function and optimization</li> <li>04. Introduction to Neural network</li> <li>05. Convolutional neural networks (CNNs)</li> <li>06. Training neural networks I</li> <li>07. Training neural networks II</li> <li>08. Deep learning software</li> <li>09. CNN architectures</li> <li>10. Recurrent Neural networks</li> <li>11. Detection and Segmentation</li> <li>12. Visualizing and Understanding</li> <li>13. Generative models</li> <li>14. Deep reinforcement learning</li> <li>15. Efficient Methods and Hardware for Deep Learning</li> <li>16. Adversarial Examples and Adversarial Training</li> </ul>"},{"location":"CS/ML/CS231n/#course-info","title":"Course Info","text":"<ul> <li> <p>Website: http://cs231n.stanford.edu/</p> </li> <li> <p>Lectures link: https://www.youtube.com/playlist?list=PLC1qU-LWwrF64f4QKQT-Vg5Wr4qEE1Zxk</p> </li> <li> <p>Full syllabus link: http://cs231n.stanford.edu/syllabus.html</p> </li> <li> <p>Assignments solutions: https://github.com/Burton2000/CS231n-2017</p> </li> <li> <p>Number of lectures: 16</p> </li> <li> <p>Course description:</p> </li> <li> <p>Computer Vision has become ubiquitous in our society, with applications in search, image understanding, apps, mapping, medicine, drones, and self-driving cars. Core to many of these applications are visual recognition tasks such as image classification, localization and detection. Recent developments in neural network (aka \u201cdeep learning\u201d) approaches have greatly advanced the performance of these state-of-the-art visual recognition systems. This course is a deep dive into details of the deep learning architectures with a focus on learning end-to-end models for these tasks, particularly image classification. During the 10-week course, students will learn to implement, train and debug their own neural networks and gain a detailed understanding of cutting-edge research in computer vision. The final assignment will involve training a multi-million parameter convolutional neural network and applying it on the largest image classification dataset (ImageNet). We will focus on teaching how to set up the problem of image recognition, the learning algorithms (e.g. backpropagation), practical engineering tricks for training and fine-tuning the networks and guide the students through hands-on assignments and a final course project. Much of the background and materials of this course will be drawn from the ImageNet Challenge.</p> </li> </ul>"},{"location":"CS/ML/CS231n/#01-introduction-to-cnn-for-visual-recognition","title":"01. Introduction to CNN for visual recognition","text":"<ul> <li>A brief history of Computer vision starting from the late 1960s to 2017.</li> <li>Computer vision problems includes image classification, object localization, object detection, and scene understanding.</li> <li>Imagenet is one of the biggest datasets in image classification available right now.</li> <li>Starting 2012 in the Imagenet competition, CNN (Convolutional neural networks) is always winning.</li> <li>CNN actually has been invented in 1997 by Yann Lecun.</li> </ul>"},{"location":"CS/ML/CS231n/#02-image-classification","title":"02. Image classification","text":"<ul> <li>Image classification problem has a lot of challenges like illumination and viewpoints.</li> <li></li> <li>An image classification algorithm can be solved with K nearest neighborhood (KNN) but it can poorly solve the problem. The properties of KNN are:</li> <li>Hyperparameters of KNN are: k and the distance measure</li> <li>K is the number of neighbors we are comparing to.</li> <li>Distance measures include:<ul> <li>L2 distance (Euclidean distance)</li> <li>Best for non coordinate points</li> <li>L1 distance (Manhattan distance)</li> <li>Best for coordinate points</li> </ul> </li> <li>Hyperparameters can be optimized using Cross-validation as following (In our case we are trying tp predict K):</li> <li>Split your dataset into <code>f</code> folds.</li> <li>Given predicted hyperparameters:<ul> <li>Train your algorithm with f-1 folds and test it with the remain flood. and repeat this with every fold.</li> </ul> </li> <li>Choose the hyperparameters that gives the best training values (Average over all folds)</li> <li>Linear SVM classifier is an option for solving the image classification problem, but the curse of dimensions makes it stop improving at some point.</li> <li>Logistic regression is a also a solution for image classification problem, but image classification problem is non linear!</li> <li>Linear classifiers has to run the following equation: <code>Y = wX + b</code> </li> <li>shape of <code>w</code> is the same as <code>x</code> and shape of <code>b</code> is 1.</li> <li>We can add 1 to X vector and remove the bias so that: <code>Y = wX</code></li> <li>shape of <code>x</code> is <code>oldX+1</code> and <code>w</code> is the same as <code>x</code></li> <li>We need to know how can we get <code>w</code>'s and <code>b</code>'s that makes the classifier runs at best.</li> </ul>"},{"location":"CS/ML/CS231n/#03-loss-function-and-optimization","title":"03. Loss function and optimization","text":"<ul> <li> <p>In the last section we talked about linear classifier but we didn't discussed how we could train the parameters of that model to get best <code>w</code>'s and <code>b</code>'s.</p> </li> <li> <p>We need a loss function to measure how good or bad our current parameters.</p> </li> <li> <p><code>python     Loss = L[i] =(f(X[i],W),Y[i])     Loss_for_all = 1/N * Sum(Li(f(X[i],W),Y[i]))      # Indicates the average</code></p> </li> <li> <p>Then we find a way to minimize the loss function given some parameters. This is called optimization.</p> </li> <li> <p>Loss function for a linear SVM classifier:</p> </li> <li> <p><code>L[i] = Sum where all classes except the predicted class (max(0, s[j] - s[y[i]] + 1))</code></p> </li> <li>We call this the hinge loss.</li> <li>Loss function means we are happy if the best prediction are the same as the true value other wise we give an error with 1 margin.</li> <li>Example:<ul> <li></li> <li>Given this example we want to compute the loss of this image.</li> <li><code>L = max (0, 437.9 - (-96.8) + 1) + max(0, 61.95 - (-96.8) + 1) = max(0, 535.7) + max(0, 159.75) = 695.45</code></li> <li>Final loss is 695.45 which is big and reflects that the cat score needs to be the best over all classes as its the lowest value now. We need to minimize that loss.</li> </ul> </li> <li> <p>Its OK for the margin to be 1. But its a hyperparameter too.</p> </li> <li> <p>If your loss function gives you zero, are this value is the same value for your parameter? No there are a lot of parameters that can give you best score.</p> </li> <li> <p>You\u2019ll sometimes hear about people instead using the squared hinge loss SVM (or L2-SVM). that penalizes violated margins more strongly (quadratically instead of linearly). The unsquared version is more standard, but in some datasets the squared hinge loss can work better.</p> </li> <li> <p>We add regularization for the loss function so that the discovered model don't overfit the data.</p> </li> <li> <p><code>python     Loss = L = 1/N * Sum(Li(f(X[i],W),Y[i])) + lambda * R(W)</code></p> </li> <li> <p>Where <code>R</code> is the regularizer, and <code>lambda</code> is the regularization term.</p> </li> <li> <p>There are different regularizations techniques:</p> </li> <li> Regularizer Equation Comments L2 <code>R(W) = Sum(W^2)</code> Sum all the W squared L1 <code>R(W) = Sum(lWl)</code> Sum of all Ws with abs Elastic net (L1 + L2) <code>R(W) = beta * Sum(W^2) + Sum(lWl)</code> Dropout No Equation </li> <li> <p>Regularization prefers smaller <code>W</code>s over big <code>W</code>s.</p> </li> <li> <p>Regularizations is called weight decay. biases should not included in regularization.</p> </li> <li> <p>Softmax loss (Like linear regression but works for more than 2 classes):</p> </li> <li> <p>Softmax function:</p> <ul> <li><code>python   A[L] = e^(score[L]) / sum(e^(score[L]), NoOfClasses)</code></li> </ul> </li> <li> <p>Sum of the vector should be 1.</p> </li> <li> <p>Softmax loss:</p> <ul> <li> <p><code>python   Loss = -logP(Y = y[i]|X = x[i])</code></p> </li> <li> <p>Log of the probability of the good class. We want it to be near 1 thats why we added a minus.</p> </li> <li> <p>Softmax loss is called cross-entropy loss.</p> </li> </ul> </li> <li> <p>Consider this numerical problem when you are computing Softmax:</p> <ul> <li>```python   f = np.array([123, 456, 789]) # example with 3 classes and each having large scores   p = np.exp(f) / np.sum(np.exp(f)) # Bad: Numeric problem, potential blowup</li> </ul> <p># instead: first shift the values of f so that the highest number is 0:   f -= np.max(f) # f becomes [-666, -333, 0]   p = np.exp(f) / np.sum(np.exp(f)) # safe to do, gives the correct answer   ```</p> </li> <li> <p>Optimization:</p> </li> <li> <p>How we can optimize loss functions we discussed?</p> </li> <li> <p>Strategy one:</p> <ul> <li>Get a random parameters and try all of them on the loss and get the best loss. But its a bad idea.</li> <li>Strategy two:</li> <li> <p>Follow the slope.</p> </li> <li> <p></p> </li> <li> <p>Image source.</p> </li> <li> <p>Our goal is to compute the gradient of each parameter we have.</p> </li> <li> <p>Numerical gradient: Approximate, slow, easy to write.   (But its useful in debugging.)</p> </li> <li> <p>Analytic gradient: Exact, Fast, Error-prone.   (Always used in practice)</p> </li> <li> <p>After we compute the gradient of our parameters, we compute the gradient descent:</p> </li> <li><code>python   W = W - learning_rate * W_grad</code>   ```</li> </ul> <p>```</p> </li> <li> <p>learning_rate is so important hyper parameter you should get the best value of it first of all the hyperparameters.</p> <ul> <li> <p>stochastic gradient descent:</p> </li> <li> <p>Instead of using all the date, use a mini batch of examples (32/64/128 are commonly used) for faster results.</p> </li> </ul> </li> </ul>"},{"location":"CS/ML/CS231n/#04-introduction-to-neural-network","title":"04. Introduction to Neural network","text":"<ul> <li> <p>Computing the analytic gradient for arbitrary complex functions:</p> </li> <li> <p>What is a Computational graphs?</p> <ul> <li>Used to represent any function. with nodes.</li> <li>Using Computational graphs can easy lead us to use a technique that called back-propagation. Even with complex models like CNN and RNN.</li> </ul> </li> <li> <p>Back-propagation simple example:</p> <ul> <li> <p>Suppose we have <code>f(x,y,z) = (x+y)z</code></p> </li> <li> <p>Then graph can be represented this way:</p> </li> <li> <p><code>X              \\      (+)--&gt; q ---(*)--&gt; f     /           /   Y            /               /              /   Z---------/</code></p> </li> <li> <p>We made an intermediate variable <code>q</code>  to hold the values of <code>x+y</code></p> </li> <li> <p>Then we have:</p> </li> <li> <p><code>python     q = (x+y)              # dq/dx = 1 , dq/dy = 1     f = qz                 # df/dq = z , df/dz = q</code></p> </li> <li> <p>Then:</p> </li> <li> <p><code>python     df/dq = z     df/dz = q     df/dx = df/dq * dq/dx = z * 1 = z       # Chain rule     df/dy = df/dq * dq/dy = z * 1 = z       # Chain rule</code></p> </li> </ul> </li> <li> <p>So in the Computational graphs, we call each operation <code>f</code>. For each <code>f</code> we calculate the local gradient before we go on back propagation and then we compute the gradients in respect of the loss function using the chain rule.</p> </li> <li> <p>In the Computational graphs you can split each operation to as simple as you want but the nodes will be a lot. if you want the nodes to be smaller be sure that you can compute the gradient of this node.</p> </li> <li> <p>A bigger example:</p> <ul> <li></li> <li>Hint: the back propagation of two nodes going to one node from the back is by adding the two derivatives.</li> </ul> </li> <li> <p>Modularized implementation: forward/ backward API (example multiply code):</p> <ul> <li><code>python   class MultuplyGate(object):     \"\"\"     x,y are scalars     \"\"\"     def forward(x,y):       z = x*y       self.x = x  # Cache       self.y = y    # Cache       # We cache x and y because we know that the derivatives contains them.       return z     def backward(dz):       dx = self.y * dz         #self.y is dx       dy = self.x * dz       return [dx, dy]</code></li> </ul> </li> <li> <p>If you look at a deep learning framework you will find it follow the Modularized implementation where each class has a definition for forward and backward. For example:</p> <ul> <li>Multiplication</li> <li>Max</li> <li>Plus</li> <li>Minus</li> <li>Sigmoid</li> <li>Convolution</li> </ul> </li> <li> <p>So to define neural network as a function:</p> </li> <li> <p>(Before) Linear score function: <code>f = Wx</code></p> </li> <li>(Now) 2-layer neural network:    <code>f = W2*max(0,W1*x)</code> <ul> <li>Where max is the RELU non linear function</li> </ul> </li> <li>(Now) 3-layer neural network:    <code>f = W3*max(0,W2*max(0,W1*x)</code></li> <li> <p>And so on..</p> </li> <li> <p>Neural networks is a stack of some simple operation that forms complex operations.</p> </li> </ul>"},{"location":"CS/ML/CS231n/#05-convolutional-neural-networks-cnns","title":"05. Convolutional neural networks (CNNs)","text":"<ul> <li>Neural networks history:</li> <li>First perceptron machine was developed by Frank Rosenblatt in 1957. It was used to recognize letters of the alphabet. Back propagation wasn't developed yet.</li> <li>Multilayer perceptron was developed in 1960 by Adaline/Madaline. Back propagation wasn't developed yet.</li> <li>Back propagation was developed in 1986 by Rumeelhart.</li> <li>There was a period which nothing new was happening with NN. Cause of the limited computing resources and data.</li> <li>In 2006 Hinton released a paper that shows that we can train a deep neural network using Restricted Boltzmann machines to initialize the weights then back propagation.</li> <li>The first strong results was in 2012 by Hinton in speech recognition. And the Alexnet \"Convolutional neural networks\" that wins the image net in 2012 also by Hinton's team.</li> <li>After that NN is widely used in various applications.</li> <li>Convolutional neural networks history:</li> <li>Hubel &amp; Wisel in 1959 to 1968 experiments on cats cortex found that there are a topographical mapping in the cortex and that the neurons has hireical organization from simple to complex.</li> <li>In 1998, Yann Lecun gives the paper Gradient-based learning applied to document recognition that introduced the Convolutional neural networks. It was good for recognizing zip letters but couldn't run on a more complex examples.</li> <li>In 2012 AlexNet used the same Yan Lecun architecture and won the image net challenge. The difference from 1998 that now we have a large data sets that can be used also the power of the GPUs solved a lot of performance problems.</li> <li>Starting from 2012 there are CNN that are used for various tasks (Here are some applications):<ul> <li>Image classification.</li> <li>Image retrieval.</li> <li>Extracting features using a NN and then do a similarity matching.</li> <li>Object detection.</li> <li>Segmentation.</li> <li>Each pixel in an image takes a label.</li> <li>Face recognition.</li> <li>Pose recognition.</li> <li>Medical images.</li> <li>Playing Atari games with reinforcement learning.</li> <li>Galaxies classification.</li> <li>Street signs recognition.</li> <li>Image captioning.</li> <li>Deep dream.</li> </ul> </li> <li>ConvNet architectures make the explicit assumption that the inputs are images, which allows us to encode certain properties into the architecture.</li> <li>There are a few distinct types of Layers in ConvNet (e.g. CONV/FC/RELU/POOL are by far the most popular)</li> <li>Each Layer may or may not have parameters (e.g. CONV/FC do, RELU/POOL don\u2019t)</li> <li>Each Layer may or may not have additional hyperparameters (e.g. CONV/FC/POOL do, RELU doesn\u2019t)</li> <li>How Convolutional neural networks works?</li> <li>A fully connected layer is a layer in which all the neurons is connected. Sometimes we call it a dense layer.<ul> <li>If input shape is <code>(X, M)</code> the weighs shape for this will be <code>(NoOfHiddenNeurons, X)</code></li> </ul> </li> <li>Convolution layer is a layer in which we will keep the structure of the input by a filter that goes through all the image.<ul> <li>We do this with dot product: <code>W.T*X + b</code>. This equation uses the broadcasting technique.</li> <li>So we need to get the values of <code>W</code> and <code>b</code></li> <li>We usually deal with the filter (<code>W</code>) as a vector not a matrix.</li> </ul> </li> <li>We call output of the convolution activation map. We need to have multiple activation map.<ul> <li>Example if we have 6 filters, here are the shapes:</li> <li>Input image                        <code>(32,32,3)</code></li> <li>filter size                              <code>(5,5,3)</code><ul> <li>We apply 6 filters. The depth must be three because the input map has depth of three.</li> </ul> </li> <li>Output of Conv.                 <code>(28,28,6)</code> <ul> <li>if one filter it will be   <code>(28,28,1)</code></li> </ul> </li> <li>After RELU                          <code>(28,28,6)</code> </li> <li>Another filter                     <code>(5,5,6)</code></li> <li>Output of Conv.                 <code>(24,24,10)</code></li> </ul> </li> <li>It turns out that convNets learns in the first layers the low features and then the mid-level features and then the high level features.</li> <li>After the Convnets we can have a linear classifier for a classification task.</li> <li>In Convolutional neural networks usually we have some (Conv ==&gt; Relu)s and then we apply a pool operation to downsample the size of the activation.</li> <li>What is stride when we are doing convolution:</li> <li>While doing a conv layer we have many choices to make regarding the stride of which we will take. I will explain this by examples.</li> <li>Stride is skipping while sliding. By default its 1.</li> <li>Given a matrix with shape of <code>(7,7)</code> and a filter with shape <code>(3,3)</code>:<ul> <li>If stride is <code>1</code> then the output shape will be <code>(5,5)</code> <code># 2 are dropped</code></li> <li>If stride is <code>2</code> then the output shape will be <code>(3,3)</code> <code># 4 are dropped</code></li> <li>If stride is <code>3</code> it doesn't work.</li> </ul> </li> <li> <p>A general formula would be <code>((N-F)/stride +1)</code></p> <ul> <li>If stride is <code>1</code> then <code>O = ((7-3)/1)+1 = 4 + 1 = 5</code></li> <li>If stride is <code>2</code> then <code>O = ((7-3)/2)+1 = 2 + 1 = 3</code></li> <li>If stride is <code>3</code> then <code>O = ((7-3)/3)+1 = 1.33 + 1 = 2.33</code> <code># doesn't work</code></li> </ul> </li> <li> <p>In practice its common to zero pad the border.   <code># Padding from both sides.</code></p> </li> <li>Give a stride of <code>1</code> its common to pad to this equation:  <code>(F-1)/2</code> where F is the filter size<ul> <li>Example <code>F = 3</code> ==&gt; Zero pad with <code>1</code></li> <li>Example <code>F = 5</code> ==&gt; Zero pad with <code>2</code></li> </ul> </li> <li>If we pad this way we call this same convolution.</li> <li>Adding zeros gives another features to the edges thats why there are different padding techniques like padding the corners not zeros but in practice zeros works!</li> <li>We do this to maintain our full size of the input. If we didn't do that the input will be shrinking too fast and we will lose a lot of data.</li> <li>Example:</li> <li>If we have input of shape <code>(32,32,3)</code> and ten filters with shape is <code>(5,5)</code> with stride <code>1</code> and pad <code>2</code><ul> <li>Output size will be <code>(32,32,10)</code> <code># We maintain the size.</code></li> </ul> </li> <li>Size of parameters per filter <code>= 5*5*3 + 1 = 76</code></li> <li>All parameters <code>= 76 * 10 = 76</code></li> <li>Number of filters is usually common to be to the power of 2.           <code># To vectorize well.</code></li> <li>So here are the parameters for the Conv layer:</li> <li>Number of filters K.<ul> <li>Usually a power of 2.</li> </ul> </li> <li>Spatial content size F.<ul> <li>3,5,7 ....</li> </ul> </li> <li>The stride S. <ul> <li>Usually 1 or 2        (If the stride is big there will be a downsampling but different of pooling) </li> </ul> </li> <li>Amount of Padding<ul> <li>If we want the input shape to be as the output shape, based on the F if 3 its 1, if F is 5 the 2 and so on.</li> </ul> </li> <li>Pooling makes the representation smaller and more manageable.</li> <li>Pooling Operates over each activation map independently.</li> <li>Example of pooling is the maxpooling.</li> <li>Parameters of max pooling is the size of the filter and the stride\"<ul> <li>Example <code>2x2</code> with stride <code>2</code> <code># Usually the two parameters are the same 2 , 2</code></li> </ul> </li> <li>Also example of pooling is average pooling.</li> <li>In this case it might be learnable.</li> </ul>"},{"location":"CS/ML/CS231n/#06-training-neural-networks-i","title":"06. Training neural networks I","text":"<ul> <li> <p>As a revision here are the Mini batch stochastic gradient descent algorithm steps:</p> </li> <li> <p>Loop:</p> <ol> <li>Sample a batch of data.</li> <li>Forward prop it through the graph (network) and get loss.</li> <li>Backprop to calculate the gradients.</li> <li>Update the parameters using the gradients.</li> </ol> </li> <li> <p>Activation functions:</p> </li> <li> <p>Different choices for activation function includes Sigmoid, tanh, RELU, Leaky RELU, Maxout, and ELU.</p> </li> <li> <p></p> </li> <li> <p>Sigmoid:</p> <ul> <li>Squashes the numbers between [0,1]</li> <li>Used as a firing rate like human brains.</li> <li><code>Sigmoid(x) = 1 / (1 + e^-x)</code></li> <li>Problems with sigmoid:</li> <li>big values neurons kill the gradients.<ul> <li>Gradients are in most cases near 0 (Big values/small values), that kills the updates if the graph/network are large.</li> </ul> </li> <li>Not Zero-centered.<ul> <li>Didn't produce zero-mean data.</li> </ul> </li> <li><code>exp()</code> is a bit compute expensive.<ul> <li>just to mention. We have a more complex operations in deep learning like convolution.</li> </ul> </li> </ul> </li> <li> <p>Tanh:</p> <ul> <li>Squashes the numbers between [-1,1]</li> <li>Zero centered.</li> <li>Still big values neurons \"kill\" the gradients.</li> <li><code>Tanh(x)</code> is the equation.</li> <li>Proposed by Yann Lecun in 1991.</li> </ul> </li> <li> <p>RELU (Rectified linear unit):</p> <ul> <li><code>RELU(x) = max(0,x)</code></li> <li>Doesn't kill the gradients.</li> <li>Only small values that are killed. Killed the gradient in the half</li> <li>Computationally efficient.</li> <li>Converges much faster than Sigmoid and Tanh <code>(6x)</code></li> <li>More biologically plausible than sigmoid.</li> <li>Proposed by Alex Krizhevsky in 2012 Toronto university. (AlexNet)</li> <li>Problems:</li> <li>Not zero centered.</li> <li>If weights aren't initialized good, maybe 75% of the neurons will be dead and thats a waste computation. But its still works. This is an active area of research to optimize this.</li> <li>To solve the issue mentioned above, people might initialize all the biases by 0.01</li> </ul> </li> <li> <p>Leaky RELU:</p> <ul> <li><code>leaky_RELU(x) = max(0.01x,x)</code></li> <li>Doesn't kill the gradients from both sides.</li> <li>Computationally efficient.</li> <li>Converges much faster than Sigmoid and Tanh (6x)</li> <li>Will not die.</li> <li>PRELU is placing the 0.01 by a variable alpha which is learned as a parameter.</li> </ul> </li> <li> <p>Exponential linear units (ELU):</p> <ul> <li> <p><code>ELU(x) = { x                                           if x &gt; 0            alpah *(exp(x) -1)                          if x &lt;= 0              # alpah are a learning parameter   }</code></p> </li> <li> <p>It has all the benefits of RELU</p> </li> <li> <p>Closer to zero mean outputs and adds some robustness to noise.</p> </li> <li> <p>problems</p> </li> <li> <p><code>exp()</code> is a bit compute expensive. </p> </li> </ul> </li> <li> <p>Maxout activations:</p> <ul> <li><code>maxout(x) = max(w1.T*x + b1, w2.T*x + b2)</code></li> <li>Generalizes RELU and Leaky RELU</li> <li>Doesn't die!</li> <li>Problems:</li> <li>oubles the number of parameters per neuron</li> </ul> </li> <li> <p>In practice:</p> <ul> <li>Use RELU. Be careful for your learning rates.</li> <li>Try out Leaky RELU/Maxout/ELU</li> <li>Try out tanh but don't expect much.</li> <li>Don't use sigmoid!</li> </ul> </li> <li> <p>Data preprocessing:</p> </li> <li> <p>Normalize the data:</p> </li> <li> <p>```python     # Zero centered data. (Calculate the mean for every input).     # On of the reasons we do this is because we need data to be between positive and negative and not all the be negative or positive.      X -= np.mean(X, axis = 1)</p> </li> <li> <p>To normalize images:</p> <ul> <li>Subtract the mean image (E.g. Alexnet)</li> <li>Mean image shape is the same as the input images.</li> <li>Or Subtract per-channel mean </li> <li>Means calculate the mean for each channel of all images. Shape is 3 (3 channels)</li> </ul> </li> <li> <p>Weight initialization:</p> </li> <li> <p>What happened when initialize all Ws with zeros?</p> <ul> <li>All the neurons will do exactly the same thing. They will have the same gradient and they will have the same update.</li> <li>So if W's of a specific layer is equal the thing described happened</li> </ul> </li> <li> <p>First idea is to initialize the w's with small random numbers:</p> <ul> <li> <p><code>python   W = 0.01 * np.random.rand(D, H)   # Works OK for small networks but it makes problems with deeper networks!</code></p> </li> <li> <p>The standard deviations is going to zero in deeper networks. and the gradient will vanish sooner in deep networks.</p> </li> <li> <p><code>python   W = 1 * np.random.rand(D, H)    # Works OK for small networks but it makes problems with deeper networks!</code></p> </li> <li> <p>The network will explode with big numbers!</p> </li> </ul> </li> <li> <p>Xavier initialization:</p> <ul> <li> <p><code>python   W = np.random.rand(in, out) / np.sqrt(in)</code></p> </li> <li> <p>It works because we want the variance of the input to be as the variance of the output.</p> </li> <li> <p>But it has an issue, It breaks when you are using RELU.</p> </li> </ul> </li> <li> <p>He initialization (Solution for the RELU issue):</p> <ul> <li> <p><code>python   W = np.random.rand(in, out) / np.sqrt(in/2)</code></p> </li> <li> <p>Solves the issue with RELU. Its recommended when you are using RELU</p> </li> </ul> </li> <li> <p>Proper initialization is an active area of research.</p> </li> <li> <p>Batch normalization:</p> </li> <li> <p>is a technique to provide any layer in a Neural Network with inputs that are zero mean/unit variance.</p> </li> <li>It speeds up the training. You want to do this a lot.<ul> <li>Made by Sergey Ioffe and Christian Szegedy at 2015.</li> </ul> </li> <li>We make a Gaussian activations in each layer. by calculating the mean and the variance.</li> <li>Usually inserted after (fully connected or Convolutional layers) and (before nonlinearity).</li> <li>Steps (For each output of a layer)<ol> <li>First we compute the mean and variance^2 of the batch for each feature.</li> <li>We normalize by subtracting the mean and dividing by square root of (variance^2 + epsilon)</li> <li>epsilon to not divide by zero</li> <li>Then we make a scale and shift variables: <code>Result = gamma * normalizedX + beta</code> </li> <li>gamma and beta are learnable parameters.</li> <li>it basically possible to say \u201cHey!! I don\u2019t want zero mean/unit variance input, give me back the raw input - it\u2019s better for me.\u201d</li> <li>Hey shift and scale by what you want not just the mean and variance!</li> </ol> </li> <li>The algorithm makes each layer flexible (It chooses which distribution it wants)</li> <li>We initialize the BatchNorm Parameters to transform the input to zero mean/unit variance distributions but during training they can learn that any other distribution might be better.</li> <li>During the running of the training we need to calculate the globalMean and globalVariance for each layer by using weighted average.</li> <li>Benefits of Batch Normalization:<ul> <li>Networks train faster.</li> <li>Allows higher learning rates.</li> <li>helps reduce the sensitivity to the initial starting weights.</li> <li>Makes more activation functions viable.</li> <li>Provides some regularization.</li> <li>Because we are calculating mean and variance for each batch that gives a slight regularization effect.</li> </ul> </li> <li>In conv layers, we will have one variance and one mean per activation map.</li> <li> <p>Batch normalization have worked best for CONV and regular deep NN, But for recurrent NN and reinforcement learning its still an active research area.</p> <ul> <li>Its challengey in reinforcement learning because the batch is small.</li> </ul> </li> <li> <p>Baby sitting the learning process</p> </li> <li> <p>Preprocessing of data.</p> </li> <li>Choose the architecture.</li> <li>Make a forward pass and check the loss (Disable regularization). Check if the loss is reasonable.</li> <li>Add regularization, the loss should go up!</li> <li>Disable the regularization again and take a small number of data and try to train the loss and reach zero loss.<ul> <li>You should overfit perfectly for small datasets.</li> </ul> </li> <li>Take your full training data, and small regularization then try some value of learning rate.<ul> <li>If loss is barely changing, then the learning rate is small.</li> <li>If you got <code>NAN</code> then your NN exploded and your learning rate is high.</li> <li>Get your learning rate range by trying the min value (That can change) and the max value that doesn't explode the network.</li> </ul> </li> <li> <p>Do Hyperparameters optimization to get the best hyperparameters values.</p> </li> <li> <p>Hyperparameter Optimization</p> </li> <li> <p>Try Cross validation strategy.</p> <ul> <li>Run with a few ephocs, and try to optimize the ranges.</li> </ul> </li> <li>Its best to optimize in log space.</li> <li>Adjust your ranges and try again.</li> <li>Its better to try random search instead of grid searches (In log space)</li> </ul>"},{"location":"CS/ML/CS231n/#then-apply-the-standard-deviation-hint-in-images-we-dont-do-this","title":"Then apply the standard deviation. Hint: in images we don't do this.","text":"<p>X /= np.std(X, axis = 1) ```</p>"},{"location":"CS/ML/CS231n/#07-training-neural-networks-ii","title":"07. Training neural networks II","text":"<ul> <li> <p>Optimization algorithms:</p> </li> <li> <p>Problems with stochastic gradient descent:</p> <ul> <li>if loss quickly in one direction and slowly in another (For only two variables), you will get very slow progress along shallow dimension, jitter along steep direction. Our NN will have a lot of parameters then the problem will be more.</li> <li>Local minimum or saddle points</li> <li>If SGD went into local minimum we will stuck at this point because the gradient is zero.</li> <li>Also in saddle points the gradient will be zero so we will stuck.</li> <li>Saddle points says that at some point:<ul> <li>Some gradients will get the loss up.</li> <li>Some gradients will get the loss down.</li> <li>And that happens more in high dimensional (100 million dimension for example)</li> </ul> </li> <li>The problem of deep NN is more about saddle points than about local minimum because deep NN has high dimensions (Parameters)</li> <li>Mini batches are noisy because the gradient is not taken for the whole batch.</li> </ul> </li> <li> <p>SGD + momentum:</p> <ul> <li> <p>Build up velocity as a running mean of gradients:</p> </li> <li> <p><code>python   # Computing weighted average. rho best is in range [0.9 - 0.99]   V[t+1] = rho * v[t] + dx   x[t+1] = x[t] - learningRate * V[t+1]</code></p> </li> <li> <p><code>V[0]</code> is zero.</p> </li> <li> <p>Solves the saddle point and local minimum problems.</p> </li> <li> <p>It overshoots the problem and returns to it back.</p> </li> </ul> </li> <li> <p>Nestrov momentum:</p> <ul> <li> <p><code>python   dx = compute_gradient(x)   old_v = v   v = rho * v - learning_rate * dx   x+= -rho * old_v + (1+rho) * v</code></p> </li> <li> <p>Doesn't overshoot the problem but slower than SGD + momentum</p> </li> </ul> </li> <li> <p>AdaGrad</p> <ul> <li> <p>```python   grad_squared = 0   while(True):     dx = compute_gradient(x)</p> </li> </ul> </li> <li> <p>RMSProp</p> <ul> <li> <p>```python   grad_squared = 0   while(True):     dx = compute_gradient(x)</p> </li> <li> <p>People uses this instead of AdaGrad</p> </li> </ul> </li> <li> <p>Adam</p> <ul> <li>Calculates the momentum and RMSProp as the gradients.</li> <li>It need a Fixing bias to fix starts of gradients.</li> <li>Is the best technique so far runs best on a lot of problems.</li> <li>With <code>beta1 = 0.9</code> and <code>beta2 = 0.999</code> and <code>learning_rate = 1e-3</code> or <code>5e-4</code> is a great starting point for many models!</li> </ul> </li> <li> <p>Learning decay</p> <ul> <li>Ex. decay learning rate by half every few epochs.</li> <li>To help the learning rate not to bounce out.</li> <li>Learning decay is common with SGD+momentum but not common with Adam.</li> <li>Dont use learning decay from the start at choosing your hyperparameters. Try first and check if you need decay or not.</li> </ul> </li> <li> <p>All the above algorithms we have discussed is a first order optimization.</p> </li> <li> <p>Second order optimization</p> <ul> <li>Use gradient and Hessian to from quadratic approximation.</li> <li>Step to the minima of the approximation.</li> <li>What is nice about this update?</li> <li>It doesn't has a learning rate in some of the versions.</li> <li>But its unpractical for deep learning</li> <li>Has O(N^2) elements.</li> <li>Inverting takes O(N^3).</li> <li>L-BFGS is a version of second order optimization</li> <li>Works with batch optimization but not with mini-batches.</li> </ul> </li> <li> <p>In practice first use ADAM and if it didn't work try L-BFGS.</p> </li> <li> <p>Some says all the famous deep architectures uses SGS + Nestrov momentum</p> </li> <li> <p>Regularization</p> </li> <li> <p>So far we have talked about reducing the training error, but we care about most is how our model will handle unseen data!</p> </li> <li>What if the gab of the error between training data and validation data are too large?</li> <li>This error is called high variance.</li> <li>Model Ensembles:<ul> <li>Algorithm:</li> <li>Train multiple independent models of the same architecture with different initializations.</li> <li>At test time average their results.</li> <li>It can get you extra 2% performance.</li> <li>It reduces the generalization error.</li> <li>You can use some snapshots of your NN at the training ensembles them and take the results.</li> </ul> </li> <li>Regularization solves the high variance problem. We have talked about L1, L2 Regularization.</li> <li>Some Regularization techniques are designed for only NN and can do better.</li> <li>Drop out:<ul> <li>In each forward pass, randomly set some of the neurons to zero. Probability of dropping is a hyperparameter that are 0.5 for almost cases.</li> <li>So you will chooses some activation and makes them zero.</li> <li>It works because:</li> <li>It forces the network to have redundant representation; prevent co-adaption of features!</li> <li>If you think about this, It ensemble some of the models in the same model!</li> <li>At test time we might multiply each dropout layer by the probability of the dropout.</li> <li>Sometimes at test time we don't multiply anything and leave it as it is.</li> <li>With drop out it takes more time to train.</li> </ul> </li> <li>Data augmentation:<ul> <li>Another technique that makes Regularization.</li> <li>Change the data!</li> <li>For example flip the image, or rotate it.</li> <li>Example in ResNet:</li> <li>Training: Sample random crops and scales:<ol> <li>Pick random L in range [256,480]</li> <li>Resize training image, short side = L</li> <li>Sample random 224x244 patch.</li> </ol> </li> <li>Testing: average a fixed set of crops<ol> <li>Resize image at 5 scales: {224, 256, 384, 480, 640}</li> <li>For each size, use 10 224x224 crops: 4 corners + center + flips</li> </ol> </li> <li>Apply Color jitter or PCA</li> <li>Translation, rotation, stretching.</li> </ul> </li> <li>Drop connect<ul> <li>Like drop out idea it makes a regularization.</li> <li>Instead of dropping the activation, we randomly zeroing the weights.</li> </ul> </li> <li>Fractional Max Pooling<ul> <li>Cool regularization idea. Not commonly used.</li> <li>Randomize the regions in which we pool.</li> </ul> </li> <li> <p>Stochastic depth</p> <ul> <li>New idea.</li> <li>Eliminate layers, instead on neurons.</li> <li>Has the similar effect of drop out but its a new idea.</li> </ul> </li> <li> <p>Transfer learning:</p> </li> <li> <p>Some times your data is overfitted by your model because the data is small not because of regularization.</p> </li> <li> <p>You need a lot of data if you want to train/use CNNs.</p> </li> <li> <p>Steps of transfer learning</p> <ol> <li>Train on a big dataset that has common features with your dataset. Called pretraining.</li> <li>Freeze the layers except the last layer and feed your small dataset to learn only the last layer.</li> <li>Not only the last layer maybe trained again, you can fine tune any number of layers you want based on the number of data you have</li> </ol> </li> <li> <p>Guide to use transfer learning:</p> <ul> <li> Very Similar dataset very different dataset very little dataset Use Linear classifier on top layer You're in trouble.. Try linear classifier from different stages quite a lot of data Finetune a few layers Finetune a large layers </li> </ul> </li> <li> <p>Transfer learning is the normal not an exception.</p> </li> </ul>"},{"location":"CS/ML/CS231n/#here-is-a-problem-the-grad_squared-isnt-decayed-gets-so-large","title":"here is a problem, the grad_squared isn't decayed (gets so large)","text":"<p>grad_squared += dx * dx         </p> <p>x -= (learning_rate*dx) / (np.sqrt(grad_squared) + 1e-7)       ```</p>"},{"location":"CS/ML/CS231n/#solved-adagra","title":"Solved ADAgra","text":"<p>grad_squared = decay_rate * grad_squared + (1-grad_squared) * dx * dx  </p> <p>x -= (learning_rate*dx) / (np.sqrt(grad_squared) + 1e-7)       ```</p>"},{"location":"CS/ML/CS231n/#08-deep-learning-software","title":"08. Deep learning software","text":"<ul> <li>This section changes a lot every year in CS231n due to rabid changes in the deep learning softwares.</li> <li>CPU vs GPU</li> <li>GPU The graphics card was developed to render graphics to play games or make 3D media,. etc.<ul> <li>NVIDIA vs AMD</li> <li>Deep learning choose NVIDIA over AMD GPU because NVIDIA is pushing research forward deep learning also makes it architecture more suitable for deep learning.</li> </ul> </li> <li>CPU has fewer cores but each core is much faster and much more capable; great at sequential tasks. While GPUs has more cores but each core is much slower \"dumber\"; great for parallel tasks.</li> <li>GPU cores needs to work together. and has its own memory.</li> <li>Matrix multiplication is from the operations that are suited for GPUs. It has MxN independent operations that can be done on parallel.</li> <li>Convolution operation also can be paralyzed because it has independent operations.</li> <li>Programming GPUs frameworks:<ul> <li>CUDA (NVIDIA only)</li> <li>Write c-like code that runs directly on the GPU.</li> <li>Its hard to build a good optimized code that runs on GPU. Thats why they provided high level APIs.</li> <li>Higher level APIs: cuBLAS, cuDNN, etc</li> <li>CuDNN has implemented back prop. , convolution, recurrent and a lot more for you!</li> <li>In practice you won't write a parallel code. You will use the code implemented and optimized by others!</li> <li>OpenCl</li> <li>Similar to CUDA, but runs on any GPU.</li> <li>Usually Slower .</li> <li>Haven't much support yet from all deep learning softwares.</li> </ul> </li> <li>There are a lot of courses for learning parallel programming.</li> <li>If you aren't careful, training can bottleneck on reading dara and transferring to GPU. So the solutions are:<ul> <li>Read all the data into RAM. # If possible</li> <li>Use SSD instead of HDD</li> <li>Use multiple CPU threads to prefetch data!</li> <li>While the GPU are computing, a CPU thread will fetch the data for you.</li> <li>A lot of frameworks implemented that for you because its a little bit painful!</li> </ul> </li> <li>Deep learning Frameworks</li> <li>Its super fast moving!</li> <li>Currently available frameworks:<ul> <li>Tensorflow (Google)</li> <li>Caffe (UC Berkeley)</li> <li>Caffe2 (Facebook)</li> <li>Torch (NYU / Facebook)</li> <li>PyTorch (Facebook)</li> <li>Theano (U monteral) </li> <li>Paddle (Baidu)</li> <li>CNTK (Microsoft)</li> <li>MXNet (Amazon)</li> </ul> </li> <li>The instructor thinks that you should focus on Tensorflow and PyTorch.</li> <li>The point of deep learning frameworks:<ul> <li>Easily build big computational graphs.</li> <li>Easily compute gradients in computational graphs.</li> <li>Run it efficiently on GPU (cuDNN - cuBLAS)</li> </ul> </li> <li>Numpy doesn't run on GPU.</li> <li>Most of the frameworks tries to be like NUMPY in the forward pass and then they compute the gradients for you.</li> <li>Tensorflow (Google)</li> <li>Code are two parts:<ol> <li>Define computational graph.</li> <li>Run the graph and reuse it many times.</li> </ol> </li> <li>Tensorflow uses a static graph architecture.</li> <li>Tensorflow variables live in the graph. while the placeholders are feed each run.</li> <li>Global initializer function initializes the variables that lives in the graph.</li> <li>Use predefined optimizers and losses.</li> <li>You can make a full layers with layers.dense function.</li> <li>Keras (High level wrapper):<ul> <li>Keras is a layer on top pf Tensorflow, makes common things easy to do.</li> <li>So popular!</li> <li>Trains a full deep NN in a few lines of codes.</li> </ul> </li> <li>There are a lot high level wrappers:<ul> <li>Keras</li> <li>TFLearn</li> <li>TensorLayer</li> <li>tf.layers   <code>#Ships with tensorflow</code></li> <li>tf-Slim   <code>#Ships with tensorflow</code></li> <li>tf.contrib.learn   <code>#Ships with tensorflow</code></li> <li>Sonnet <code># New from deep mind</code></li> </ul> </li> <li>Tensorflow has pretrained models that you can use while you are using transfer learning.</li> <li>Tensorboard adds logging to record loss, stats. Run server and get pretty graphs!</li> <li>It has distributed code if you want to split your graph on some nodes.</li> <li> <p>Tensorflow is actually inspired from Theano. It has the same inspirations and structure.</p> </li> <li> <p>PyTorch (Facebook)</p> </li> <li> <p>Has three layers of abstraction:</p> <ul> <li>Tensor: <code>ndarray</code> but runs on GPU     <code>#Like numpy arrays in tensorflow</code></li> <li>Variable: Node in a computational graphs; stores data and gradient <code>#Like Tensor, Variable, Placeholders</code></li> <li>Module: A NN layer; may store state or learnable weights<code>#Like tf.layers in tensorflow</code></li> </ul> </li> <li>In PyTorch the graphs runs in the same loop you are executing which makes it easier for debugging. This is called a dynamic graph.</li> <li>In PyTorch you can define your own autograd functions by writing forward and backward for tensors. Most of the times it will implemented for you.</li> <li>Torch.nn is a high level api like keras in tensorflow. You can create the models and go on and on.<ul> <li>You can define your own nn module!</li> </ul> </li> <li>Also Pytorch contains optimizers like tensorflow.</li> <li>It contains a data loader that wraps a Dataset and provides minbatches, shuffling and multithreading.</li> <li>PyTorch contains the best and super easy to use pretrained models</li> <li>PyTorch contains Visdom that are like tensorboard. but Tensorboard seems to be more powerful.</li> <li>PyTorch is new and still evolving compared to Torch. Its still in beta state.</li> <li> <p>PyTorch is best for research.</p> </li> <li> <p>Tensorflow builds the graph once, then run them many times (Called static graph)</p> </li> <li> <p>In each PyTorch iteration we build a new graph (Called dynamic graph)</p> </li> <li> <p>Static vs dynamic graphs:</p> </li> <li> <p>Optimization:</p> <ul> <li>With static graphs, framework can optimize the graph for you before it runs.</li> </ul> </li> <li> <p>Serialization</p> <ul> <li> <p>Static: Once graph is built, can serialize it and run it without the code that built the graph. Ex use the graph in c++</p> </li> <li> <p>Dynamic: Always need to keep the code around.</p> </li> </ul> </li> <li> <p>Conditional</p> <ul> <li>Is easier in dynamic graphs. And more complicated in static graphs.</li> </ul> </li> <li> <p>Loops:</p> <ul> <li>Is easier in dynamic graphs. And more complicated in static graphs.</li> </ul> </li> <li> <p>Tensorflow fold make dynamic graphs easier in Tensorflow through dynamic batching.</p> </li> <li> <p>Dynamic graph applications include: recurrent networks and recursive networks.</p> </li> <li> <p>Caffe2 uses static graphs and can train model in python also works on IOS and Android</p> </li> <li> <p>Tensorflow/Caffe2 are used a lot in production especially on mobile.</p> </li> </ul>"},{"location":"CS/ML/CS231n/#09-cnn-architectures","title":"09. CNN architectures","text":"<ul> <li> <p>This section talks about the famous CNN architectures. Focuses on CNN architectures that won ImageNet competition since 2012.</p> </li> <li> <p></p> </li> <li> <p>These architectures includes: AlexNet, VGG, GoogLeNet, and ResNet.</p> </li> <li> <p>Also we will discuss some interesting architectures as we go.</p> </li> <li> <p>The first ConvNet that was made was LeNet-5 architectures are:by Yann Lecun at 1998.</p> </li> <li> <p>Architecture are: <code>CONV-POOL-CONV-POOL-FC-FC-FC</code></p> <ul> <li></li> </ul> </li> <li>Each conv filters was <code>5x5</code> applied at stride 1</li> <li>Each pool was <code>2x2</code> applied at stride <code>2</code></li> <li>It was useful in Digit recognition.</li> <li>In particular the insight that image features are distributed across the entire image, and convolutions with learnable parameters are an effective way to extract similar features at multiple location with few parameters.</li> <li> <p>It contains exactly 5  layers</p> </li> <li> <p>In 2010 Dan Claudiu Ciresan and Jurgen Schmidhuber published one of the very fist implementations of GPU Neural nets. This implementation had both forward and backward implemented on a a NVIDIA GTX 280 graphic processor of an up to 9 layers neural network.</p> </li> <li> <p>AlexNet (2012):</p> </li> <li> <p>ConvNet that started the evolution and wins the ImageNet at 2012.</p> </li> <li>Architecture are: <code>CONV1-MAXPOOL1-NORM1-CONV2-MAXPOOL2-NORM2-CONV3-CONV4-CONV5-MAXPOOL3-FC6-FC7-FC8</code></li> <li>Contains exactly 8 layers the first 5 are Convolutional and the last 3 are fully connected layers.</li> <li>AlexNet accuracy error was <code>16.4%</code></li> <li>For example if the input is 227 x 227 x3 then these are the shapes of the of the outputs at each layer:<ul> <li>CONV1 (96 11 x 11 filters at stride 4, pad 0)</li> <li>Output shape <code>(55,55,96)</code>,   Number of weights are <code>(11*11*3*96)+96 = 34944</code></li> <li>MAXPOOL1 (3 x 3 filters applied at stride 2)</li> <li>Output shape <code>(27,27,96)</code>,   No Weights</li> <li>NORM1</li> <li>Output shape <code>(27,27,96)</code>,  We don't do this any more</li> <li>CONV2 (256 5 x 5 filters at stride 1, pad 2)</li> <li>MAXPOOL2 (3 x 3 filters at stride 2)</li> <li>NORM2</li> <li>CONV3 (384 3 x 3 filters ar stride 1, pad 1)</li> <li>CONV4 (384 3 x 3 filters ar stride 1, pad 1)</li> <li>CONV5 (256 3 x 3 filters ar stride 1, pad 1)</li> <li>MAXPOOL3 (3 x 3 filters at stride 2)</li> <li>Output shape <code>(6,6,256)</code></li> <li>FC6 (4096)</li> <li>FC7 (4096)</li> <li>FC8 (1000 neurons for class score)</li> </ul> </li> <li>Some other details:<ul> <li>First use of RELU.</li> <li>Norm layers but not used any more.</li> <li>heavy data augmentation</li> <li>Dropout <code>0.5</code></li> <li>batch size <code>128</code></li> <li>SGD momentum <code>0.9</code></li> <li>Learning rate <code>1e-2</code> reduce by 10 at some iterations</li> <li>7 CNN ensembles!</li> </ul> </li> <li>AlexNet was trained on GTX 580 GPU with only 3 GB which wasn't enough to train in one machine so they have spread the feature maps in half. The first AlexNet was distributed!</li> <li>Its still used in transfer learning in a lot of tasks.</li> <li> <p>Total number of parameters are <code>60 million</code></p> </li> <li> <p>ZFNet (2013)</p> </li> <li> <p>Won in 2013 with error 11.7%</p> </li> <li>It has the same general structure but they changed a little in hyperparameters to get the best output.</li> <li>Also contains 8 layers.</li> <li> <p>AlexNet but:</p> <ul> <li><code>CONV1</code>: change from (11 x 11 stride 4) to (7 x 7 stride 2)</li> <li><code>CONV3,4,5</code>: instead of 384, 384, 256 filters use 512, 1024, 512</li> </ul> </li> <li> <p>OverFeat (2013)</p> </li> <li> <p>Won the localization in imageNet in 2013</p> </li> <li> <p>We show how a multiscale and sliding window approach can be efficiently implemented within a ConvNet. We also introduce a novel deep learning approach to localization by learning to predict object boundaries.</p> </li> <li> <p>VGGNet (2014) (Oxford)</p> </li> <li> <p>Deeper network with more layers.</p> </li> <li>Contains 19 layers.</li> <li>Won on 2014 with GoogleNet with error 7.3%</li> <li>Smaller filters with deeper layers.</li> <li>The great advantage of VGG was the insight that multiple 3 \u00d7 3 convolution in sequence can emulate the effect of larger receptive fields, for examples 5 \u00d7 5 and 7 \u00d7 7.</li> <li>Used the simple 3 x 3 Conv all through the network.<ul> <li>3 (3 x 3) filters has the same effect as 7 x 7</li> </ul> </li> <li></li> <li>The Architecture contains several CONV layers then POOL layer over 5 times and then the full connected layers.</li> <li>It has a total memory of 96MB per image for only forward propagation!<ul> <li>Most memory are in the earlier layers</li> </ul> </li> <li>Total number of parameters are 138 million<ul> <li>Most of the parameters are in the fully connected layers</li> </ul> </li> <li>Has a similar details in training like AlexNet. Like using momentum and dropout.</li> <li> <p>VGG19 are an upgrade for VGG16 that are slightly better but with more memory</p> <ul> <li></li> </ul> </li> <li> <p>GoogleNet (2014)</p> </li> <li> <p>Deeper network with more layers.</p> </li> <li>Contains 22 layers.</li> <li>It has Efficient Inception module.</li> <li>Only 5 million parameters! 12x less than AlexNet</li> <li>Won on 2014 with VGGNet with error 6.7%</li> <li>Inception module:<ul> <li>Design a good local network topology (network within a network (NiN)) and then stack these modules on top of each other.</li> <li>It consists of:</li> <li>Apply parallel filter operations on the input from previous layer<ul> <li>Multiple convs of sizes (1 x 1, 3 x 3, 5 x 5) </li> <li>Adds padding to maintain the sizes.</li> <li>Pooling operation. (Max Pooling)</li> <li>Adds padding to maintain the sizes.</li> </ul> </li> <li>Concatenate all filter outputs together depth-wise.</li> <li>For example:</li> <li>Input for inception module is 28 x 28 x 256</li> <li>Then the parallel filters applied:<ul> <li>(1 x 1), 128 filter               <code># output shape (28,28,128)</code></li> <li>(3 x 3), 192 filter                 <code># output shape (28,28,192)</code></li> <li>(5 x 5), 96 filter                   <code># output shape (28,28,96)</code></li> <li>(3 x 3) Max pooling            <code># output shape (28,28,256)</code></li> </ul> </li> <li>After concatenation this will be <code>(28,28,672)</code></li> <li>By this design -We call Naive- it has a big computation complexity.</li> <li>The last example will make:<ul> <li>[1 x 1 conv, 128] ==&gt; 28 * 28 * 128 * 1 * 1 * 256 = 25 Million approx</li> <li>[3 x 3 conv, 192] ==&gt; 28 * 28 * 192 3 3 * 256 = 346 Million approx</li> <li>[5 x 5 conv, 96] ==&gt; 28 * 28 * 96 * 5 * 5 * 256 = 482 Million approx</li> <li>In total around 854 Million operation!</li> </ul> </li> <li>Solution: bottleneck layers that use 1x1 convolutions to reduce feature depth.</li> <li>Inspired from NiN (Network in network)</li> <li></li> <li>The bottleneck solution will make a total operations of 358M on this example which is good compared with the naive implementation.</li> </ul> </li> <li>So GoogleNet stacks this Inception module multiple times to get a full architecture of a network that can solve a problem without the Fully connected layers.</li> <li>Just to mention, it uses an average pooling layer at the end before the classification step.</li> <li>Full architecture:<ul> <li></li> </ul> </li> <li>In February 2015 Batch-normalized Inception was introduced as Inception V2. Batch-normalization computes the mean and standard-deviation of all feature maps at the output of a layer, and normalizes their responses with these values.</li> <li> <p>In December 2015 they introduced a paper \"Rethinking the Inception Architecture for Computer Vision\" which explains the older inception models well also introducing a new version V3.</p> </li> <li> <p>The first GoogleNet and VGG was before batch normalization invented so they had some hacks to train the NN and converge well.</p> </li> <li> <p>ResNet (2015) (Microsoft Research)</p> </li> <li> <p>152-layer model for ImageNet. Winner by 3.57% which is more than human level error.</p> </li> <li> <p>This is also the very first time that a network of &gt; hundred, even 1000 layers was trained.</p> </li> <li> <p>Swept all classification and detection competitions in ILSVRC\u201915 and COCO\u201915!</p> </li> <li> <p>What happens when we continue stacking deeper layers on a \u201cplain\u201d Convolutional neural network?</p> <ul> <li>The deeper model performs worse, but it\u2019s not caused by overfitting!</li> <li>The learning stops performs well somehow because deeper NN are harder to optimize!</li> </ul> </li> <li> <p>The deeper model should be able to perform at least as well as the shallower model.</p> </li> <li> <p>A solution by construction is copying the learned layers from the shallower model and setting additional layers to identity mapping.</p> </li> <li> <p>Residual block:</p> <ul> <li> <p>Microsoft came with the Residual block which has this architecture:</p> </li> <li> <p></p> </li> <li> <p><code>python   # Instead of us trying To learn a new representation, We learn only Residual   Y = (W2* RELU(W1x+b1) + b2) + X</code></p> </li> <li> <p>Say you have a network till a depth of N layers. You only want to add a new layer if you get something extra out of adding that layer.</p> </li> <li> <p>One way to ensure this new (N+1)th layer learns something new about your network is to also provide the input(x) without any transformation to the output of the (N+1)th layer. This essentially drives the new layer to learn something different from what the input has already encoded.</p> </li> <li> <p>The other advantage is such connections help in handling the Vanishing gradient problem in very deep networks.</p> </li> </ul> </li> <li> <p>With the Residual block we can now have a deep NN of any depth without the fearing that we can't optimize the network.</p> </li> <li> <p>ResNet with a large number of layers started to use a bottleneck layer similar to the Inception bottleneck to reduce the dimensions.</p> <ul> <li></li> </ul> </li> <li> <p>Full ResNet architecture:</p> <ul> <li>Stack residual blocks.</li> <li></li> <li>Every residual block has two 3 x 3 conv layers.</li> <li>Additional conv layer at the beginning.</li> <li>No FC layers at the end (only FC 1000 to output classes)</li> <li>Periodically, double number of filters and downsample spatially using stride 2 (/2 in each dimension)</li> <li>Training ResNet in practice:</li> <li>Batch Normalization after every CONV layer.</li> <li>Xavier/2 initialization from He et al.</li> <li>SGD + Momentum (<code>0.9</code>) </li> <li>Learning rate: 0.1, divided by 10 when validation error plateaus</li> <li>Mini-batch size <code>256</code></li> <li>Weight decay of <code>1e-5</code></li> <li>No dropout used.</li> </ul> </li> <li> <p>Inception-v4: Resnet + Inception and was founded in 2016.</p> </li> <li> <p>The complexity comparing over all the architectures:</p> </li> <li> <p></p> </li> <li>VGG: Highest memory, most operations.</li> <li> <p>GoogLeNet: most efficient.</p> </li> <li> <p>ResNets Improvements:</p> </li> <li> <p>(2016) Identity Mappings in Deep Residual Networks</p> <ul> <li>From the creators of ResNet.</li> <li>Gives better performance.</li> </ul> </li> <li>(2016) Wide Residual Networks<ul> <li>Argues that residuals are the important factor, not depth</li> <li>50-layer wide ResNet outperforms 152-layer original ResNet</li> <li>Increasing width instead of depth more computationally efficient (parallelizable)</li> </ul> </li> <li> <p>(2016) Deep Networks with Stochastic Depth</p> <ul> <li>Motivation: reduce vanishing gradients and training time through short networks during training.</li> <li>Randomly drop a subset of layers during each training pass</li> <li>Use full deep network at test time.</li> </ul> </li> <li> <p>Beyond ResNets:</p> </li> <li> <p>(2017) FractalNet: Ultra-Deep Neural Networks without Residuals</p> <ul> <li>Argues that key is transitioning effectively from shallow to deep and residual representations are not necessary.</li> <li>Trained with dropping out sub-paths</li> <li>Full network at test time.</li> </ul> </li> <li>(2017) Densely Connected Convolutional Networks</li> <li> <p>(2017) SqueezeNet: AlexNet-level Accuracy With 50x Fewer Parameters and &lt;0.5Mb Model Size</p> <ul> <li>Good for production.</li> <li>It is a re-hash of many concepts from ResNet and Inception, and show that after all, a better design of architecture will deliver small network sizes and parameters without needing complex compression algorithms.</li> </ul> </li> <li> <p>Conclusion:</p> </li> <li> <p>ResNet current best default.</p> </li> <li>Trend towards extremely deep networks</li> <li>In the last couple of years, some models all using the shortcuts like \"ResNet\" to eaisly flow the gradients.</li> </ul>"},{"location":"CS/ML/CS231n/#10-recurrent-neural-networks","title":"10. Recurrent Neural networks","text":"<ul> <li> <p>Vanilla Neural Networks \"Feed neural networks\", input of fixed size goes through some hidden units and then go to output. We call it a one to one network.</p> </li> <li> <p>Recurrent Neural Networks RNN Models:</p> </li> <li> <p></p> </li> <li>One to many<ul> <li>Example: Image Captioning</li> <li>image ==&gt; sequence of words</li> </ul> </li> <li>Many to One<ul> <li>Example: Sentiment Classification</li> <li>sequence of words ==&gt; sentiment</li> </ul> </li> <li> <p>Many to many</p> <ul> <li>Example: Machine Translation</li> <li>seq of words in one language ==&gt; seq of words in another language</li> <li>Example: Video classification on frame level</li> </ul> </li> <li> <p>RNNs can also work for Non-Sequence Data (One to One problems)</p> </li> <li> <p>It worked in Digit classification through taking a series of \u201cglimpses\u201d</p> <ul> <li>\u201cMultiple Object Recognition with Visual Attention\u201d, ICLR 2015.</li> </ul> </li> <li> <p>It worked on generating images one piece at a time</p> <ul> <li>i.e generating a captcha</li> </ul> </li> <li> <p>So what is a recurrent neural network?</p> </li> <li> <p>Recurrent core cell that take an input x and that cell has an internal state that are updated each time it reads an input.</p> </li> <li> <p></p> </li> <li> <p>The RNN block should return a vector.</p> </li> <li> <p>We can process a sequence of vectors x by applying a recurrence formula at every time step:</p> <ul> <li> <p><code>python   h[t] = fw (h[t-1], x[t])          # Where fw is some function with parameters W</code></p> </li> <li> <p>The same function and the same set of parameters are used at every time step.</p> </li> </ul> </li> <li> <p>(Vanilla) Recurrent Neural Network:</p> <ul> <li> <p><code>h[t] = tanh (W[h,h]*h[t-1] + W[x,h]*x[t])    # Then we save h[t]   y[t] = W[h,y]*h[t]</code></p> </li> <li> <p>This is the simplest example of a RNN.</p> </li> </ul> </li> <li> <p>RNN works on a sequence of related data.</p> </li> <li> <p>Recurrent NN Computational graph:</p> </li> <li> <p></p> </li> <li><code>h0</code> are initialized to zero.</li> <li>Gradient of <code>W</code> is the sum of all the <code>W</code> gradients that has been calculated!</li> <li>A many to many graph:<ul> <li></li> <li>Also the last is the sum of all losses and the weights of Y is one and is updated through summing all the gradients!</li> </ul> </li> <li>A many to one graph:<ul> <li></li> </ul> </li> <li>A one to many graph:<ul> <li></li> </ul> </li> <li> <p>sequence to sequence graph:</p> <ul> <li></li> <li>Encoder and decoder philosophy.</li> </ul> </li> <li> <p>Examples:</p> </li> <li> <p>Suppose we are building words using characters. We want a model to predict the next character of a sequence. Lets say that the characters are only <code>[h, e, l, o]</code> and the words are [hello]</p> <ul> <li>Training:</li> <li></li> <li>Only the third prediction here is true. The loss needs to be optimized.</li> <li>We can train the network by feeding the whole word(s).</li> <li>Testing time:</li> <li></li> <li>At test time we work with a character by character. The output character will be the next input with the other saved hidden activations.</li> <li>This link contains all the code but uses Truncated Backpropagation through time as we will discuss.</li> </ul> </li> <li> <p>Backpropagation through time Forward through entire sequence to compute loss, then backward through entire sequence to compute gradient.</p> </li> <li> <p>But if we choose the whole sequence it will be so slow and take so much memory and will never converge!</p> </li> <li> <p>So in practice people are doing \"Truncated Backpropagation through time\" as we go on we Run forward and backward through chunks of the sequence instead of whole sequence</p> </li> <li> <p>Then Carry hidden states forward in time forever, but only backpropagate for some smaller number of steps.</p> </li> <li> <p>Example on image captioning:</p> </li> <li> <p></p> </li> <li>They use  token to finish running. <li> <p>The biggest dataset for image captioning is Microsoft COCO.</p> </li> <li> <p>Image Captioning with Attention is a project in which when the RNN is generating captions, it looks at a specific part of the image not the whole image.</p> </li> <li> <p>Image Captioning with Attention technique is also used in \"Visual Question Answering\" problem</p> </li> <li> <p>Multilayer RNNs is generally using some layers as the hidden layer that are feed into again. LSTM is a multilayer RNNs.</p> </li> <li> <p>Backward flow of gradients in RNN can explode or vanish. Exploding is controlled with gradient clipping. Vanishing is controlled with additive interactions (LSTM)</p> </li> <li> <p>LSTM stands for Long Short Term Memory. It was designed to help the vanishing gradient problem on RNNs.</p> </li> <li> <p>It consists of:</p> <ul> <li>f: Forget gate, Whether to erase cell</li> <li>i: Input gate, whether to write to cell</li> <li>g: Gate gate (?), How much to write to cell</li> <li>o: Output gate, How much to reveal cell</li> </ul> </li> <li></li> <li></li> <li>The LSTM gradients are easily computed like ResNet</li> <li> <p>The LSTM is keeping data on the long or short memory as it trains means it can remember not just the things from last layer but layers.</p> </li> <li> <p>Highway networks is something between ResNet and LSTM that is still in research.</p> </li> <li> <p>Better/simpler architectures are a hot topic of current research</p> </li> <li> <p>Better understanding (both theoretical and empirical) is needed.</p> </li> <li> <p>RNN is used for problems that uses sequences of related inputs more. Like NLP and Speech recognition.</p> </li>"},{"location":"CS/ML/CS231n/#11-detection-and-segmentation","title":"11. Detection and Segmentation","text":"<ul> <li> <p>So far we are talking about image classification problem. In this section we will talk about Segmentation, Localization, Detection.</p> </li> <li> <p>Semantic Segmentation</p> </li> <li> <p>We want to Label each pixel in the image with a category label.</p> </li> <li> <p></p> </li> <li> <p>As you see the cows in the image, Semantic Segmentation Don\u2019t differentiate instances, only care about pixels.</p> </li> <li> <p>The first idea is to use a sliding window. We take a small window size and slide it all over the picture. For each window we want to label the center pixel.</p> <ul> <li>It will work but its not a good idea because it will be computational expensive!</li> <li>Very inefficient! Not reusing shared features between overlapping patches.</li> <li>In practice nobody uses this.</li> </ul> </li> <li> <p>The second idea is designing a network as a bunch of Convolutional layers to make predictions for pixels all at once!</p> <ul> <li>Input is the whole image. Output is the image with each pixel labeled.</li> <li>We need a lot of labeled data. And its very expensive data.</li> <li>It needs a deep Conv. layers.</li> <li>The loss is cross entropy between each pixel provided.</li> <li>Data augmentation are good here.</li> <li>The problem with this implementation that convolutions at original image resolution will be very expensive.</li> <li>So in practice we don't see something like this right now.</li> </ul> </li> <li> <p>The third idea is based on the last idea. The difference is that we are downsampling and upsampling inside the network.</p> <ul> <li> <p>We downsample because using the whole image as it is very expensive. So we go on multiple layers downsampling and then upsampling in the end.</p> </li> <li> <p>Downsampling is an operation like Pooling and strided convolution.</p> </li> <li> <p>Upsampling is like \"Nearest Neighbor\" or \"Bed of Nails\" or \"Max unpooling\"</p> </li> <li> <p>Nearest Neighbor example:</p> <ul> <li><code>Input:   1  2               Output:   1  1  2  2            3  4                         1  1  2  2                                         3  3  4  4                                         3  3  4  4</code></li> </ul> </li> <li> <p>Bed of Nails example:</p> <ul> <li><code>Input:   1  2               Output:   1  0  2  0            3  4                         0  0  0  0                                         3  0  4  0                                         0  0  0  0</code></li> </ul> </li> <li> <p>Max unpooling is depending on the earlier steps that was made by max pooling. You fill the pixel where max pooling took place and then fill other pixels by zero.</p> </li> <li> <p>Max unpooling seems to be the best idea for upsampling.</p> </li> <li> <p>There are an idea of Learnable Upsampling called \"Transpose Convolution\"</p> </li> <li> <p>Rather than making a convolution we make the reverse. </p> </li> <li>Also called:<ul> <li>Upconvolution.</li> <li>Fractionally strided convolution</li> <li>Backward strided convolution</li> </ul> </li> <li>Learn the artimitic of the upsampling please refer to chapter 4 in this paper.</li> </ul> </li> <li> <p>Classification + Localization:</p> </li> <li> <p>In this problem we want to classify the main object in the image and its location as a rectangle.</p> </li> <li>We assume there are one object.</li> <li>We will create a multi task NN. The architecture are as following:<ul> <li>Convolution network layers connected to:</li> <li>FC layers that classify the object. <code># The plain classification problem we know</code></li> <li>FC layers that connects to a four numbers <code>(x,y,w,h)</code><ul> <li>We treat Localization as a regression problem.</li> </ul> </li> </ul> </li> <li>This problem will have two losses:<ul> <li>Softmax loss for classification</li> <li>Regression (Linear loss) for the localization (L2 loss)</li> </ul> </li> <li>Loss = SoftmaxLoss + L2 loss</li> <li>Often the first Conv layers are pretrained NNs like AlexNet!</li> <li> <p>This technique can be used in so many other problems like:  Human Pose Estimation.</p> </li> <li> <p>Object Detection</p> </li> <li> <p>A core idea of computer vision. We will talk by details in this problem.</p> </li> <li>The difference between \"Classification + Localization\" and this problem is that here we want to detect one or mode different objects and its locations!</li> <li>First idea is to use a sliding window<ul> <li>Worked well and long time.</li> <li>The steps are:</li> <li>Apply a CNN to many different crops of the image, CNN classifies each crop as object or background.</li> <li>The problem is we need  to apply CNN to huge number of locations and scales, very computationally expensive!</li> <li>The brute force sliding window will make us take thousands of thousands of time.</li> </ul> </li> <li>Region Proposals will help us deciding which region we should run our NN at:<ul> <li>Find blobby image regions that are likely to contain objects.</li> <li>Relatively fast to run; e.g. Selective Search gives 1000 region proposals in a few seconds on CPU</li> </ul> </li> <li>So now we can apply one of the Region proposals networks and then apply the first idea.</li> <li>There is another idea which is called R-CNN<ul> <li></li> <li>The idea is bad because its taking parts of the image -With Region Proposals- if different sizes and feed it to CNN after scaling them all to one size. Scaling is bad</li> <li>Also its very slow.</li> </ul> </li> <li>Fast R-CNN is another idea that developed on R-CNN<ul> <li></li> <li>It uses one CNN to do everything.</li> </ul> </li> <li>Faster R-CNN does its own region proposals by Inserting Region Proposal Network (RPN) to predict proposals from features.<ul> <li>The fastest of the R-CNNs.</li> </ul> </li> <li>Another idea is Detection without Proposals: YOLO / SSD<ul> <li>YOLO stands for you only look once.</li> <li>YOLO/SDD is two separate algorithms.</li> <li>Faster but not as accurate.</li> </ul> </li> <li> <p>Takeaways</p> <ul> <li>Faster R-CNN is slower but more accurate.</li> <li>SSD/YOLO is much faster but not as accurate.</li> </ul> </li> <li> <p>Denese Captioning</p> </li> <li> <p>Denese Captioning is \"Object Detection + Captioning\"</p> </li> <li> <p>Paper that covers this idea can be found here.</p> </li> <li> <p>Instance Segmentation</p> </li> <li> <p>This is like the full problem.</p> </li> <li></li> <li>Rather than we want to predict the bounding box, we want to know which pixel label but also distinguish them.</li> <li>There are a lot of ideas.</li> <li>There are a new idea \"Mask R-CNN\"<ul> <li>Like R-CNN but inside it we apply the Semantic Segmentation</li> <li>There are a lot of good results out of this paper.</li> <li>It sums all the things that we have discussed in this lecture.</li> <li>Performance of this seems good.</li> </ul> </li> </ul>"},{"location":"CS/ML/CS231n/#12-visualizing-and-understanding","title":"12. Visualizing and Understanding","text":"<ul> <li> <p>We want to know what\u2019s going on inside ConvNets?</p> </li> <li> <p>People want to trust the black box (CNN) and know how it exactly works and give and good decisions.</p> </li> <li> <p>A first approach is to visualize filters of the first layer.</p> </li> <li> <p>Maybe the shape of the first layer filter is 5 x 5 x 3, and the number of filters are 16. Then we will have 16 different \"colored\" filter images.</p> </li> <li>It turns out that these filters learns primitive shapes and oriented edges like the human brain does.</li> <li>These filters really looks the same on each Conv net you will train, Ex if you tried to get it out of AlexNet, VGG, GoogleNet, or ResNet.</li> <li> <p>This will tell you what is the first convolution layer is looking for in the image.</p> </li> <li> <p>We can visualize filters from the next layers but they won't tell us anything.</p> </li> <li> <p>Maybe the shape of the first layer filter is 5 x 5 x 20, and the number of filters are 16. Then we will have 16*20 different \"gray\" filter images.</p> </li> <li> <p>In AlexNet, there was some FC layers in the end. If we took the 4096-dimensional feature vector for an image, and collecting these feature vectors.</p> </li> <li> <p>If we made a nearest neighbors between these feature vectors and get the real images of these features we will get something very good compared with running the KNN on the images directly!</p> </li> <li></li> <li>This similarity tells us that these CNNs are really getting the semantic meaning of these images instead of on the pixels level!</li> <li> <p>We can make a dimensionality reduction on the 4096 dimensional feature and compress it to 2 dimensions.</p> <ul> <li>This can be made by PCA, or t-SNE.</li> <li>t-SNE are used more with deep learning to visualize the data. Example can be found here.</li> </ul> </li> <li> <p>We can Visualize the activation maps.</p> </li> <li> <p>For example if CONV5 feature map is 128 x 13 x 13, We can visualize it as 128 13 x 13 gray-scale images.</p> </li> <li></li> <li>One of these features are activated corresponding to the input, so now we know that this particular map are looking for something.</li> <li> <p>Its done by Yosinski et. More info are here.</p> </li> <li> <p>There are something called Maximally Activating Patches that can help us visualize the intermediate features in Convnets</p> </li> <li> <p>The steps of doing this is as following:</p> <ul> <li>We choose a layer then a neuron</li> <li>Ex. We choose Conv5 in AlexNet which is 128 x 13 x 13 then pick channel (Neuron) 17/128</li> <li>Run many images through the network, record values of chosen channel.</li> <li>Visualize image patches that correspond to maximal activations.</li> <li>We will find that each neuron is looking into a specific part of the image.</li> <li>Extracted images are extracted using receptive field.</li> </ul> </li> <li> <p>Another idea is Occlusion Experiments</p> </li> <li> <p>We mask part of the image before feeding to CNN, draw heat-map of probability (Output is true) at each mask location</p> </li> <li>It will give you the most important parts of the image in which the Conv. Network has learned from.</li> <li> <p></p> </li> <li> <p>Saliency Maps tells which pixels matter for classification</p> </li> <li> <p>Like Occlusion Experiments but with a completely different approach</p> </li> <li>We Compute gradient of (unnormalized) class score with respect to image pixels, take absolute value and max over RGB channels. It will get us a gray image that represents the most important areas in the image.</li> <li> <p>This can be used for Semantic Segmentation sometimes.</p> </li> <li> <p>(guided) backprop Makes something like Maximally Activating Patches but unlike it gets the pixels in which we are caring of.</p> </li> <li> <p>In this technique choose a channel like Maximally Activating Patches and then compute gradient of neuron value with respect to image pixels</p> </li> <li> <p>Images come out nicer if you only backprop positive gradients through each RELU (guided backprop)</p> </li> <li> <p>Gradient Ascent</p> </li> <li> <p>Generate a synthetic image that maximally activates a neuron.</p> </li> <li> <p>Reverse of gradient decent. Instead of taking the minimum it takes the maximum.</p> </li> <li> <p>We want to maximize the neuron with the input image. So here instead we are trying to learn the image that maximize the activation:</p> <ul> <li><code>python   # R(I) is Natural image regularizer, f(I) is the neuron value.   I *= argmax(f(I)) + R(I)</code></li> </ul> </li> <li> <p>Steps of gradient ascent</p> <ul> <li>Initialize image to zeros.</li> <li>Forward image to compute current scores.</li> <li>Backprop to get gradient of neuron value with respect to image pixels.</li> <li>Make a small update to the image</li> </ul> </li> <li> <p><code>R(I)</code> may equal to L2 of generated image.</p> </li> <li> <p>To get a better results we use a better regularizer:</p> <ul> <li>penalize L2 norm of image; also during optimization periodically:</li> <li>Gaussian blur image</li> <li>Clip pixels with small values to 0</li> <li>Clip pixels with small gradients to 0</li> </ul> </li> <li> <p>A better regularizer makes out images cleaner!</p> </li> <li> <p></p> </li> <li> <p>The results in the latter layers seems to mean something more than the other layers.</p> </li> <li> <p>We can fool CNN by using this procedure:</p> </li> <li> <p>Start from an arbitrary image.          <code># Random picture based on nothing.</code></p> </li> <li>Pick an arbitrary class. <code># Random class</code></li> <li>Modify the image to maximize the class.</li> <li> <p>Repeat until network is fooled.</p> </li> <li> <p>Results on fooling the network is pretty surprising!</p> </li> <li> <p></p> </li> <li> <p>For human eyes they are the same, but it fooled the network by adding just some noise!</p> </li> <li> <p>DeepDream: Amplify existing features</p> </li> <li> <p>Google released deep dream on their website.</p> </li> <li>What its actually doing is the same procedure as fooling the NN that we discussed, but rather than synthesizing an image to maximize a specific neuron, instead try to amplify the neuron activations at some layer in the network.</li> <li>Steps:<ul> <li>Forward: compute activations at chosen layer.     <code># form an input image (Any image)</code></li> <li>Set gradient of chosen layer equal to its activation.</li> <li>Equivalent to <code>I* = arg max[I] sum(f(I)^2)</code></li> <li>Backward: Compute gradient on image.</li> <li>Update image.</li> </ul> </li> <li> <p>The code of deep dream is online you can download and check it yourself.</p> </li> <li> <p>Feature Inversion</p> </li> <li> <p>Gives us to know what types of elements parts of the image are captured at different layers in the network.</p> </li> <li> <p>Given a CNN feature vector for an image, find a new image that: </p> <ul> <li>Matches the given feature vector.</li> <li>looks natural (image prior regularization) </li> </ul> </li> <li> <p>Texture Synthesis</p> </li> <li> <p>Old problem in computer graphics.</p> </li> <li>Given a sample patch of some texture, can we generate a bigger image of the same texture?</li> <li>There is an algorithm which doesn't depend on NN:<ul> <li>Wei and Levoy, Fast Texture Synthesis using Tree-structured Vector Quantization, SIGGRAPH 2000</li> <li>Its a really simple algorithm</li> </ul> </li> <li>The idea here is that this is an old problem and there are a lot of algorithms that has already solved it but simple algorithms doesn't work well on complex textures!</li> <li> <p>An idea of using NN has been proposed on 2015 based on gradient ascent and called it \"Neural Texture Synthesis\"</p> <ul> <li>It depends on something called Gram matrix.</li> </ul> </li> <li> <p>Neural Style Transfer =  Feature + Gram Reconstruction</p> </li> <li> <p>Gatys, Ecker, and Bethge, Image style transfer using Convolutional neural networks, CVPR 2016</p> </li> <li> <p>Implementation by pytorch here.</p> </li> <li> <p>Style transfer requires many forward / backward passes through VGG; very slow!</p> </li> <li> <p>Train another neural network to perform style transfer for us!</p> </li> <li>Fast Style Transfer is the solution.</li> <li>Johnson, Alahi, and Fei-Fei, Perceptual Losses for Real-Time Style Transfer and Super-Resolution, ECCV 2016</li> <li> <p>https://github.com/jcjohnson/fast-neural-style</p> </li> <li> <p>There are a lot of work on these style transfer and it continues till now!</p> </li> <li> <p>Summary:</p> </li> <li> <p>Activations: Nearest neighbors, Dimensionality reduction, maximal patches, occlusion</p> </li> <li>Gradients: Saliency maps, class visualization, fooling images, feature inversion</li> <li>Fun: DeepDream, Style Transfer</li> </ul>"},{"location":"CS/ML/CS231n/#13-generative-models","title":"13. Generative models","text":"<ul> <li> <p>Generative models are type of Unsupervised learning.</p> </li> <li> <p>Supervised vs Unsupervised Learning:</p> </li> <li> Supervised Learning Unsupervised Learning Data structure Data: (x, y), and x is data, y is label Data: x, Just data, no labels! Data price Training data is expensive in a lot of cases. Training data are cheap! Goal Learn a function to map x -&gt; y Learn some underlying hidden structure of the data Examples Classification, regression, object detection, semantic segmentation, image captioning Clustering, dimensionality reduction, feature learning, density estimation </li> <li> <p>Autoencoders are a Feature learning technique.</p> </li> <li> <p></p> </li> <li>It contains an encoder and a decoder. The encoder downsamples the image while the decoder upsamples the features.</li> <li> <p>The loss are L2 loss.</p> </li> <li> <p>Density estimation is where we want to learn/estimate the underlaying distribution for the data!</p> </li> <li> <p>There are a lot of research open problems in unsupervised learning compared with supervised learning!</p> </li> <li> <p>Generative Models</p> </li> <li> <p>Given training data, generate new samples from same distribution.</p> </li> <li>Addresses density estimation, a core problem in unsupervised learning.</li> <li>We have different ways to do this:<ul> <li>Explicit density estimation: explicitly define and solve for the learning model.</li> <li>Learn model that can sample from the learning model without explicitly defining it.</li> </ul> </li> <li>Why Generative Models?<ul> <li>Realistic samples for artwork, super-resolution, colorization, etc</li> <li>Generative models of time-series data can be used for simulation and planning (reinforcement learning applications!)</li> <li>Training generative models can also enable inference of latent representations that can be useful as general features</li> </ul> </li> <li>Taxonomy of Generative Models:<ul> <li></li> </ul> </li> <li> <p>In this lecture we will discuss: PixelRNN/CNN, Variational Autoencoder, and GANs as they are the popular models in research now.</p> </li> <li> <p>PixelRNN and PixelCNN</p> </li> <li> <p>In a full visible belief network we use the chain rule to decompose likelihood of an image x into product of 1-d distributions</p> <ul> <li><code>p(x) = sum(p(x[i]| x[1]x[2]....x[i-1]))</code></li> <li>Where p(x) is the Likelihood of image x and x[i] is Probability of i\u2019th pixel value given all previous pixels.</li> </ul> </li> <li>To solve the problem we need to maximize the likelihood of training data but the distribution is so complex over pixel values.</li> <li>Also we will need to define ordering of previous pixels.</li> <li>PixelRNN<ul> <li>Founded by [van der Oord et al. 2016]</li> <li>Dependency on previous pixels modeled using an RNN (LSTM)</li> <li>Generate image pixels starting from corner</li> <li>Drawback: sequential generation is slow! because you have to generate pixel by pixel!</li> </ul> </li> <li>PixelCNN<ul> <li>Also Founded by [van der Oord et al. 2016]</li> <li>Still generate image pixels starting from corner.</li> <li>Dependency on previous pixels now modeled using a CNN over context region</li> <li>Training is faster than PixelRNN (can parallelize convolutions since context region values known from training images)</li> <li>Generation must still proceed sequentially still slow.</li> </ul> </li> <li>There are some tricks to improve PixelRNN &amp; PixelCNN.</li> <li> <p>PixelRNN and PixelCNN can generate good samples and are still active area of research.</p> </li> <li> <p>Autoencoders</p> </li> <li> <p>Unsupervised approach for learning a lower-dimensional feature representation from unlabeled training data.</p> </li> <li>Consists of Encoder and decoder.</li> <li>The encoder:<ul> <li>Converts the input x to the features z. z should be smaller than x to get only the important values out of the input. We can call this dimensionality reduction.</li> <li>The encoder can be made with:</li> <li>Linear or non linear layers (earlier days days)</li> <li>Deep fully connected NN (Then)</li> <li>RELU CNN (Currently we use this on images)</li> </ul> </li> <li>The decoder:<ul> <li>We want the encoder to map the features we have produced to output something similar to x or the same x.</li> <li>The decoder can be made with the same techniques we made the encoder and currently it uses a RELU CNN.</li> </ul> </li> <li>The encoder is a conv layer while the decoder is deconv layer! Means Decreasing and then increasing.</li> <li>The loss function is L2 loss function:<ul> <li><code>L[i] = |y[i] - y'[i]|^2</code></li> <li>After training we though away the decoder.<code># Now we have the features we need</code></li> </ul> </li> <li>We can use this encoder we have to make a supervised model.<ul> <li>The value of this it can learn a good feature representation to the input you have.</li> <li>A lot of times we will have a small amount of data to solve problem. One way to tackle this is to use an Autoencoder that learns how to get features from images and train your small dataset on top of that model.</li> </ul> </li> <li> <p>The question is can we generate data (Images) from this Autoencoder?</p> </li> <li> <p>Variational Autoencoders (VAE)</p> </li> <li> <p>Probabilistic spin on Autoencoders - will let us sample from the model to generate data!</p> </li> <li>We have z as the features vector that has been formed using the encoder.</li> <li>We then choose prior p(z) to be simple, e.g. Gaussian. <ul> <li>Reasonable for hidden attributes: e.g. pose, how much smile.</li> </ul> </li> <li>Conditional p(x|z) is complex (generates image) =&gt; represent with neural network</li> <li>But we cant compute integral for P(z)p(x|z)dz as the following equation:<ul> <li></li> </ul> </li> <li>After resolving all the equations that solves the last equation we should get this:<ul> <li></li> </ul> </li> <li>Variational Autoencoder are an approach to generative models but Samples blurrier and lower quality compared to state-of-the-art (GANs)</li> <li> <p>Active areas of research:</p> <ul> <li>More flexible approximations, e.g. richer approximate posterior instead of diagonal Gaussian</li> <li>Incorporating structure in latent variables</li> </ul> </li> <li> <p>Generative Adversarial Networks (GANs)</p> </li> <li> <p>GANs don\u2019t work with any explicit density function!</p> </li> <li> <p>Instead, take game-theoretic approach: learn to generate from training distribution through 2-player game.</p> </li> <li> <p>Yann LeCun, who oversees AI research at Facebook, has called GANs:</p> <ul> <li> <p>The coolest idea in deep learning in the last 20 years</p> </li> </ul> </li> <li> <p>Problem: Want to sample from complex, high-dimensional training distribution. No direct way to do this as we have discussed!</p> </li> <li> <p>Solution: Sample from a simple distribution, e.g. random noise. Learn transformation to training distribution.</p> </li> <li> <p>So we create a noise image which are drawn from simple distribution feed it to NN we will call it a generator network that should learn to transform this into the distribution we want.</p> </li> <li> <p>Training GANs: Two-player game:</p> <ul> <li>Generator network: try to fool the discriminator by generating real-looking images.</li> <li>Discriminator network: try to distinguish between real and fake images.</li> </ul> </li> <li> <p>If we are able to train the Discriminator well then we can train the generator to generate the right images.</p> </li> <li> <p>The loss function of GANs as minimax game are here:</p> <ul> <li></li> </ul> </li> <li> <p>The label of the generator network will be 0 and the real images are 1.</p> </li> <li> <p>To train the network we will do:</p> <ul> <li>Gradient ascent on discriminator.</li> <li>Gradient ascent on generator but with different loss.</li> </ul> </li> <li> <p>You can read the full algorithm with the equations here:</p> <ul> <li></li> </ul> </li> <li> <p>Aside: Jointly training two networks is challenging, can be unstable. Choosing objectives with better loss landscapes helps training is an active area of research.</p> </li> <li> <p>Convolutional Architectures:</p> <ul> <li>Generator is an upsampling network with fractionally-strided convolutions Discriminator is a Convolutional network.</li> <li>Guidelines for stable deep Conv GANs:</li> <li>Replace any pooling layers with strided convs (discriminator) and fractional-strided convs with (Generator).</li> <li>Use batch norm for both networks.</li> <li>Remove fully connected hidden layers for deeper architectures.</li> <li>Use RELU activation in generator for all layers except the output which uses Tanh</li> <li>Use leaky RELU in discriminator for all the layers.</li> </ul> </li> <li> <p>2017 is the year of the GANs! it has exploded and there are some really good results.</p> </li> <li> <p>Active areas of research also is GANs for all kinds of applications.</p> </li> <li> <p>The GAN zoo can be found here: https://github.com/hindupuravinash/the-gan-zoo</p> </li> <li> <p>Tips and tricks for using GANs: https://github.com/soumith/ganhacks</p> </li> <li> <p>NIPS 2016 Tutorial GANs: https://www.youtube.com/watch?v=AJVyzd0rqdc</p> </li> </ul>"},{"location":"CS/ML/CS231n/#14-deep-reinforcement-learning","title":"14. Deep reinforcement learning","text":"<ul> <li>This section contains a lot of math.</li> <li>Reinforcement learning problems are involving an agent interacting with an environment, which provides numeric reward signals.</li> <li>Steps are:</li> <li>Environment --&gt; State <code>s[t]</code> --&gt; Agent --&gt; Action <code>a[t]</code> --&gt; Environment --&gt; <code>Reward r[t]</code> + Next state <code>s[t+1]</code> --&gt; Agent --&gt; and so on..</li> <li>Our goal is learn how to take actions in order to maximize reward.</li> <li>An example is Robot Locomotion:</li> <li>Objective: Make the robot move forward</li> <li>State: Angle and position of the joints</li> <li>Action: Torques applied on joints</li> <li>1 at each time step upright + forward movement</li> <li>Another example is Atari Games:</li> <li>Deep learning has a good state of art in this problem.</li> <li>Objective: Complete the game with the highest score.</li> <li>State: Raw pixel inputs of the game state.</li> <li>Action: Game controls e.g. Left, Right, Up, Down</li> <li>Reward: Score increase/decrease at each time step</li> <li>Go game is another example which AlphaGo team won in the last year (2016) was a big achievement for AI and deep learning because the problem was so hard.</li> <li>We can mathematically formulate the RL (reinforcement learning) by using Markov Decision Process</li> <li>Markov Decision Process</li> <li>Defined by (<code>S</code>, <code>A</code>, <code>R</code>, <code>P</code>, <code>Y</code>) where:<ul> <li><code>S</code>: set of possible states.</li> <li><code>A</code>: set of possible actions</li> <li><code>R</code>: distribution of reward given (state, action) pair</li> <li><code>P</code>: transition probability i.e. distribution over next state given (state, action) pair</li> <li><code>Y</code>: discount factor    <code># How much we value rewards coming up soon verses later on.</code></li> </ul> </li> <li>Algorithm:<ul> <li>At time step <code>t=0</code>, environment samples initial state <code>s[0]</code></li> <li>Then, for t=0 until done:</li> <li>Agent selects action <code>a[t]</code></li> <li>Environment samples reward from <code>R</code> with (<code>s[t]</code>, <code>a[t]</code>)</li> <li>Environment samples next state from <code>P</code> with (<code>s[t]</code>, <code>a[t]</code>)</li> <li>Agent receives reward <code>r[t]</code> and next state <code>s[t+1]</code></li> </ul> </li> <li>A policy <code>pi</code>  is a function from S to A that specifies what action to take in each state.</li> <li>Objective: find policy <code>pi*</code> that maximizes cumulative discounted reward: <code>Sum(Y^t * r[t], t&gt;0)</code></li> <li>For example:<ul> <li></li> </ul> </li> <li>Solution would be:<ul> <li></li> </ul> </li> <li>The value function at state <code>s</code>, is the expected cumulative reward from following the policy from state <code>s</code>:</li> <li><code>V[pi](s) = Sum(Y^t * r[t], t&gt;0) given s0 = s, pi</code></li> <li>The Q-value function at state s and action <code>a</code>, is the expected cumulative reward from taking action <code>a</code> in state <code>s</code> and then following the policy:</li> <li><code>Q[pi](s,a) = Sum(Y^t * r[t], t&gt;0) given s0 = s,a0 = a, pi</code></li> <li>The optimal Q-value function <code>Q*</code> is the maximum expected cumulative reward achievable from a given (state, action) pair:</li> <li><code>Q*[s,a] = Max(for all of pi on (Sum(Y^t * r[t], t&gt;0) given s0 = s,a0 = a, pi))</code></li> <li>Bellman equation</li> <li>Important thing is RL.</li> <li>Given any state action pair (s,a) the value of this pair is going to be the reward that you are going to get r plus the value of the state that you end in.</li> <li><code>Q*[s,a] = r + Y * max Q*(s',a') given s,a  # Hint there is no policy in the equation</code></li> <li>The optimal policy <code>pi*</code> corresponds to taking the best action in any state as specified by <code>Q*</code></li> <li>We can get the optimal policy using the value iteration algorithm that uses the Bellman equation as an iterative update</li> <li></li> <li>Due to the huge space dimensions in real world applications we will use a function approximator to estimate <code>Q(s,a)</code>. E.g. a neural network! this is called Q-learning</li> <li>Any time we have a complex function that we cannot represent we use Neural networks!</li> <li>Q-learning</li> <li>The first deep learning algorithm that solves the RL.</li> <li>Use a function approximator to estimate the action-value function</li> <li>If the function approximator is a deep neural network =&gt; deep q-learning</li> <li>The loss function:<ul> <li></li> </ul> </li> <li>Now lets consider the \"Playing Atari Games\" problem:</li> <li>Our total reward are usually the reward we are seeing in the top of the screen.</li> <li>Q-network Architecture:<ul> <li></li> </ul> </li> <li>Learning from batches of consecutive samples is a problem. If we recorded a training data and set the NN to work with it, if the data aren't enough we will go to a high bias error. so we should use \"experience replay\" instead of consecutive samples where the NN will try the game again and again until it masters it.</li> <li>Continually update a replay memory table of transitions (<code>s[t]</code> , <code>a[t]</code> , <code>r[t]</code> , <code>s[t+1]</code>) as game (experience) episodes are played.</li> <li>Train Q-network on random minibatches of transitions from the replay memory, instead of consecutive samples.</li> <li>The full algorithm:<ul> <li></li> </ul> </li> <li>A video that demonstrate the algorithm on Atari game can be found here: \"https://www.youtube.com/watch?v=V1eYniJ0Rnk\"</li> <li>Policy Gradients</li> <li>The second deep learning algorithm that solves the RL.</li> <li>The problem with Q-function is that the Q-function can be very complicated.<ul> <li>Example: a robot grasping an object has a very high-dimensional state.</li> <li>But the policy can be much simpler: just close your hand.</li> </ul> </li> <li>Can we learn a policy directly, e.g. finding the best policy from a collection of policies?</li> <li>Policy Gradients equations:<ul> <li></li> </ul> </li> <li>Converges to a local minima of <code>J(ceta)</code>, often good enough!</li> <li>REINFORCE algorithm is the algorithm that will get/predict us the best policy</li> <li>Equation and intuition of the Reinforce algorithm:<ul> <li></li> <li>the problem was high variance with this equation can we solve this?</li> <li>variance reduction is an active research area!</li> </ul> </li> <li>Recurrent Attention Model (RAM) is an algorithm that are based on REINFORCE algorithm and is used for image classification problems:<ul> <li>Take a sequence of \u201cglimpses\u201d selectively focusing on regions of the image, to predict class</li> <li>Inspiration from human perception and eye movements.</li> <li>Saves computational resources =&gt; scalability<ul> <li>If an image with high resolution you can save a lot of computations</li> </ul> </li> <li>Able to ignore clutter / irrelevant parts of image</li> <li>RAM is used now in a lot of tasks: including fine-grained image recognition, image captioning, and visual question-answering</li> </ul> </li> <li>AlphaGo are using a mix of supervised learning and reinforcement learning, It also using policy gradients.</li> <li>A good course from Standford on deep reinforcement learning</li> <li>http://web.stanford.edu/class/cs234/index.html</li> <li>https://www.youtube.com/playlist?list=PLkFD6_40KJIwTmSbCv9OVJB3YaO4sFwkX</li> <li>A good course on deep reinforcement learning (2017)</li> <li>http://rll.berkeley.edu/deeprlcourse/</li> <li>https://www.youtube.com/playlist?list=PLkFD6_40KJIznC9CDbVTjAF2oyt8_VAe3</li> <li>A good article</li> <li>https://www.kdnuggets.com/2017/09/5-ways-get-started-reinforcement-learning.html</li> </ul>"},{"location":"CS/ML/CS231n/#15-efficient-methods-and-hardware-for-deep-learning","title":"15. Efficient Methods and Hardware for Deep Learning","text":"<ul> <li>The original lecture was given by Song Han a PhD Candidate at standford.</li> <li>Deep Conv nets, Recurrent nets, and deep reinforcement learning are shaping a lot of applications and changing a lot of our lives.</li> <li>Like self driving cars, machine translations, alphaGo and so on.</li> <li>But the trend now says that if we want a high accuracy we need a larger (Deeper) models.</li> <li>The model size in ImageNet competation from 2012 to 2015 has increased 16x to achieve a high accurecy.</li> <li>Deep speech 2 has 10x training operations than deep speech 1 and thats in only one year! <code># At Baidu</code></li> <li>There are three challenges we got from this</li> <li>Model Size<ul> <li>Its hard to deploy larger models on our PCs, mobiles, or cars.</li> </ul> </li> <li>Speed<ul> <li>ResNet152 took 1.5 weeks to train and give the 6.16% accurecy!</li> <li>Long training time limits ML researcher\u2019s productivity</li> </ul> </li> <li>Energy Efficiency<ul> <li>AlphaGo: 1920 CPUs and 280 GPUs. $3000 electric bill per game</li> <li>If we use this on our mobile it will drain the battery.</li> <li>Google mentioned in thier blog if all the users used google speech for 3 minutes, they have to double thier data-center!</li> <li>Where is the Energy Consumed?</li> <li>larger model =&gt; more memory reference =&gt; more energy</li> </ul> </li> <li>We can improve the Efficiency of Deep Learning by Algorithm-Hardware Co-Design.</li> <li>From both the hardware and the algorithm perspectives.</li> <li>Hardware 101: the Family</li> <li>General Purpose <code># Used for any hardware</code><ul> <li>CPU               <code># Latency oriented, Single strong threaded like a single elepahnt</code></li> <li>GPU         <code># Throughput oriented, So many small threads like a lot of ants</code></li> <li>GPGPU</li> <li>Specialized HW <code>#Tuned for a domain of applications</code><ul> <li>FPGA# Programmable logic, Its cheaper but less effiecnet`</li> <li>ASIC<code># Fixed logic, Designed for a certian applications (Can be designed for deep learning applications)</code></li> </ul> </li> </ul> </li> <li>Hardware 101: Number Representation</li> <li>Numbers in computer are represented with a discrete memory.</li> <li>Its very good and energy efficent for hardware to go from 32 bit to 16 bit in float point operations.</li> <li>Part 1: Algorithms for Efficient Inference</li> <li>Pruning neural networks<ul> <li>Idea is can we remove some of the weights/neurons and the NN still behave the same?</li> <li>In 2015 Han made AlexNet parameters from 60 million to 6 Million! by using the idea of Pruning.</li> <li>Pruning can be applied to CNN and RNN, iteratively it will reach the same accurecy as the original.</li> <li>Pruning actually happends to humans:</li> <li>Newborn(50 Trillion Synapses) ==&gt; 1 year old(1000 Trillion Synapses) ==&gt; Adolescent(500 Trillion Synapses)</li> <li>Algorithm:</li> <li>Get Trained network.</li> <li>Evaluate importance of neurons.</li> <li>Remove the least important neuron.</li> <li>Fine tune the network.</li> <li>If we need to continue Pruning we go to step 2 again else we stop.</li> </ul> </li> <li>Weight Sharing<ul> <li>The idea is that we want to make the numbers is our models less.</li> <li>Trained Quantization:</li> <li>Example: all weight values that are 2.09, 2.12, 1.92, 1.87 will be replaced by 2</li> <li>To do that we can make k means clustering on a filter for example and reduce the numbers in it. By using this we can also reduce the number of operations that are used from calculating the gradients.</li> <li>After Trained Quantization the Weights are Discrete.</li> <li>Trained Quantization can reduce the number of bits we need for a number in each layer significantly.</li> <li>Pruning + Trained Quantization can Work Together to reduce the size of the model.</li> <li>Huffman Coding</li> <li>We can use Huffman Coding to reduce/compress the number of bits of the weight.</li> <li>In-frequent weights: use more bits to represent.</li> <li>Frequent weights: use less bits to represent.</li> <li>Using Pruning + Trained Quantization + Huffman Coding is called deep compression.</li> <li></li> <li></li> <li>SqueezeNet<ul> <li>All the models we have talked about till now was using a pretrained models. Can we make a new arcitecutre that saves memory and computations?</li> <li>SqueezeNet gets the alexnet accurecy with 50x fewer parameters and 0.5 model size.</li> </ul> </li> <li>SqueezeNet can even be further compressed by applying deep compression on them.</li> <li>Models are now more energy efficient and has speed up a lot.</li> <li>Deep compression was applied in Industry through facebook and Baidu.</li> </ul> </li> <li>Quantization<ul> <li>Algorithm (Quantizing the Weight and Activation):</li> <li>Train with float.</li> <li>Quantizing the weight and activation:<ul> <li>Gather the statistics for weight and activation.</li> <li>Choose proper radix point position.</li> </ul> </li> <li>Fine-tune in float format.</li> <li>Convert to fixed-point format.</li> </ul> </li> <li>Low Rank Approximation<ul> <li>Is another size reduction algorithm that are used for CNN.</li> <li>Idea is decompose the conv layer and then try both of the composed layers.</li> </ul> </li> <li>Binary / Ternary Net<ul> <li>Can we only use three numbers to represent weights in NN?</li> <li>The size will be much less with only -1, 0, 1.</li> <li>This is a new idea that was published in 2017 \"Zhu, Han, Mao, Dally. Trained Ternary Quantization, ICLR\u201917\"</li> <li>Works after training.</li> <li>They have tried it on AlexNet and it has reached almost the same error as AlexNet.</li> <li>Number of operation will increase per register: https://xnor.ai/</li> </ul> </li> <li>Winograd Transformation<ul> <li>Based on 3x3 WINOGRAD Convolutions which makes less operations than the ordiany convolution</li> <li>cuDNN 5 uses the WINOGRAD Convolutions which has improved the speed.</li> </ul> </li> <li>Part 2: Hardware for Efficient Inference</li> <li>There are a lot of ASICs that we developed for deep learning. All in which has the same goal of minimize memory access.<ul> <li>Eyeriss MIT</li> <li>DaDiannao</li> <li>TPU Google (Tensor processing unit)</li> <li>It can be put to replace the disk in the server.</li> <li>Up to 4 cards per server.</li> <li>Power consumed by this hardware is a lot less than a GPU and the size of the chip is less.</li> <li>EIE Standford</li> <li>By Han at 2016 [et al. ISCA\u201916]</li> <li>We don't save zero weights and make quantization for the numbers from the hardware.</li> <li>He says that EIE has a better Throughput and energy efficient.</li> </ul> </li> <li>Part 3: Algorithms for Efficient Training</li> <li>Parallelization<ul> <li>Data Parallel \u2013 Run multiple inputs in parallel</li> <li>Ex. Run two images in the same time!</li> <li>Run multiple training examples in parallel.</li> <li>Limited by batch size.</li> <li>Gradients have to be applied by a master node.</li> <li>Model Parallel</li> <li>Split up the Model \u2013 i.e. the network</li> <li>Split model over multiple processors By layer.</li> <li>Hyper-Parameter Parallel</li> <li>Try many alternative networks in parallel.</li> <li>Easy to get 16-64 GPUs training one model in parallel.</li> </ul> </li> <li>Mixed Precision with FP16 and FP32<ul> <li>We have discussed that if we use 16 bit real numbers all over the model the energy cost will be less by x4.</li> <li>Can we use a model entirely with 16 bit number? We can partially do this with mixed FP16 and FP32. We use 16 bit everywhere but at some points we need the FP32.</li> <li>By example in multiplying FP16 by FP16 we will need FP32.</li> <li>After you train the model you can be a near accuracy of the famous models like AlexNet and ResNet.</li> </ul> </li> <li>Model Distillation<ul> <li>The question is can we use a senior (Good) trained neural network(s) and make them guide a student (New) neural network?</li> <li>For more information look at Hinton et al. Dark knowledge / Distilling the Knowledge in a Neural Network</li> </ul> </li> <li>DSD: Dense-Sparse-Dense Training<ul> <li>Han et al. \u201cDSD: Dense-Sparse-Dense Training for Deep Neural Networks\u201d, ICLR 2017</li> <li>Has a better regularization.</li> <li>The idea is Train the model lets call this the Dense, we then apply Pruning to it lets call this sparse.</li> <li>DSD produces same model architecture but can find better optimization solution arrives at better local minima, and achieves higher prediction accuracy.</li> <li>After the above two steps we go connect the remain connection and learn them again (To dense again).</li> <li>This improves the performace a lot in many deep learning models.</li> </ul> </li> <li>Part 4: Hardware for Efficient Training</li> <li>GPUs for training:<ul> <li>Nvidia PASCAL GP100 (2016)</li> <li>Nvidia Volta GV100 (2017)</li> <li>Can make mixed precision operations!</li> <li>So powerful.</li> <li>The new neclar bomb!</li> </ul> </li> <li>Google Announced \"Google Cloud TPU\" on May 2017!<ul> <li>Cloud TPU delivers up to 180 teraflops to train and run machine learning models.</li> <li>One of our new large-scale translation models used to take a full day to train on 32 of the best commercially-available GPUs\u2014now it trains to the same accuracy in an afternoon using just one eighth of a TPU pod.</li> </ul> </li> <li>We have moved from PC Era ==&gt; Mobile-First Era ==&gt; AI-First Era</li> </ul>"},{"location":"CS/ML/CS231n/#16-adversarial-examples-and-adversarial-training","title":"16. Adversarial Examples and Adversarial Training","text":"<ul> <li>What are adversarial examples?</li> <li>Since 2013, deep neural networks have matched human performance at..<ul> <li>Face recognition</li> <li>Object recognition</li> <li>Captcha recognition</li> <li>Because its accuracy was higher than humans, Websites tried to find another solution than Captcha.</li> <li>And other tasks..</li> </ul> </li> <li>Before 2013 no body was surprised if they saw a computer made a mistake! But now the deep learning exists and its so important to know the problems and the causes.</li> <li>Adversarial are problems and unusual mistake that deep learning make.</li> <li>This topic wasn't hot until deep learning can now do better and better than human!</li> <li>An adversarial is an example that has been carefully computed to to be misclassified.</li> <li>In a lot of cases the adversarial image isn't changed much compared to the original image from the human perspective.</li> <li>History of recent papers:<ul> <li>Biggio 2013: fool neural nets.</li> <li>Szegedy et al 2013: fool ImageNet classifiers imperceptibly</li> <li>Goodfellow et al 2014: cheap, closed form attack.</li> </ul> </li> <li>So the first story was in 2013. When Szegedy had a CNN that can classify images very well.<ul> <li>He wanted to understand more about how CNN works to improve it.</li> <li>He give an image of an object and by using gradient ascent he tried to update the images so that it can be another object.</li> <li>Strangely he found that the result image hasn't changed much from the human perspective!</li> <li>If you tried it you won't notify any change and you will think that this is a bug! but it isn't if you go for the image you will notice that they are completely different!</li> </ul> </li> <li>These mistakes can be found in almost any deep learning algorithm we have studied!<ul> <li>It turns out that RBF (Radial Basis Network) can resist this.</li> <li>Deep Models for Density Estimation can resist this.</li> </ul> </li> <li>Not just for neural nets can be fooled:<ul> <li>Linear models</li> <li>Logistic regression</li> <li>Softmax regression</li> <li>SVMs</li> <li>Decision trees </li> <li>Nearest neighbors</li> </ul> </li> <li>Why do adversarial happen?</li> <li>In the process in trying to understand what is happening, in 2016 they thought it was from overfitting models in the high dimensional data case.<ul> <li>Because in such high dimensions we could have some random errors which can be found.</li> <li>So if we trained a model with another parameters it should not make the same mistake?</li> <li>They found that not right. Models are reaching to the same mistakes so it doesn't mean its overfitting.</li> </ul> </li> <li>In the previous mentioned experiment the found that the problem is caused by systematic thing not a random.<ul> <li>If they add some vector to an example it would misclassified to any model.</li> </ul> </li> <li>Maybe they are coming from underfitting not overfitting.</li> <li>Modern deep nets are very piecewise linear<ul> <li>Rectified linear unit</li> <li>Carefully tuned sigmoid  <code># Most of the time we are inside the linear curve</code></li> <li>Maxout</li> <li>LSTM</li> </ul> </li> <li>Relation between the parameter and the output are non linear because it's multiplied together thats what make training NN difficult, while mapping from linear from input and output are linear and much easier.</li> <li>How can adversarial be used to compromise machine learning systems?</li> <li>If we are experimenting how easy a NN to fool, We want to make sure we are actually fooling it not just changing the output class, and if we are attackers we want to make this behavior to the NN (Get hole).</li> <li>When we build Adversarial example we use the max norm constrain to perturbation.</li> <li>The fast gradient sign method:<ul> <li>This method comes from the fact that almost all NN are using a linear activations (Like RELU) the assumption we have told before.</li> <li>No pixel can be changed more than some amount epsilon.</li> <li>Fast way is to take the gradient of the cost you used to train the network with respect to the input and then take the sign of that gradient multiply this by epsilon.</li> <li>Equation:</li> <li><code>Xdash = x + epslion * (sign of the gradient)</code></li> <li>Where Xdash is the adversarial example and x is the normal example</li> <li>So it can be detected by just using the sign (direction) and some epsilon.</li> </ul> </li> <li>Some attacks are based on ADAM optimizer.</li> <li>Adversarial examples are not random noises!</li> <li>NN are trained on some distribution and behaves well in that distribution. But if you shift this distribution the NN won't answer the right answers. They will be so easy to fool.</li> <li>deep RL can also be fooled.</li> <li>Attack of the weights:<ul> <li>In linear models, We can take the learned weights image, take the signs of the image and add it to any example to force the class of the weights to be true. Andrej Karpathy, \"Breaking Linear Classifiers on ImageNet\"</li> </ul> </li> <li>It turns out that some of the linaer models performs well (We cant get advertisal from them easily)<ul> <li>In particular Shallow RBFs network resist adversarial perturbation # By The fast gradient sign method</li> <li>The problem is RBFs doesn't get so much accuracy on the datasets because its just a shallow model and if you tried to get this model deeper the gradients will become zero in almost all the layers.</li> <li>RBFs are so difficult to train even with batch norm. algorithm.</li> <li>Ian thinks if we have a better hyper parameters or a better optimization algorithm that gradient decent we will be able to train RBFs and solve the adversarial problem!</li> </ul> </li> <li>We also can use another model to fool current model. Ex use an SVM to fool a deep NN.<ul> <li>For more details follow the paper: \"Papernot 2016\"</li> </ul> </li> <li>Transferability Attack<ol> <li>Target model with unknown weights, machine learning algorithm, training set; maybe non differentiable</li> <li>Make your training set from this model using inputs from you, send them to the model and then get outputs from the model</li> <li>Train you own model. \"Following some table from Papernot 2016\"</li> <li>Create an Adversarial example on your model.</li> <li>Use these examples against the model you are targeting.</li> <li>You are almost likely to get good results and fool this target!</li> </ol> </li> <li>In Transferability Attack to increase your probability by 100% of fooling a network, You can make more than just one model may be five models and then apply them. \"(Liu et al, 2016)\"</li> <li>Adversarial Examples are works for human brain also! for example images that tricks your eyes. They are a lot over the Internet.</li> <li>In practice some researches have fooled real models from (MetaMind, Amazon, Google)</li> <li>Someone has uploaded some perturbation into facebook and facebook was fooled :D</li> <li>What are the defenses?</li> <li>A lot of defenses Ian tried failed really bad! Including:<ul> <li>Ensembles</li> <li>Weight decay</li> <li>Dropout</li> <li>Adding noise at train time or at test time</li> <li>Removing perturbation with an autoencoder </li> <li>Generative modeling</li> </ul> </li> <li>Universal approximator theorem<ul> <li>Whatever shape we would like our classification function to have a big enough NN can make it.</li> <li>We could have train a NN that detects the Adversarial!</li> </ul> </li> <li>Linear models &amp; KNN can be fooled easier than NN. Neural nets can actually become more secure than other models. Adversarial trained neural nets have the best empirical success rate on adversarial examples of any machine learning model.<ul> <li>Deep NNs can be trained with non linear functions but we will just need a good optimization technique or solve the problem with using such linear activator like \"RELU\"</li> </ul> </li> <li>How to use adversarial examples to improve machine learning, even when there is no adversary?</li> <li>Universal engineering machine (model-based optimization)        <code>#Is called Universal engineering machine by Ian</code><ul> <li>For example:</li> <li>Imagine that we want to design a car that are fast.</li> <li>We trained a NN to look at the blueprints of a car and tell us if the blueprint will make us a fast car or not.</li> <li>The idea here is to optimize the input to the network so that the output will max this could give us the best blueprint for a car!</li> <li>Make new inventions by finding input that maximizes model\u2019s predicted performance.</li> <li>Right now by using adversarial examples we are just getting the results we don't like but if we have solve this problem we can have the fastest car, the best GPU, the best chair, new drugs.....</li> </ul> </li> <li>The whole adversarial is an active area of research especially defending the network!</li> <li>Conclusion</li> <li>Attacking is easy</li> <li>Defending is difficult</li> <li>Adversarial training provides regularization and semi-supervised learning </li> <li>The out-of-domain input problem is a bottleneck for model-based optimization generally</li> <li>There are a Github code that can make you learn everything about adversarial by code (Built above tensorflow):</li> <li>An adversarial example library for constructing attacks, building defenses, and benchmarking both: https://github.com/tensorflow/cleverhans</li> </ul> <p>  These Notes was made by Mahmoud Badry @2017</p>"},{"location":"Languages/English/English_Grammar_Notes/","title":"English Grammar Notes","text":"<ul> <li>English Grammar Notes<ul> <li>\u53e5\u5b50\u7ed3\u6784<ul> <li>\u82f1\u8bed\u53e5\u5b50\u6838\u5fc3\u539f\u5219</li> <li>\u5e73\u884c\u7ed3\u6784</li> <li>\u6bd4\u8f83\u7ed3\u6784</li> </ul> </li> <li>\u4e09\u5927\u4ece\u53e5<ul> <li>\u4ece\u53e5\u603b\u8bba</li> <li>\u5b9a\u8bed\u4ece\u53e5 adj.</li> <li>\u540d\u8bcd\u6027\u4ece\u53e5 n.</li> <li>\u72b6\u8bed\u4ece\u53e5 adv.</li> </ul> </li> <li>\u5c0f\u7ed3\u6784\uff08\u53ef\u4ee5\u548c\u4ece\u53e5\u4e92\u6362\uff09<ul> <li>\u975e\u8c13\u8bed\u52a8\u8bcd</li> <li>\u4ecb\u8bcd\u77ed\u8bed</li> <li>\u5f62\u5bb9\u8bcd\u77ed\u8bed\u4f5c\u540e\u7f6e\u5b9a\u8bed</li> <li>\u540c\u4f4d\u8bed</li> <li>\u63d2\u5165\u8bed</li> </ul> </li> <li>\u5c0f\u8bcd\u5927\u7528<ul> <li>\u52a8\u8bcd</li> <li>\u540d\u8bcd</li> <li>\u51a0\u8bcd</li> <li>\u4ee3\u8bcd</li> <li>\u4ecb\u8bcd</li> <li>\u5f62\u5bb9\u8bcd\u3001\u526f\u8bcd</li> </ul> </li> <li>\u7279\u6b8a\u53e5\u5f0f<ul> <li>\u5168\u90e8\u5012\u88c5</li> <li>\u90e8\u5206\u5012\u88c5</li> </ul> </li> </ul> </li> </ul> <p>Form &amp; Meaning: \u7528\u51c6\u786e\u7684Form\u4f20\u8fbe\u60f3\u8981\u4f20\u8fbe\u7684Meaning\u3002</p> <p></p>"},{"location":"Languages/English/English_Grammar_Notes/#_1","title":"\u53e5\u5b50\u7ed3\u6784","text":""},{"location":"Languages/English/English_Grammar_Notes/#_2","title":"\u82f1\u8bed\u53e5\u5b50\u6838\u5fc3\u539f\u5219","text":"<p>\u82f1\u8bed\u7684\u6700\u5c0f\u5355\u4f4d\u662f==\u53e5\u5b50==\uff0c\u4e00\u4e2a\u53e5\u53f7\uff08or\u5206\u53f7\uff09\u91cc\u9762\uff0c==\u6709\u4e14\u4ec5\u6709==<code>\u4e00\u4e2a\u4e3b\u8bed+\u4e00\u4e2a\u8c13\u8bed\u52a8\u8bcd</code></p> <p>Tips\uff1a</p> <ol> <li>\u60c5\u6001\u52a8\u8bcd<code>can do sth</code>\uff0c\u5373\u8c13\u8bed\u90e8\u5206\u3002</li> <li>\u4e3b\u8bed\u548c\u8c13\u8bed\u4e4b\u95f4\u6ca1\u6709\u9017\u53f7\u3002</li> <li><code>Take sb as an example.</code>==\uff08\u7948\u4f7f\u53e5\uff09==\u662f\u4e00\u53e5\u8bdd</li> </ol>"},{"location":"Languages/English/English_Grammar_Notes/#_3","title":"\u5e73\u884c\u7ed3\u6784","text":"<p>\u5e76\u5217\u8fde\u8bcd\u8fde\u63a5\u4e24\u4e2a\u6216\u591a\u4e2a\u8bed\u6cd5\u7ed3\u6784\u76f8\u540c\u7684==\u6210\u5206==\u3002</p> <p>\u5e38\u89c1\u7684\u5e76\u5217\u8fde\u8bcd\u6709:</p> <ol> <li><code>and, but, or, so</code></li> <li><code>not...but..., not only...but also..., both...and..., neither...nor..., either...or...,...rather than..., ...as well as...</code></li> </ol> <ul> <li>\u591a\u7ec4\u5e73\u884c A, B and C,and D</li> </ul> <p>Key points:</p> <ol> <li>\u5e76\u5217\u8fde\u8bcd\u524d\u540e\u8bed\u6cd5\u7ed3\u6784\u9700\u8981\u4e00\u81f4\u3002</li> </ol> <p>\u5e76\u5217\u8fde\u8bcd\u4e0d\u80fd\u653e\u5728\u53e5\u9996\uff0c\u5728\u53e5\u4e2d\u7684\u4f4d\u7f6e\u9700\u6709\u524d\u6709\u540e\u3002</p> <ol> <li>\u526f\u8bcd\u548c\u4ecb\u8bcd\u77ed\u8bed==\uff08\u72b6\u8bed\uff09==\u4e0d\u662f\u5e76\u5217\u8fde\u8bcd\uff0c\u4e0d\u80fd\u5e76\u5217\u4e24\u4e2a\u6210\u5206\u3002</li> </ol> <p>==\u72b6\u8bed==\u53ef\u5220\u53bb&amp;\u4f4d\u7f6e\u968f\u610f\uff0c\u4e0d\u5f71\u54cd\u53e5\u5b50\u7684\u8bed\u6cd5\u7ed3\u6784\u3002</p> <ol> <li>\u5e76\u5217\u8fde\u8bcd\u540e\u9762\u4e0d\u80fd\u52a0\u9017\u53f7\uff0c\u524d\u9762\u53ef\u52a0\u53ef\u4e0d\u52a0\u3002</li> </ol>"},{"location":"Languages/English/English_Grammar_Notes/#_4","title":"\u6bd4\u8f83\u7ed3\u6784","text":"<p>\u5e38\u89c1\u7684\u6bd4\u8f83\u7ed3\u6784\uff1a</p> <ul> <li><code>more...than...</code></li> <li><code>compared with...</code></li> <li><code>like../unlike...</code></li> <li><code>in contrast to...</code></li> </ul> <p>Key points:</p> <ol> <li>\u6bd4\u8f83\u5bf9\u8c61\u8981\u4e00\u81f4</li> </ol> <p>\u4e00\u81f4\u610f\u5473\u7740\u6bd4\u8f83\u5bf9\u8c61\u7684\u5f62\u5f0f\u548c\u610f\u4e49\u9700\u8981\u5bf9\u7b49</p>"},{"location":"Languages/English/English_Grammar_Notes/#_5","title":"\u4e09\u5927\u4ece\u53e5","text":""},{"location":"Languages/English/English_Grammar_Notes/#_6","title":"\u4ece\u53e5\u603b\u8bba","text":"<p>\u5728\u4e3b\u53e5\u4e2d\uff0c\u672c\u8eab\u662f==xx\u8bcd\u6027==\u7684\u5730\u65b9\u51fa\u73b0\u4e00\u4e2a\u53e5\u5b50\uff0c\u5c31\u53eb==xx\u8bcd\u6027\u4ece\u53e5==</p> <p>Key points:</p> <ol> <li>\u4ece\u53e5\u4e0d\u80fd\u5355\u72ec\u6210\u53e5</li> <li>\u4e3b\u53e5\u662f\u5b8c\u6574\u7684\u53e5\u5b50\uff0c\u8981\u6709\u4e3b\u8bed\u548c\u8c13\u8bed</li> <li>\u4ece\u53e5\u4e5f\u662f\u5b8c\u6574\u7684\u53e5\u5b50\uff0c\u8981\u6709\u4e3b\u8bed\u548c\u8c13\u8bed</li> <li>\u5b9a\u8bed\u4ece\u53e5 adj. \u8fd8\u539f\u6307\u4ee3\uff0c\u4ece\u53e5\u5b8c\u6574</li> <li>\u540d\u8bcd\u6027\u4ece\u53e5 n. \u4ece\u53e5\u5b8c\u6574</li> <li>\u72b6\u8bed\u4ece\u53e5 adv. \u4ece\u53e5\u5b8c\u6574</li> </ol>"},{"location":"Languages/English/English_Grammar_Notes/#adj","title":"\u5b9a\u8bed\u4ece\u53e5 <code>adj.</code>","text":"<p>Key points:</p> <ol> <li> <p>\u8fd8\u539f==\u6307\u4ee3==\uff0c\u4ece\u53e5\u5b8c\u6574</p> </li> <li> <p>\u5173\u7cfb\u4ee3\u8bcd:<code>that/who/whom/which</code>=\u6307\u4ee3\u524d\u9762\u7684N</p> </li> <li> <p>\u5173\u7cfb\u526f\u8bcd:<code>when/where/why</code>=\u6307\u4ee3\u4ecb\u8bcd+\u524d\u9762\u7684N=<code>\u4ecb\u8bcd+which/whom</code></p> </li> <li> <p>\u5c3d\u91cf\u5c31\u8fd1\u6307\u4ee3\uff0c\u8d8a\u8fd1\u8d8a\u597d\uff0c\u907f\u514d\u6b67\u4e49</p> </li> </ol> <p>\u4e3a\u907f\u514d\u6b67\u4e49\uff0c<code>which</code>\u6700\u597d\u4e0d\u8981\u6307\u4ee3\u524d\u9762\u6574\u53e5\u8bdd\u3002\u60f3\u8981\u6307\u4ee3\u6574\u53e5\u8bdd\u4f18\u5148\u8003\u8651\u975e\u8c13\u8bed</p> <ol> <li>\u5b9a\u8bed\u4ece\u53e5\u524d\u52a0\u9017\u53f7\u4e3a\u975e\u9650\u5236\u6027\u5b9a\u4ece\uff0c\u4e0d\u52a0\u9017\u53f7\u4e3a\u9650\u5236\u6027\u5b9a\u4ece</li> </ol> <p>\u975e\u9650\u5236\u6027\u5b9a\u4ece\u987a\u7740\u7ffb\u8bd1\uff0c\u4ece\u53e5\u4e3a\u4e3b\u53e5\u7684\u8865\u5145\u4fe1\u606f\u3002</p> <p>\u9650\u5236\u6027\u5b9a\u4ece\u7ffb\u8bd1\u4e3a\u540e\u9762\u7684\u524d\u9762\uff0c\u4ece\u53e5\u662f\u9650\u5b9a\u4e3b\u53e5\u7684\u3002</p> <p>==Tip==\u8003\u8bd5\u65f6\u53ef\u4ee5\u628a\u9017\u53f7\u90fd\u52a0\u4e0a\uff08\u53e5\u5b50\u7ed3\u6784\u6e05\u6670\uff09</p>"},{"location":"Languages/English/English_Grammar_Notes/#n","title":"\u540d\u8bcd\u6027\u4ece\u53e5 <code>n.</code>","text":"<p>\u5305\u62ec<code>\u4e3b\u8bed\u4ece\u53e5\u3001\u8868\u8bed\u4ece\u53e5\u3001\u5bbe\u8bed\u4ece\u53e5\u3001\u540c\u4f4d\u8bed\u4ece\u53e5</code></p> <p>\u5f15\u5bfc\u540d\u8bcd\u6027\u4ece\u53e5\u7684\u8fde\u63a5\u8bcd\uff1a</p> <ul> <li>\u5728\u4ece\u53e5\u4e2d\u4e0d\u5145\u5f53\u6210\u5206:<code>that, whether</code></li> </ul> <p>\u4e0d\u5145\u5f53\u6210\u5206\u662f\u6307\u628a\u8fde\u63a5\u8bcd\uff08<code>that, whether</code>\uff09\u76d6\u6389\u4e0d\u770b\uff0c\u5269\u4e0b\u7684\u53e5\u5b50\u6709\u4e3b\u8c13</p> <ul> <li>\u5728\u4ece\u53e5\u4e2d\u5145\u5f53\u6210\u5206\uff1a<code>what</code><code>, who, whom, which, when, where, how</code></li> </ul> <p>Key points:</p> <ol> <li> <p>\u4ece\u53e5\u5b8c\u6574\uff0c\u8981\u6709\u4e3b\u8bed\u548c\u8c13\u8bed</p> </li> <li> <p>\u4ece\u53e5\u9700\u8981\u65f6\u9648\u8ff0\u8bed\u5e8f</p> </li> <li> <p><code>it</code>\u505a\u5f62\u5f0f\u4e3b\u8bed/\u5f62\u5f0f\u5bbe\u8bed ==\u5192\u5145native speaker\u7684\u52a0\u5206\u9879==</p> </li> <li> <p>\u5f62\u5f0f\u4e3b\u8bed\uff08\u907f\u514d\u5934X\u8fc7\u91cd)</p> <p>X + <code>is</code> + <code>n/adj.</code> :arrow_right: <code>It is</code> + <code>n/adj.</code> + X</p> <p>\u53ef\u4ee5\u628aX\u8fd8\u539f\u56de\u53bb\uff08\u66ff\u6362\u6389<code>it</code>\u68c0\u67e5\u53e5\u5b50\u662f\u5426\u6b63\u786e\u3002</p> <ol> <li>X\u662f\u7531\u8fde\u63a5\u8bcd<code>that</code>\u5f15\u5bfc\u7684\u4e3b\u8bed\u4ece\u53e5</li> <li>X\u662f<code>to do sth</code>\u4e0d\u5b9a\u5f0f</li> </ol> </li> <li> <p>\u5f62\u5f0f\u5bbe\u8bed\uff08\u907f\u514d\u4e2d\u95f4Y\u592a\u957f</p> <p><code>make, find, think, feel, consider, believe</code>+ Y + <code>adj/n.</code> :arrow_right: <code>make, find, think, feel, consider, believe</code> + <code>it</code> + <code>adj/n.</code> + Y</p> <ol> <li>Y\u662f\u8fde\u63a5\u8bcd<code>that</code>\u5f15\u5bfc\u7684\u5bbe\u8bed\u4ece\u53e5</li> <li>Y\u662f<code>(for sb) to do sth</code>\u4e0d\u5b9a\u5f0f</li> </ol> </li> </ol>"},{"location":"Languages/English/English_Grammar_Notes/#adv","title":"\u72b6\u8bed\u4ece\u53e5 <code>adv.</code>","text":"<p>==\u4ece\u5c5e\u8fde\u8bcd/\u8bcd\u7ec4== + \u4ece\u53e5\uff0c \u4e3b\u53e5</p> <p>\u4ece\u5c5e\u8fde\u8bcd\uff1a<code>When, After, Before, Because, If, Although, Even though</code></p> <p>\u8bcd\u7ec4\uff1a<code>,so that ...</code>, <code>so adj/adv. that ...</code>, <code>such n. that ...</code></p> <p>Key points:</p> <ol> <li> <p>\u4ece\u53e5\u5b8c\u6574\uff0c\u8981\u6709\u4e3b\u8bed\u548c\u8c13\u8bed</p> </li> <li> <p>\u72b6\u8bed\u4ece\u53e5\u7684\u7701\u7565</p> </li> <li> <p>\u7701\u7565\u6761\u4ef6\uff1a\u5f53\u4e3b\u53e5\u548c\u4ece\u53e5\u4e2d\u7684\u4e3b\u8bed\u4e00\u81f4\u65f6\uff0c\u53ef\u4ee5\u7701\u7565\u4ece\u53e5\u4e2d\u7684\u4e3b\u8bed\u548cbe\u52a8\u8bcd</p> </li> <li> <p>\u72b6\u8bed\u4ece\u53e5\u7701\u7565\u540e\u7684\u53e5\u5b50\u7ed3\u6784\uff1a ==\u4ece\u5c5e\u8fde\u8bcd== + ving/ved, ==\u4e3b\u8bed== + ==\u8c13\u8bed==</p> <p>\u4e3b\u8bed\u4e0ev\u4e3a\u4e3b\u52a8\u5173\u7cfb\u65f6\u7528<code>ing</code>\u5f62\u5f0f\uff0c\u88ab\u52a8\u5173\u7cfb\u65f6\u7528<code>ed</code>\u5f62\u5f0f</p> </li> </ol>"},{"location":"Languages/English/English_Grammar_Notes/#_7","title":"\u5c0f\u7ed3\u6784\uff08\u53ef\u4ee5\u548c\u4ece\u53e5\u4e92\u6362\uff09","text":"<ul> <li>\u975e\u8c13\u8bed\u52a8\u8bcd &lt;-&gt; <code>\u72b6\u4ece/\u5b9a\u4ece</code></li> <li>\u4ecb\u8bcd\u77ed\u8bed &lt;-&gt; <code>\u72b6\u4ece</code></li> <li>\u5f62\u5bb9\u8bcd\u77ed\u8bed\u4f5c\u540e\u7f6e\u5b9a\u8bed &lt;-&gt; <code>\u5b9a\u4ece</code></li> <li>\u540c\u4f4d\u8bed &lt;-&gt; <code>\u540c\u4f4d\u8bed\u4ece\u53e5</code></li> <li>\u63d2\u5165\u8bed &lt;-&gt; <code>__</code></li> </ul>"},{"location":"Languages/English/English_Grammar_Notes/#_8","title":"\u975e\u8c13\u8bed\u52a8\u8bcd","text":"<p>\u8c13\u8bed\u52a8\u8bcd</p> <ul> <li>be\u52a8\u8bcd <code>am/is are</code></li> <li>\u65f6\u6001\u53d8\u5316 <code>do/does/did</code></li> <li>\u88ab\u52a8 <code>be done</code></li> <li>\u60c5\u6001\u52a8\u8bcd <code>can do</code></li> </ul> <p>\u975e\u8c13\u8bed\u52a8\u8bcd</p> <ul> <li> <p>\u4e0d\u5b9a\u5f0f <code>to do</code></p> </li> <li> <p>\u52a8\u540d\u8bcd <code>doing</code> = <code>n.</code></p> </li> <li> <p>:star:\u5206\u8bcd\u5f62\u5f0f <code>doing/done</code></p> </li> <li> <p><code>doing</code>\u8868\u793a\u4e3b\u52a8\u3001\u6b63\u5728\u88ab</p> </li> <li><code>done</code>\u8868\u793a\u88ab\u52a8</li> </ul> <p>Key points:</p> <p>\u5206\u8bcd\u53ef\u4ee5\u4f5c\u5b9a\u8bed\u548c\u72b6\u8bed</p> <ol> <li> <p>\u5206\u8bcd\u4f5c\u5b9a\u8bed (\u4fee\u9970\u8c01\u653e\u5728\u8c01\u540e\u9762)</p> </li> <li> <p>\u53ef\u4ee5\u548c\u5b9a\u8bed\u4ece\u53e5\u4e92\u6362\uff0c\u653e\u5728\u4efb\u610f\u540d\u8bcd\u540e\u5bf9\u5176\u4fee\u9970\uff0c\u5373 <code>n.</code> + <code>doing/done</code></p> </li> <li> <p>\u5206\u8bcd\u4f5c\u72b6\u8bed \uff08\u4f4d\u4e8e\u53e5\u9996\u4f34\u968f\u4e3b\u8bed\uff1b\u4f4d\u4e8e\u53e5\u672b\u4f34\u968f\u4e3b\u8bed\u6216\u8005\u4f5c\u4e3a\u6574\u4e2a\u53e5\u5b50\u7684\u7ed3\u679c\uff0c\u53e5\u672b\u53ea\u6709<code>doing</code>\u5f62\u5f0f\uff09</p> </li> </ol> <p>\u548c\u72b6\u8bed\u4ece\u53e5\u4e92\u6362\uff0c\u653e\u5728\u53e5\u9996\u6216\u53e5\u672b\uff0c\u5373</p> <ol> <li> <p><code>Doing/Done</code>, SVO.\uff08\u4f34\u968f\u72b6\u8bed\uff09</p> <p>SVO is a sentence, which means <code>Subject + Verb + Object</code></p> <p>\u5206\u8bcd\u4f5c\u4f34\u968f\u72b6\u8bed\uff0c\u662f\u4f34\u968f\u4e3b\u8bed\u7684\u52a8\u4f5c\uff0c\u5206\u8bcd\u52a8\u4f5c\u5148\uff0c\u8c13\u8bed\u52a8\u4f5c\u540e\u3002 Doing/Done, SVO.</p> <ul> <li>\u5206\u8bcd\u91c7\u7528<code>Being done</code>\u5f62\u5f0f\u65f6\u5f3a\u8c03<code>\u6b63\u5728\u88ab</code></li> </ul> </li> <li> <p>SVO, doing. (\u4f34\u968f\u72b6\u8bed\u3001\u7ed3\u679c\u72b6\u8bed)</p> <ul> <li> <p>\u5206\u8bcd\u4f5c\u4f34\u968f\u72b6\u8bed\uff0c\u662f\u4f34\u968f\u4e3b\u8bed\u7684\u52a8\u4f5c\uff0c\u8c13\u8bed\u52a8\u4f5c\u5148\uff0c\u5206\u8bcd\u52a8\u4f5c\u540e</p> <p>e.g. Students enter the classroom, carrying their books.</p> </li> <li> <p>:star:\u5206\u8bcd\u4f5c\u7ed3\u679c\u72b6\u8bed\uff0c\u662f\u6574\u4e2a\u53e5\u5b50\u7684\u7ed3\u679c</p> <p>e.g. Students enter the classroom, surprising their teacher.</p> </li> </ul> </li> </ol>"},{"location":"Languages/English/English_Grammar_Notes/#_9","title":"\u4ecb\u8bcd\u77ed\u8bed","text":"<p>\u7ed3\u6784 <code>prep.</code> + <code>n.</code></p> <p>\u7279\u6b8a <code>with/without</code>\u7684\u590d\u5408\u7ed3\u6784</p> <p>Key points:</p> <ol> <li>\u4ecb\u8bcd + <code>doing</code>\uff0c\u6ce8\u610f\u5206\u8bcd\u7684\u903b\u8f91\u4e3b\u8bed \uff08<code>prep.</code> + doing, S V O.)</li> </ol> <p>e.g. By communicating effciently with others, employees can win support and assistance.</p> <ol> <li>\u4ecb\u8bcd\u540e\u9762\u53ea\u80fd\u52a0\u540d\u8bcd \uff08<code>prep.</code> + <code>n.</code>; \u4e0d\u80fd\u662f<code>prep.</code> + <code>\u53e5\u5b50</code>)</li> </ol> <p>e.g. because of diligence; because of the fact that \u4ece\u53e5 \uff08\u6b64\u5904\u4ece\u53e5\u4e0e\u5176\u6307\u4ee3\u7684\u540d\u8bcd\u4e3a\u5e76\u5217\u5173\u7cfb\uff0c\u4e3a\u540c\u4f4d\u8bed\u4ece\u53e5\uff09</p> <ol> <li><code>with/without</code>\u590d\u5408\u7ed3\u6784\uff0c\u4e0d\u8003\u8651\u903b\u8f91\u4e3b\u8bed</li> </ol> <p>\u7ed3\u6784\uff1a</p> <p><code>with/without</code> + <code>\u5bbe\u8bed</code> + <code>\u526f\u8bcd/\u540d\u8bcd/\u5f62\u5bb9\u8bcd/\u4ecb\u8bcd\u77ed\u8bed/doing(\u4e3b\u52a8\uff0c\u6b63\u5728\u88ab)/done(\u88ab\u52a8)/to do(\u5c06\u8981\u53d1\u751f)</code>\uff0c \u4e3b \u8c13. </p> <p>&lt;=&gt;</p> <p><code>with/without</code> + ==\u72ec\u7acb\u4e3b\u683c==\uff0c \u4e3b \u8c13.</p> <p>:warning:\u5728<code>with/without</code>\u590d\u5408\u7ed3\u6784\u4e2d\uff0c<code>\u5bbe\u8bed</code>\u548c\u540e\u9762\u7684\u903b\u8f91\u4e3b\u8bed\u5e38\u5e38\u4e0d\u540c\u3002 \uff08\u8be5\u7ed3\u6784\u9f13\u52b1\u7a81\u51fa\u8fd9\u79cd\u4e0d\u540c\uff09</p> <p>e.g. With a <code>supermarket</code> in the vicinity, people can purchase food and other necessities conveniently.</p>"},{"location":"Languages/English/English_Grammar_Notes/#_10","title":"\u5f62\u5bb9\u8bcd\u77ed\u8bed\u4f5c\u540e\u7f6e\u5b9a\u8bed","text":"<p><code>n.</code> + <code>adj\u77ed\u8bed</code></p> <p><code>adj\u77ed\u8bed</code>\u4f5c\u540e\u7f6e\u5b9a\u8bed\uff0c\u4fee\u9970\u524d\u9762\u7684<code>n.</code>\uff0c\u53ef\u4ee5\u548c\u5b9a\u8bed\u4ece\u53e5\u4e92\u6362\uff0c\u4e00\u822c\u4e0d\u52a0\u9017\u53f7\uff08\u52a0\u4e0a\u4e0d\u7b97\u9519\uff09\u3002</p> <p>e.g. People lack the expertise applicable in community acticities.</p>"},{"location":"Languages/English/English_Grammar_Notes/#_11","title":"\u540c\u4f4d\u8bed","text":"<p><code>n1, n2</code> \u5176\u4e2d\uff0c<code>n2</code>\u53eb\u505a\u540c\u4f4d\u8bed\u3002<code>n2</code>\u53ef\u4ee5\u6362\u505a\u540c\u4f4d\u8bed\u6216\u8005\u5b9a\u8bed\u4ece\u53e5\u3002</p> <p>e.g. Zhejiang university, a well-known school, attracts high school graduates.</p> <p>&lt;=&gt; Zhejiang university, which is a well-known school, attracts high school graduates.</p>"},{"location":"Languages/English/English_Grammar_Notes/#_12","title":"\u63d2\u5165\u8bed","text":"<ol> <li> <p>\u6e38\u79bb\u5728\u53e5\u5b50\u8bed\u6cd5\u4e4b\u5916\u7684\u7ed3\u6784\uff0c\u5bf9\u53e5\u4e2d\u610f\u601d\u4f5c\u8865\u5145\u8bf4\u660e\uff0c\u5220\u53bb\u4e4b\u540e\u53e5\u5b50\u8bed\u6cd5\u4ecd\u7136\u6b63\u786e</p> </li> <li> <p>\u7ed3\u6784\u5f62\u5f0f\u591a\u6837\uff0c\u6240\u6709\u7684\u4ece\u53e5\u4ee5\u53ca\u5c0f\u7ed3\u6784\u90fd\u53ef\u4ee5\u63d2\u5728\u53e5\u5b50\u4e2d\u95f4\uff0c\u4f5c\u63d2\u5165\u8bed\u3002</p> </li> <li> <p>\u63d2\u5165\u8bed\u7684\u4f4d\u7f6e\uff1a</p> </li> <li> <p>\u672c\u8eab\u653e\u5728\u4e3b\u8bed\u524d\u9762\u7684\u7ed3\u6784\uff0c\u63d2\u5728\u4e86\u4e3b\u8bed\u7684\u540e\u9762</p> <p>e.g.After graduating from BNU, she became a teacher.</p> <p>\u200b  She, after graduating from BNU, became a teacher.</p> </li> <li> <p>\u8865\u5145\u5728\u4efb\u4f55\u7a81\u7136\u60f3\u89e3\u91ca\u8bf4\u660e\u7684\u90e8\u5206\u540e\u9762\uff08\u9605\u8bfb\u4e2d\u8f83\u591a\uff0c\u548c\u5e73\u5e38\u5b66\u7684\u975e\u8c13\u8bed\u3001\u4ece\u53e5\u7b49\u4e00\u6837\uff09</p> <p>e.g. People, especially with dissension, should communicate with teachers.</p> </li> </ol>"},{"location":"Languages/English/English_Grammar_Notes/#_13","title":"\u5c0f\u8bcd\u5927\u7528","text":"<p>:warning:\u5199\u4f5c\u8fc7\u7a0b\u4e2d\u63d0\u9ad8\u8bcd\u6027\u610f\u8bc6</p>"},{"location":"Languages/English/English_Grammar_Notes/#_14","title":"\u52a8\u8bcd","text":"<p>Key points:</p> <ol> <li>\u65f6\u6001</li> </ol> <p>\u65f6\u6001\u7684\u6838\u5fc3\u662f\u5177\u4f53\u8fd9\u4e2a\u52a8\u4f5c\u662f\u4ec0\u4e48\u65f6\u5019\u5c31\u7528\u4ec0\u4e48\u65f6\u6001\uff0c\u4e00\u4e2a\u53e5\u5b50\u7684\u65f6\u6001\u53ef\u4ee5\u4e0d\u540c\u3002</p> <p>e.g. She said the sun rises from the east.</p> <ul> <li> <p>\u72ec\u7acb\u5199\u4f5c\u65f6\u6001</p> <ul> <li>\u903b\u8f91\u63a8\u7406\u90e8\u5206\u90fd\u662f\u73b0\u5728\u65f6\u6001\uff0c\u5c06\u6765\u65f6\u6001\u3002</li> </ul> <p>==\u73b0\u5728\u65f6\u6001==\u5305\u62ec\uff1a \u4e00\u822c\u73b0\u5728\u65f6<code>do/does</code>\uff0c\u73b0\u5728\u5b8c\u6210\u65f6<code>has/have done</code>\uff0c\u73b0\u5728\u8fdb\u884c\u65f6<code>is/are doing</code>\uff0c\u73b0\u5728\u5b8c\u6210\u8fdb\u884c\u65f6<code>has/have been doing</code></p> <p>==\u5c06\u6765\u65f6\u6001==\u5305\u62ec: \u4e00\u822c\u5c06\u6765\u65f6 <code>will do</code></p> <ul> <li>\u4f8b\u5b50\u90e8\u5206\uff1a \u770b\u5177\u4f53\u7684\u52a8\u4f5c</li> </ul> <p>e.g. Take a student graduating from Peking University as an example, who once worked hard in college and was offered a position in Google. In the workplace, he maintains a strong relationship with others.</p> </li> <li> <p>\u7efc\u5408\u5199\u4f5c\u65f6\u6001</p> <ul> <li>\u6a21\u7248\uff1a\u7edf\u4e00\u65f6\u6001\uff0c\u63a8\u8350\u7edf\u4e00\u7528\u4e00\u822c\u73b0\u5728\u65f6</li> <li>\u586b\u5165\uff1a\u6839\u636e\u63cf\u8ff0\u5bf9\u8c61\uff0c\u548c\u9605\u8bfb\u4fdd\u6301\u4e00\u81f4</li> </ul> </li> <li> <p>\u4e3b\u8c13\u4e00\u81f4</p> </li> </ul> <p>\u770b\u6e05\u4e3b\u8bed\u662f\u8c01\uff0c\u4e3b\u8bed\u7684\u5355\u590d\u6570</p> <ol> <li> <p>\u4ec5\u6709\u4e00\u4e2a<code>un./to do/doing</code>\u65f6\uff0c\u8c13\u8bed\u5355\u6570\uff1b\u4f46\u7528<code>and</code>\u5e76\u5217\u65f6\uff0c\u8c13\u8bed\u590d\u6570</p> <p>e.g. Tea is my farite drink.</p> <p>\u200b  Tea and milk are my favorite drinks.</p> </li> <li> <p>\u5c31\u8fd1\u539f\u5219</p> <p>\u591a\u4e2a\u5e76\u5217\u540d\u8bcd\u4e2d\uff0c\u5355\u590d\u6570\u901a\u8fc7\u6700\u540e\u4e00\u4e2a\u786e\u5b9a </p> <p><code>not only A but also</code> B; <code>neither A nor</code> B; <code>either A or</code> B; <code>A or</code> B</p> <p>e.g. Neither students nor their teacher is tired.</p> </li> <li> <p>\u533a\u522b\u526f\u8bcd\u548c\u60c5\u6001\u52a8\u8bcd</p> <p>:warning:\u526f\u8bcd\u5220\u53bb\u5224\u65ad\u52a8\u8bcd\u5355\u590d\u6570\uff0c\u60c5\u6001\u52a8\u8bcd\u540e\u9762\u90fd\u662f\u539f\u578b</p> </li> </ol>"},{"location":"Languages/English/English_Grammar_Notes/#_15","title":"\u540d\u8bcd","text":"<p>Key points:</p> <ol> <li> <p><code>cn.</code> or <code>un.</code>?</p> </li> <li> <p>\u4efb\u4f55\u540d\u8bcd\u90fd\u6709\u590d\u6570\u5f62\u5f0f\uff0c\u9700\u8981\u6839\u636e\u5176\u5728\u53e5\u4e2d\u7684\u5177\u4f53\u610f\u601d\u5224\u65ad\uff0c\u5982<code>waters/equipments/peoples</code></p> </li> <li> <p>\u6839\u636e\u610f\u601d\uff0c\u53ef\u4ee5\u4e00\u4e2a\u4e00\u4e2a\u6570\u7684\u5c31\u53ef\u4ee5\u91c7\u7528\u590d\u6570\u5f62\u5f0f\uff0c\u5426\u5219\u4e0d\u91c7\u7528</p> </li> <li> <p>\u540d\u8bcd\u7684\u5b58\u5728\u5f62\u5f0f</p> </li> <li> <p>\u53ef\u6570\u540d\u8bcd\u7684\u5b58\u5728\u5f62\u5f0f\uff08<code>cn.</code>\u4e0d\u5355\u72ec\u88f8\u5954\uff0c\u9700\u8981\u52a0\u590d\u6570/\u51a0\u8bcd/\u7269\u4e3b\u4ee3\u8bcd\uff09</p> <p><code>cns</code>, <code>the cns</code>, <code>the/a/an cn</code>, <code>his/Tom's cn/cns</code></p> </li> <li> <p>\u4e0d\u53ef\u6570\u540d\u8bcd\u7684\u5b58\u5728\u5f62\u5f0f</p> <p><code>un</code>, <code>the un</code></p> </li> </ol>"},{"location":"Languages/English/English_Grammar_Notes/#_16","title":"\u51a0\u8bcd","text":"<p>Key points:</p> <ol> <li> <p>\u4ec0\u4e48\u65f6\u5019\u52a0<code>the</code>\uff1f</p> </li> <li> <p>\u524d\u6587\u6709\u63d0\u5230\u8fc7</p> <p>e.g. She bought a computer. I like the computer.</p> </li> <li> <p>\u8be5\u540d\u8bcd\u540e\u9762\u6709\u4fee\u9970</p> <p>e.g. The computer she bought was cheap.</p> <p>\u200b   The computers in the store are cheap.</p> </li> </ol> <p>==Tip==\u5b66\u4f1a\u7528\u4ee3\u8bcd\uff0c\u907f\u514d\u7ea0\u7ed3\u662f\u5426\u52a0<code>the</code></p> <p>\u200b    e.g. Neither students nor their teacher is tired.</p>"},{"location":"Languages/English/English_Grammar_Notes/#_17","title":"\u4ee3\u8bcd","text":"<p>Key points:</p> <ol> <li>\u4ee3\u8bcd\u662f\u6307\u4ee3\u540d\u8bcd\u7684\u8bcd\uff0c\u5c3d\u91cf\u4e0d\u6307\u4ee3\u53e5\u5b50\u3002</li> </ol> <p>e.g.<code>Original:</code> Students suffer from much pressure and it worries parents.</p> <p>\u200b    <code>Revision:</code> Students suffer from much pressure, worrying their parents.</p> <ol> <li>\u76f8\u540c\u7684\u4ee3\u8bcd\u6307\u4ee3\u76f8\u540c\u7684\u5bf9\u8c61</li> </ol> <p>e.g. <code>Original:</code> Teachers impart knowledge to students and ==they== are best their best friends.</p> <p>\u200b     <code>Revision:</code> Teachers, who impart knowledge to students, are their best friends.</p> <ol> <li>\u4ee3\u8bcd\u80fd\u8ba9\u6587\u7ae0\u8bed\u8a00\u548c\u903b\u8f91\u66f4\u7d27\u51d1</li> </ol> <p>e.g. <code>Original:</code> Education triggers heated discussions over the methods of improving the quality.</p> <p>\u200b     <code>Revision:</code> Ecucation triggers heated discussions over the methods of improving its quality.</p>"},{"location":"Languages/English/English_Grammar_Notes/#_18","title":"\u4ecb\u8bcd","text":"<p>Key points:</p> <p>\u4ecb\u8bcd\u80fd\u8868\u660e\u4e24\u4e2a\u4e8b\u7269\u4e4b\u95f4\u7684\u660e\u786e\u5173\u7cfb\uff0c\u5373<code>B prep. A</code> &gt; <code>AB</code></p> <p>e.g. the quality of education &gt; the education quality (\u4e0d\u591f\u597d\uff0c\u4e5f\u4e0d\u5bf9)"},{"location":"Languages/English/English_Grammar_Notes/#_19","title":"\u5f62\u5bb9\u8bcd\u3001\u526f\u8bcd","text":"<p><code>adj.</code>\u4fee\u9970<code>n.</code>\uff1b <code>adv.</code>\u4fee\u9970<code>adj./adv./verb./\u4e00\u4e2a\u53e5\u5b50</code></p> <p>Key points:</p> <ol> <li><code>adj.</code>\u548c<code>adv.</code>\u7684\u6bd4\u8f83\u7ea7</li> </ol> <p><code>more</code>\u65e2\u662f<code>adj.</code>\u53c8\u662f<code>adv.</code></p> <p>e.g. She does more exercise. (\u63d0\u524dthe more\u7684\u90e8\u5206)</p> <ul> <li> <p>more\u4f5c\u4e3a<code>adj.</code>\uff0cthe more\u7684\u90e8\u5206\u5373<code>the more + n.</code></p> <p>The more exercise she does,</p> </li> <li> <p>more\u4f5c\u4e3a<code>adv.</code>\u4fee\u9970<code>verb.</code>\uff0cthe more\u7684\u90e8\u5206\u5373<code>the more</code></p> <p>The more she does exercise,</p> </li> </ul>"},{"location":"Languages/English/English_Grammar_Notes/#_20","title":"\u7279\u6b8a\u53e5\u5f0f","text":"<p>==\u5168\u90e8\u5012\u88c5\uff1a==\u628a\u8c13\u8bed\uff08\u542b<code>be</code>\uff09\u5168\u90e8\u653e\u5728\u4e3b\u8bed\u4e4b\u524d</p> <p>==\u90e8\u5206\u5012\u88c5\uff1a==\u53ea\u628a\u52a9\u52a8\u8bcd\u6216\u60c5\u6001\u52a8\u8bcd\u653e\u5728\u4e3b\u8bed\u4e4b\u524d</p> <ul> <li>\u5e38\u89c1\u7684\u52a9\u52a8\u8bcd\uff1a<code>be, have, has, do, does, did</code></li> </ul> <p><code>be</code>\u88ab\u8ba4\u4e3a\u662f\u7cfb\u52a8\u8bcd\uff0c\u4e5f\u5c31\u662f\u8c13\u8bed\uff1b\u4e5f\u88ab\u8ba4\u4e3a\u662f\u52a9\u52a8\u8bcd\uff0c\u4e0d\u7ba1\u534a\u5012\u88c5\u8fd8\u662f\u5168\u90e8\u5012\u88c5\uff0c\u90fd\u628a<code>be</code>\u653e\u5728\u524d\u9762</p> <ul> <li>\u5e38\u89c1\u7684\u60c5\u6001\u52a8\u8bcd\uff1a <code>will, should, shall, must, would</code></li> </ul> <p>\u5199\u4f5c\u5e38\u7528\u5012\u88c5\uff1a</p> <ol> <li>\u5168\u5012\u88c5\uff1a</li> </ol> <p><code>Among Ns is xx.</code>\u4e00\u822c\u7528\u4e8e\u4e3e\u4f8b</p> <ol> <li> <p>\u534a\u5012\u88c5\uff1a</p> </li> <li> <p><code>\u5426\u5b9a\u8bcd\u653e\u5728\u53e5\u9996</code> + <code>\u534a\u5012\u88c5</code></p> <p>\u7279\u6b8a\uff1a <code>Not only</code> + <code>\u534a\u5012\u88c5</code>, <code>but also</code> + <code>\u6b63\u5e38\u5b8c\u6574</code></p> </li> <li> <p><code>Only</code> + <code>\u4ecb\u8bcd\u77ed\u8bed</code> + <code>\u534a\u5012\u88c5</code></p> </li> <li> <p><code>So/such</code> + <code>\u534a\u5012\u88c5</code> + <code>that</code> + <code>\u6b63\u5e38\u5b8c\u6574</code></p> </li> </ol>"},{"location":"Languages/English/English_Grammar_Notes/#_21","title":"\u5168\u90e8\u5012\u88c5","text":"<p>\u8868\u793a\u5730\u70b9\u7684\u72b6\u8bed\u7f6e\u4e8e\u53e5\u9996\uff0c\u53e5\u5b50\u53d1\u751f\u5168\u5012\u88c5</p> <p>e.g. In front of computers sits a student.</p> <p>\u200b    Among overwhelmed students is Tom</p>"},{"location":"Languages/English/English_Grammar_Notes/#_22","title":"\u90e8\u5206\u5012\u88c5","text":"<p>Key points:</p> <ol> <li>\u5426\u5b9a\u526f\u8bcd/\u4ecb\u8bcd\u77ed\u8bed\u653e\u5728\u53e5\u9996 + \u534a\u5012\u88c5</li> </ol> <p>==\u5e38\u89c1\u7684\u5426\u5b9a\u526f\u8bcd/\u4ecb\u8bcd\u77ed\u8bed\uff1a== never, seldom, rarely, barely, scarcely, hardly, little, neither, nor, under no cirtumstances, on no occasion, in no case</p> <p>e.g. Never should we ignore the significance of education.</p> <p>\u200b    Under no circumstances can people be excluded from junk food. <p>\u7279\u6b8a\u7684\u5426\u5b9a\u8bcd\uff1a <code>Not only</code> + <code>\u534a\u5012\u88c5</code>, <code>but also</code> + <code>\u6b63\u5e38\u5b8c\u6574</code></p> <p>e.g. Not only will they hone their skills, but (also) they can cultivate their personalities.</p> <p>:warning: <code>...not only...but also...</code>\u7ed3\u6784\u4e0d\u5012\u88c5\u65f6\uff0c\u540e\u534a\u90e8\u5206\u4e3b\u8bed\u53ef\u4ee5\u7701\u53bb\uff1b<code>Not only</code>\u653e\u5728\u53e5\u9996\u65f6\uff0c\u53e5\u5b50\u5fc5\u987b\u534a\u5012\u88c5\uff0c\u5012\u88c5\u65f6\uff0c<code>but also</code>\u540e\u9762\u4e3b\u8bed\u4e0d\u53ef\u7701\u53bb</p> <ol> <li>Only + \u72b6\u8bed + \u534a\u5012\u88c5</li> </ol> <p><code>Only</code> + <code>\u526f\u8bcd/\u4ecb\u8bcd\u77ed\u8bed/\u72b6\u8bed\u4ece\u53e5</code> + \u534a\u5012\u88c5</p> <ol> <li><code>so/such ... that</code>\u7ed3\u6784\u4e2d<code>So/Such</code>\u653e\u5728\u53e5\u9996\uff0c\u53d1\u751f\u534a\u5012\u88c5</li> </ol> <p>\u6b63\u5e38\u7684<code>so/such ... that</code>\u7ed3\u6784\u4e3a\uff1a</p> <ul> <li>\u4e3b \u8c13 <code>so</code> + <code>adj./adv.</code> + that\u7ed3\u679c\u72b6\u8bed\u4ece\u53e5</li> <li>\u4e3b \u8c13 <code>such</code> + <code>n.</code> + that\u7ed3\u679c\u72b6\u8bed\u4ece\u53e5</li> </ul> <p>e.g.1 <code>Original:</code> He is so honest that his subordinates admires him.</p> <p>\u200b      <code>Revision:</code> So honest is he that his subordinates admires him.</p> <p>e.g.2 <code>Original:</code> He works so hard that he wins the competition.</p> <p>\u200b      <code>Revision:</code> So hard does he works that he wins the competition.</p>"},{"location":"Languages/English/IELTS_Notes/","title":"IELTS Notes","text":"<ul> <li>IELTS Notes<ul> <li>Grammar<ul> <li>\u53e5\u5b50\u7ed3\u6784<ul> <li>\u82f1\u8bed\u53e5\u5b50\u6838\u5fc3\u539f\u5219</li> <li>\u5e73\u884c\u7ed3\u6784</li> <li>\u6bd4\u8f83\u7ed3\u6784</li> </ul> </li> <li>\u4e09\u5927\u4ece\u53e5<ul> <li>\u4ece\u53e5\u603b\u8bba</li> <li>\u5b9a\u8bed\u4ece\u53e5 adj.</li> <li>\u540d\u8bcd\u6027\u4ece\u53e5 n.</li> <li>\u72b6\u8bed\u4ece\u53e5 adv.</li> </ul> </li> <li>\u5c0f\u7ed3\u6784\uff08\u53ef\u4ee5\u548c\u4ece\u53e5\u4e92\u6362\uff09<ul> <li>\u975e\u8c13\u8bed\u52a8\u8bcd</li> <li>\u4ecb\u8bcd\u77ed\u8bed</li> <li>\u5f62\u5bb9\u8bcd\u77ed\u8bed\u4f5c\u540e\u7f6e\u5b9a\u8bed</li> <li>\u540c\u4f4d\u8bed</li> <li>\u63d2\u5165\u8bed</li> </ul> </li> <li>\u5c0f\u8bcd\u5927\u7528<ul> <li>\u52a8\u8bcd</li> <li>\u540d\u8bcd</li> <li>\u51a0\u8bcd</li> <li>\u4ee3\u8bcd</li> <li>\u4ecb\u8bcd</li> <li>\u5f62\u5bb9\u8bcd\u3001\u526f\u8bcd</li> </ul> </li> <li>\u7279\u6b8a\u53e5\u5f0f<ul> <li>\u5168\u90e8\u5012\u88c5</li> <li>\u90e8\u5206\u5012\u88c5</li> </ul> </li> </ul> </li> <li>Writing<ul> <li>\u5c0f\u4f5c\u6587</li> <li>\u5927\u4f5c\u6587<ul> <li>\u4f18\u7f3a\u70b9\u7c7b\u9898\u76ee</li> <li>\u89c2\u70b9\u7c7b\u9898\u76ee</li> <li>\u8bba\u8ff0\u7c7b\u9898\u76ee</li> <li>\u62a5\u544a\u7c7b\u9898\u76ee</li> <li>\u6df7\u5408\u7c7b\u9898\u76ee</li> </ul> </li> </ul> </li> </ul> </li> </ul>"},{"location":"Languages/English/IELTS_Notes/#grammar","title":"Grammar","text":"<p>Form &amp; Meaning: \u7528\u51c6\u786e\u7684Form\u4f20\u8fbe\u60f3\u8981\u4f20\u8fbe\u7684Meaning\u3002</p> <p></p>"},{"location":"Languages/English/IELTS_Notes/#_1","title":"\u53e5\u5b50\u7ed3\u6784","text":""},{"location":"Languages/English/IELTS_Notes/#_2","title":"\u82f1\u8bed\u53e5\u5b50\u6838\u5fc3\u539f\u5219","text":"<p>\u82f1\u8bed\u7684\u6700\u5c0f\u5355\u4f4d\u662f==\u53e5\u5b50==\uff0c\u4e00\u4e2a\u53e5\u53f7\uff08or\u5206\u53f7\uff09\u91cc\u9762\uff0c==\u6709\u4e14\u4ec5\u6709==<code>\u4e00\u4e2a\u4e3b\u8bed+\u4e00\u4e2a\u8c13\u8bed\u52a8\u8bcd</code></p> <p>Tips\uff1a</p> <ol> <li>\u60c5\u6001\u52a8\u8bcd<code>can do sth</code>\uff0c\u5373\u8c13\u8bed\u90e8\u5206\u3002</li> <li>\u4e3b\u8bed\u548c\u8c13\u8bed\u4e4b\u95f4\u6ca1\u6709\u9017\u53f7\u3002</li> <li><code>Take sb as an example.</code>==\uff08\u7948\u4f7f\u53e5\uff09==\u662f\u4e00\u53e5\u8bdd</li> </ol>"},{"location":"Languages/English/IELTS_Notes/#_3","title":"\u5e73\u884c\u7ed3\u6784","text":"<p>\u5e76\u5217\u8fde\u8bcd\u8fde\u63a5\u4e24\u4e2a\u6216\u591a\u4e2a\u8bed\u6cd5\u7ed3\u6784\u76f8\u540c\u7684==\u6210\u5206==\u3002</p> <p>\u5e38\u89c1\u7684\u5e76\u5217\u8fde\u8bcd\u6709:</p> <ol> <li><code>and, but, or, so</code></li> <li><code>not...but..., not only...but also..., both...and..., neither...nor..., either...or...,...rather than..., ...as well as...</code></li> </ol> <ul> <li>\u591a\u7ec4\u5e73\u884c A, B and C,and D</li> </ul> <p>Key points:</p> <ol> <li>\u5e76\u5217\u8fde\u8bcd\u524d\u540e\u8bed\u6cd5\u7ed3\u6784\u9700\u8981\u4e00\u81f4\u3002</li> </ol> <p>\u5e76\u5217\u8fde\u8bcd\u4e0d\u80fd\u653e\u5728\u53e5\u9996\uff0c\u5728\u53e5\u4e2d\u7684\u4f4d\u7f6e\u9700\u6709\u524d\u6709\u540e\u3002</p> <ol> <li>\u526f\u8bcd\u548c\u4ecb\u8bcd\u77ed\u8bed==\uff08\u72b6\u8bed\uff09==\u4e0d\u662f\u5e76\u5217\u8fde\u8bcd\uff0c\u4e0d\u80fd\u5e76\u5217\u4e24\u4e2a\u6210\u5206\u3002</li> </ol> <p>==\u72b6\u8bed==\u53ef\u5220\u53bb&amp;\u4f4d\u7f6e\u968f\u610f\uff0c\u4e0d\u5f71\u54cd\u53e5\u5b50\u7684\u8bed\u6cd5\u7ed3\u6784\u3002</p> <ol> <li>\u5e76\u5217\u8fde\u8bcd\u540e\u9762\u4e0d\u80fd\u52a0\u9017\u53f7\uff0c\u524d\u9762\u53ef\u52a0\u53ef\u4e0d\u52a0\u3002</li> </ol>"},{"location":"Languages/English/IELTS_Notes/#_4","title":"\u6bd4\u8f83\u7ed3\u6784","text":"<p>\u5e38\u89c1\u7684\u6bd4\u8f83\u7ed3\u6784\uff1a</p> <ul> <li><code>more...than...</code></li> <li><code>compared with...</code></li> <li><code>like../unlike...</code></li> <li><code>in contrast to...</code></li> </ul> <p>Key points:</p> <ol> <li>\u6bd4\u8f83\u5bf9\u8c61\u8981\u4e00\u81f4</li> </ol> <p>\u4e00\u81f4\u610f\u5473\u7740\u6bd4\u8f83\u5bf9\u8c61\u7684\u5f62\u5f0f\u548c\u610f\u4e49\u9700\u8981\u5bf9\u7b49</p>"},{"location":"Languages/English/IELTS_Notes/#_5","title":"\u4e09\u5927\u4ece\u53e5","text":""},{"location":"Languages/English/IELTS_Notes/#_6","title":"\u4ece\u53e5\u603b\u8bba","text":"<p>\u5728\u4e3b\u53e5\u4e2d\uff0c\u672c\u8eab\u662f==xx\u8bcd\u6027==\u7684\u5730\u65b9\u51fa\u73b0\u4e00\u4e2a\u53e5\u5b50\uff0c\u5c31\u53eb==xx\u8bcd\u6027\u4ece\u53e5==</p> <p>Key points:</p> <ol> <li>\u4ece\u53e5\u4e0d\u80fd\u5355\u72ec\u6210\u53e5</li> <li>\u4e3b\u53e5\u662f\u5b8c\u6574\u7684\u53e5\u5b50\uff0c\u8981\u6709\u4e3b\u8bed\u548c\u8c13\u8bed</li> <li>\u4ece\u53e5\u4e5f\u662f\u5b8c\u6574\u7684\u53e5\u5b50\uff0c\u8981\u6709\u4e3b\u8bed\u548c\u8c13\u8bed</li> <li>\u5b9a\u8bed\u4ece\u53e5 adj. \u8fd8\u539f\u6307\u4ee3\uff0c\u4ece\u53e5\u5b8c\u6574</li> <li>\u540d\u8bcd\u6027\u4ece\u53e5 n. \u4ece\u53e5\u5b8c\u6574</li> <li>\u72b6\u8bed\u4ece\u53e5 adv. \u4ece\u53e5\u5b8c\u6574</li> </ol>"},{"location":"Languages/English/IELTS_Notes/#adj","title":"\u5b9a\u8bed\u4ece\u53e5 <code>adj.</code>","text":"<p>Key points:</p> <ol> <li> <p>\u8fd8\u539f==\u6307\u4ee3==\uff0c\u4ece\u53e5\u5b8c\u6574</p> </li> <li> <p>\u5173\u7cfb\u4ee3\u8bcd:<code>that/who/whom/which</code>=\u6307\u4ee3\u524d\u9762\u7684N</p> </li> <li> <p>\u5173\u7cfb\u526f\u8bcd:<code>when/where/why</code>=\u6307\u4ee3\u4ecb\u8bcd+\u524d\u9762\u7684N=<code>\u4ecb\u8bcd+which/whom</code></p> </li> <li> <p>\u5c3d\u91cf\u5c31\u8fd1\u6307\u4ee3\uff0c\u8d8a\u8fd1\u8d8a\u597d\uff0c\u907f\u514d\u6b67\u4e49</p> </li> </ol> <p>\u4e3a\u907f\u514d\u6b67\u4e49\uff0c<code>which</code>\u6700\u597d\u4e0d\u8981\u6307\u4ee3\u524d\u9762\u6574\u53e5\u8bdd\u3002\u60f3\u8981\u6307\u4ee3\u6574\u53e5\u8bdd\u4f18\u5148\u8003\u8651\u975e\u8c13\u8bed</p> <ol> <li>\u5b9a\u8bed\u4ece\u53e5\u524d\u52a0\u9017\u53f7\u4e3a\u975e\u9650\u5236\u6027\u5b9a\u4ece\uff0c\u4e0d\u52a0\u9017\u53f7\u4e3a\u9650\u5236\u6027\u5b9a\u4ece</li> </ol> <p>\u975e\u9650\u5236\u6027\u5b9a\u4ece\u987a\u7740\u7ffb\u8bd1\uff0c\u4ece\u53e5\u4e3a\u4e3b\u53e5\u7684\u8865\u5145\u4fe1\u606f\u3002</p> <p>\u9650\u5236\u6027\u5b9a\u4ece\u7ffb\u8bd1\u4e3a\u540e\u9762\u7684\u524d\u9762\uff0c\u4ece\u53e5\u662f\u9650\u5b9a\u4e3b\u53e5\u7684\u3002</p> <p>==Tip==\u8003\u8bd5\u65f6\u53ef\u4ee5\u628a\u9017\u53f7\u90fd\u52a0\u4e0a\uff08\u53e5\u5b50\u7ed3\u6784\u6e05\u6670\uff09</p>"},{"location":"Languages/English/IELTS_Notes/#n","title":"\u540d\u8bcd\u6027\u4ece\u53e5 <code>n.</code>","text":"<p>\u5305\u62ec<code>\u4e3b\u8bed\u4ece\u53e5\u3001\u8868\u8bed\u4ece\u53e5\u3001\u5bbe\u8bed\u4ece\u53e5\u3001\u540c\u4f4d\u8bed\u4ece\u53e5</code></p> <p>\u5f15\u5bfc\u540d\u8bcd\u6027\u4ece\u53e5\u7684\u8fde\u63a5\u8bcd\uff1a</p> <ul> <li>\u5728\u4ece\u53e5\u4e2d\u4e0d\u5145\u5f53\u6210\u5206:<code>that, whether</code></li> </ul> <p>\u4e0d\u5145\u5f53\u6210\u5206\u662f\u6307\u628a\u8fde\u63a5\u8bcd\uff08<code>that, whether</code>\uff09\u76d6\u6389\u4e0d\u770b\uff0c\u5269\u4e0b\u7684\u53e5\u5b50\u6709\u4e3b\u8c13</p> <ul> <li>\u5728\u4ece\u53e5\u4e2d\u5145\u5f53\u6210\u5206\uff1a<code>what</code><code>, who, whom, which, when, where, how</code></li> </ul> <p>Key points:</p> <ol> <li> <p>\u4ece\u53e5\u5b8c\u6574\uff0c\u8981\u6709\u4e3b\u8bed\u548c\u8c13\u8bed</p> </li> <li> <p>\u4ece\u53e5\u9700\u8981\u65f6\u9648\u8ff0\u8bed\u5e8f</p> </li> <li> <p><code>it</code>\u505a\u5f62\u5f0f\u4e3b\u8bed/\u5f62\u5f0f\u5bbe\u8bed ==\u5192\u5145native speaker\u7684\u52a0\u5206\u9879==</p> </li> <li> <p>\u5f62\u5f0f\u4e3b\u8bed\uff08\u907f\u514d\u5934X\u8fc7\u91cd)</p> <p>X + <code>is</code> + <code>n/adj.</code> :arrow_right: <code>It is</code> + <code>n/adj.</code> + X</p> <p>\u53ef\u4ee5\u628aX\u8fd8\u539f\u56de\u53bb\uff08\u66ff\u6362\u6389<code>it</code>\u68c0\u67e5\u53e5\u5b50\u662f\u5426\u6b63\u786e\u3002</p> <ol> <li>X\u662f\u7531\u8fde\u63a5\u8bcd<code>that</code>\u5f15\u5bfc\u7684\u4e3b\u8bed\u4ece\u53e5</li> <li>X\u662f<code>to do sth</code>\u4e0d\u5b9a\u5f0f</li> </ol> </li> <li> <p>\u5f62\u5f0f\u5bbe\u8bed\uff08\u907f\u514d\u4e2d\u95f4Y\u592a\u957f</p> <p><code>make, find, think, feel, consider, believe</code>+ Y + <code>adj/n.</code> :arrow_right: <code>make, find, think, feel, consider, believe</code> + <code>it</code> + <code>adj/n.</code> + Y</p> <ol> <li>Y\u662f\u8fde\u63a5\u8bcd<code>that</code>\u5f15\u5bfc\u7684\u5bbe\u8bed\u4ece\u53e5</li> <li>Y\u662f<code>(for sb) to do sth</code>\u4e0d\u5b9a\u5f0f</li> </ol> </li> </ol>"},{"location":"Languages/English/IELTS_Notes/#adv","title":"\u72b6\u8bed\u4ece\u53e5 <code>adv.</code>","text":"<p>==\u4ece\u5c5e\u8fde\u8bcd/\u8bcd\u7ec4== + \u4ece\u53e5\uff0c \u4e3b\u53e5</p> <p>\u4ece\u5c5e\u8fde\u8bcd\uff1a<code>When, After, Before, Because, If, Although, Even though</code></p> <p>\u8bcd\u7ec4\uff1a<code>,so that ...</code>, <code>so adj/adv. that ...</code>, <code>such n. that ...</code></p> <p>Key points:</p> <ol> <li> <p>\u4ece\u53e5\u5b8c\u6574\uff0c\u8981\u6709\u4e3b\u8bed\u548c\u8c13\u8bed</p> </li> <li> <p>\u72b6\u8bed\u4ece\u53e5\u7684\u7701\u7565</p> </li> <li> <p>\u7701\u7565\u6761\u4ef6\uff1a\u5f53\u4e3b\u53e5\u548c\u4ece\u53e5\u4e2d\u7684\u4e3b\u8bed\u4e00\u81f4\u65f6\uff0c\u53ef\u4ee5\u7701\u7565\u4ece\u53e5\u4e2d\u7684\u4e3b\u8bed\u548cbe\u52a8\u8bcd</p> </li> <li> <p>\u72b6\u8bed\u4ece\u53e5\u7701\u7565\u540e\u7684\u53e5\u5b50\u7ed3\u6784\uff1a ==\u4ece\u5c5e\u8fde\u8bcd== + ving/ved, ==\u4e3b\u8bed== + ==\u8c13\u8bed==</p> <p>\u4e3b\u8bed\u4e0ev\u4e3a\u4e3b\u52a8\u5173\u7cfb\u65f6\u7528<code>ing</code>\u5f62\u5f0f\uff0c\u88ab\u52a8\u5173\u7cfb\u65f6\u7528<code>ed</code>\u5f62\u5f0f</p> </li> </ol>"},{"location":"Languages/English/IELTS_Notes/#_7","title":"\u5c0f\u7ed3\u6784\uff08\u53ef\u4ee5\u548c\u4ece\u53e5\u4e92\u6362\uff09","text":"<ul> <li>\u975e\u8c13\u8bed\u52a8\u8bcd &lt;-&gt; <code>\u72b6\u4ece/\u5b9a\u4ece</code></li> <li>\u4ecb\u8bcd\u77ed\u8bed &lt;-&gt; <code>\u72b6\u4ece</code></li> <li>\u5f62\u5bb9\u8bcd\u77ed\u8bed\u4f5c\u540e\u7f6e\u5b9a\u8bed &lt;-&gt; <code>\u5b9a\u4ece</code></li> <li>\u540c\u4f4d\u8bed &lt;-&gt; <code>\u540c\u4f4d\u8bed\u4ece\u53e5</code></li> <li>\u63d2\u5165\u8bed &lt;-&gt; <code>__</code></li> </ul>"},{"location":"Languages/English/IELTS_Notes/#_8","title":"\u975e\u8c13\u8bed\u52a8\u8bcd","text":"<p>\u8c13\u8bed\u52a8\u8bcd</p> <ul> <li>be\u52a8\u8bcd <code>am/is are</code></li> <li>\u65f6\u6001\u53d8\u5316 <code>do/does/did</code></li> <li>\u88ab\u52a8 <code>be done</code></li> <li>\u60c5\u6001\u52a8\u8bcd <code>can do</code></li> </ul> <p>\u975e\u8c13\u8bed\u52a8\u8bcd</p> <ul> <li> <p>\u4e0d\u5b9a\u5f0f <code>to do</code></p> </li> <li> <p>\u52a8\u540d\u8bcd <code>doing</code> = <code>n.</code></p> </li> <li> <p>:star:\u5206\u8bcd\u5f62\u5f0f <code>doing/done</code></p> </li> <li> <p><code>doing</code>\u8868\u793a\u4e3b\u52a8\u3001\u6b63\u5728\u88ab</p> </li> <li><code>done</code>\u8868\u793a\u88ab\u52a8</li> </ul> <p>Key points:</p> <p>\u5206\u8bcd\u53ef\u4ee5\u4f5c\u5b9a\u8bed\u548c\u72b6\u8bed</p> <ol> <li> <p>\u5206\u8bcd\u4f5c\u5b9a\u8bed (\u4fee\u9970\u8c01\u653e\u5728\u8c01\u540e\u9762)</p> </li> <li> <p>\u53ef\u4ee5\u548c\u5b9a\u8bed\u4ece\u53e5\u4e92\u6362\uff0c\u653e\u5728\u4efb\u610f\u540d\u8bcd\u540e\u5bf9\u5176\u4fee\u9970\uff0c\u5373 <code>n.</code> + <code>doing/done</code></p> </li> <li> <p>\u5206\u8bcd\u4f5c\u72b6\u8bed \uff08\u4f4d\u4e8e\u53e5\u9996\u4f34\u968f\u4e3b\u8bed\uff1b\u4f4d\u4e8e\u53e5\u672b\u4f34\u968f\u4e3b\u8bed\u6216\u8005\u4f5c\u4e3a\u6574\u4e2a\u53e5\u5b50\u7684\u7ed3\u679c\uff0c\u53e5\u672b\u53ea\u6709<code>doing</code>\u5f62\u5f0f\uff09</p> </li> </ol> <p>\u548c\u72b6\u8bed\u4ece\u53e5\u4e92\u6362\uff0c\u653e\u5728\u53e5\u9996\u6216\u53e5\u672b\uff0c\u5373</p> <ol> <li> <p><code>Doing/Done</code>, SVO.\uff08\u4f34\u968f\u72b6\u8bed\uff09</p> <p>SVO represents a sentence, which means <code>Subject + Verb + Object</code></p> <p>\u5206\u8bcd\u4f5c\u4f34\u968f\u72b6\u8bed\uff0c\u662f\u4f34\u968f\u4e3b\u8bed\u7684\u52a8\u4f5c\uff0c\u5206\u8bcd\u52a8\u4f5c\u5148\uff0c\u8c13\u8bed\u52a8\u4f5c\u540e\u3002 Doing/Done, SVO.</p> <ul> <li>\u5206\u8bcd\u91c7\u7528<code>Being done</code>\u5f62\u5f0f\u65f6\u5f3a\u8c03<code>\u6b63\u5728\u88ab</code></li> </ul> </li> <li> <p>SVO, doing. (\u4f34\u968f\u72b6\u8bed\u3001\u7ed3\u679c\u72b6\u8bed)</p> <ul> <li> <p>\u5206\u8bcd\u4f5c\u4f34\u968f\u72b6\u8bed\uff0c\u662f\u4f34\u968f\u4e3b\u8bed\u7684\u52a8\u4f5c\uff0c\u8c13\u8bed\u52a8\u4f5c\u5148\uff0c\u5206\u8bcd\u52a8\u4f5c\u540e</p> <p>e.g. Students enter the classroom, carrying their books.</p> </li> <li> <p>:star:\u5206\u8bcd\u4f5c\u7ed3\u679c\u72b6\u8bed\uff0c\u662f\u6574\u4e2a\u53e5\u5b50\u7684\u7ed3\u679c</p> <p>e.g. Students enter the classroom, surprising their teacher.</p> </li> </ul> </li> </ol>"},{"location":"Languages/English/IELTS_Notes/#_9","title":"\u4ecb\u8bcd\u77ed\u8bed","text":"<p>\u7ed3\u6784 <code>prep.</code> + <code>n.</code></p> <p>\u7279\u6b8a <code>with/without</code>\u7684\u590d\u5408\u7ed3\u6784</p> <p>Key points:</p> <ol> <li>\u4ecb\u8bcd + <code>doing</code>\uff0c\u6ce8\u610f\u5206\u8bcd\u7684\u903b\u8f91\u4e3b\u8bed \uff08<code>prep.</code> + doing, S V O.)</li> </ol> <p>e.g. By communicating effciently with others, employees can win support and assistance.</p> <ol> <li>\u4ecb\u8bcd\u540e\u9762\u53ea\u80fd\u52a0\u540d\u8bcd \uff08<code>prep.</code> + <code>n.</code>; \u4e0d\u80fd\u662f<code>prep.</code> + <code>\u53e5\u5b50</code>)</li> </ol> <p>e.g. because of diligence; because of the fact that \u4ece\u53e5 \uff08\u6b64\u5904\u4ece\u53e5\u4e0e\u5176\u6307\u4ee3\u7684\u540d\u8bcd\u4e3a\u5e76\u5217\u5173\u7cfb\uff0c\u4e3a\u540c\u4f4d\u8bed\u4ece\u53e5\uff09</p> <ol> <li><code>with/without</code>\u590d\u5408\u7ed3\u6784\uff0c\u4e0d\u8003\u8651\u903b\u8f91\u4e3b\u8bed</li> </ol> <p>\u7ed3\u6784\uff1a</p> <p><code>with/without</code> + <code>\u5bbe\u8bed</code> + <code>\u526f\u8bcd/\u540d\u8bcd/\u5f62\u5bb9\u8bcd/\u4ecb\u8bcd\u77ed\u8bed/doing(\u4e3b\u52a8\uff0c\u6b63\u5728\u88ab)/done(\u88ab\u52a8)/to do(\u5c06\u8981\u53d1\u751f)</code>\uff0c \u4e3b \u8c13. </p> <p>&lt;=&gt;</p> <p><code>with/without</code> + ==\u72ec\u7acb\u4e3b\u683c==\uff0c \u4e3b \u8c13.</p> <p>:warning:\u5728<code>with/without</code>\u590d\u5408\u7ed3\u6784\u4e2d\uff0c<code>\u5bbe\u8bed</code>\u548c\u540e\u9762\u7684\u903b\u8f91\u4e3b\u8bed\u5e38\u5e38\u4e0d\u540c\u3002 \uff08\u8be5\u7ed3\u6784\u9f13\u52b1\u7a81\u51fa\u8fd9\u79cd\u4e0d\u540c\uff09</p> <p>e.g. With a <code>supermarket</code> in the vicinity, people can purchase food and other necessities conveniently.</p>"},{"location":"Languages/English/IELTS_Notes/#_10","title":"\u5f62\u5bb9\u8bcd\u77ed\u8bed\u4f5c\u540e\u7f6e\u5b9a\u8bed","text":"<p><code>n.</code> + <code>adj\u77ed\u8bed</code></p> <p><code>adj\u77ed\u8bed</code>\u4f5c\u540e\u7f6e\u5b9a\u8bed\uff0c\u4fee\u9970\u524d\u9762\u7684<code>n.</code>\uff0c\u53ef\u4ee5\u548c\u5b9a\u8bed\u4ece\u53e5\u4e92\u6362\uff0c\u4e00\u822c\u4e0d\u52a0\u9017\u53f7\uff08\u52a0\u4e0a\u4e0d\u7b97\u9519\uff09\u3002</p> <p>e.g. People lack the expertise applicable in community acticities.</p>"},{"location":"Languages/English/IELTS_Notes/#_11","title":"\u540c\u4f4d\u8bed","text":"<p><code>n1, n2</code> \u5176\u4e2d\uff0c<code>n2</code>\u53eb\u505a\u540c\u4f4d\u8bed\u3002<code>n2</code>\u53ef\u4ee5\u6362\u505a\u540c\u4f4d\u8bed\u6216\u8005\u5b9a\u8bed\u4ece\u53e5\u3002</p> <p>e.g. Zhejiang university, a well-known school, attracts high school graduates.</p> <p>&lt;=&gt; Zhejiang university, which is a well-known school, attracts high school graduates.</p>"},{"location":"Languages/English/IELTS_Notes/#_12","title":"\u63d2\u5165\u8bed","text":"<ol> <li> <p>\u6e38\u79bb\u5728\u53e5\u5b50\u8bed\u6cd5\u4e4b\u5916\u7684\u7ed3\u6784\uff0c\u5bf9\u53e5\u4e2d\u610f\u601d\u4f5c\u8865\u5145\u8bf4\u660e\uff0c\u5220\u53bb\u4e4b\u540e\u53e5\u5b50\u8bed\u6cd5\u4ecd\u7136\u6b63\u786e</p> </li> <li> <p>\u7ed3\u6784\u5f62\u5f0f\u591a\u6837\uff0c\u6240\u6709\u7684\u4ece\u53e5\u4ee5\u53ca\u5c0f\u7ed3\u6784\u90fd\u53ef\u4ee5\u63d2\u5728\u53e5\u5b50\u4e2d\u95f4\uff0c\u4f5c\u63d2\u5165\u8bed\u3002</p> </li> <li> <p>\u63d2\u5165\u8bed\u7684\u4f4d\u7f6e\uff1a</p> </li> <li> <p>\u672c\u8eab\u653e\u5728\u4e3b\u8bed\u524d\u9762\u7684\u7ed3\u6784\uff0c\u63d2\u5728\u4e86\u4e3b\u8bed\u7684\u540e\u9762</p> <p>e.g.After graduating from BNU, she became a teacher.</p> <p>\u200b  She, after graduating from BNU, became a teacher.</p> </li> <li> <p>\u8865\u5145\u5728\u4efb\u4f55\u7a81\u7136\u60f3\u89e3\u91ca\u8bf4\u660e\u7684\u90e8\u5206\u540e\u9762\uff08\u9605\u8bfb\u4e2d\u8f83\u591a\uff0c\u548c\u5e73\u5e38\u5b66\u7684\u975e\u8c13\u8bed\u3001\u4ece\u53e5\u7b49\u4e00\u6837\uff09</p> <p>e.g. People, especially with dissension, should communicate with teachers.</p> </li> </ol>"},{"location":"Languages/English/IELTS_Notes/#_13","title":"\u5c0f\u8bcd\u5927\u7528","text":"<p>:warning:\u5199\u4f5c\u8fc7\u7a0b\u4e2d\u63d0\u9ad8\u8bcd\u6027\u610f\u8bc6</p>"},{"location":"Languages/English/IELTS_Notes/#_14","title":"\u52a8\u8bcd","text":"<p>Key points:</p> <ol> <li>\u65f6\u6001</li> </ol> <p>\u65f6\u6001\u7684\u6838\u5fc3\u662f\u5177\u4f53\u8fd9\u4e2a\u52a8\u4f5c\u662f\u4ec0\u4e48\u65f6\u5019\u5c31\u7528\u4ec0\u4e48\u65f6\u6001\uff0c\u4e00\u4e2a\u53e5\u5b50\u7684\u65f6\u6001\u53ef\u4ee5\u4e0d\u540c\u3002</p> <p>e.g. She said the sun rises from the east.</p> <ul> <li> <p>\u72ec\u7acb\u5199\u4f5c\u65f6\u6001</p> <ul> <li>\u903b\u8f91\u63a8\u7406\u90e8\u5206\u90fd\u662f\u73b0\u5728\u65f6\u6001\uff0c\u5c06\u6765\u65f6\u6001\u3002</li> </ul> <p>==\u73b0\u5728\u65f6\u6001==\u5305\u62ec\uff1a \u4e00\u822c\u73b0\u5728\u65f6<code>do/does</code>\uff0c\u73b0\u5728\u5b8c\u6210\u65f6<code>has/have done</code>\uff0c\u73b0\u5728\u8fdb\u884c\u65f6<code>is/are doing</code>\uff0c\u73b0\u5728\u5b8c\u6210\u8fdb\u884c\u65f6<code>has/have been doing</code></p> <p>==\u5c06\u6765\u65f6\u6001==\u5305\u62ec: \u4e00\u822c\u5c06\u6765\u65f6 <code>will do</code></p> <ul> <li>\u4f8b\u5b50\u90e8\u5206\uff1a \u770b\u5177\u4f53\u7684\u52a8\u4f5c</li> </ul> <p>e.g. Take a student graduating from Peking University as an example, who once worked hard in college and was offered a position in Google. In the workplace, he maintains a strong relationship with others.</p> </li> <li> <p>\u7efc\u5408\u5199\u4f5c\u65f6\u6001</p> <ul> <li>\u6a21\u7248\uff1a\u7edf\u4e00\u65f6\u6001\uff0c\u63a8\u8350\u7edf\u4e00\u7528\u4e00\u822c\u73b0\u5728\u65f6</li> <li>\u586b\u5165\uff1a\u6839\u636e\u63cf\u8ff0\u5bf9\u8c61\uff0c\u548c\u9605\u8bfb\u4fdd\u6301\u4e00\u81f4</li> </ul> </li> <li> <p>\u4e3b\u8c13\u4e00\u81f4</p> </li> </ul> <p>\u770b\u6e05\u4e3b\u8bed\u662f\u8c01\uff0c\u4e3b\u8bed\u7684\u5355\u590d\u6570</p> <ol> <li> <p>\u4ec5\u6709\u4e00\u4e2a<code>un./to do/doing</code>\u65f6\uff0c\u8c13\u8bed\u5355\u6570\uff1b\u4f46\u7528<code>and</code>\u5e76\u5217\u65f6\uff0c\u8c13\u8bed\u590d\u6570</p> <p>e.g. Tea is my farite drink.</p> <p>\u200b  Tea and milk are my favorite drinks.</p> </li> <li> <p>\u5c31\u8fd1\u539f\u5219</p> <p>\u591a\u4e2a\u5e76\u5217\u540d\u8bcd\u4e2d\uff0c\u5355\u590d\u6570\u901a\u8fc7\u6700\u540e\u4e00\u4e2a\u786e\u5b9a </p> <p><code>not only A but also</code> B; <code>neither A nor</code> B; <code>either A or</code> B; <code>A or</code> B</p> <p>e.g. Neither students nor their teacher is tired.</p> </li> <li> <p>\u533a\u522b\u526f\u8bcd\u548c\u60c5\u6001\u52a8\u8bcd</p> <p>:warning:\u526f\u8bcd\u5220\u53bb\u5224\u65ad\u52a8\u8bcd\u5355\u590d\u6570\uff0c\u60c5\u6001\u52a8\u8bcd\u540e\u9762\u90fd\u662f\u539f\u578b</p> </li> </ol>"},{"location":"Languages/English/IELTS_Notes/#_15","title":"\u540d\u8bcd","text":"<p>Key points:</p> <ol> <li> <p><code>cn.</code> or <code>un.</code>?</p> </li> <li> <p>\u4efb\u4f55\u540d\u8bcd\u90fd\u6709\u590d\u6570\u5f62\u5f0f\uff0c\u9700\u8981\u6839\u636e\u5176\u5728\u53e5\u4e2d\u7684\u5177\u4f53\u610f\u601d\u5224\u65ad\uff0c\u5982<code>waters/equipments/peoples</code></p> </li> <li> <p>\u6839\u636e\u610f\u601d\uff0c\u53ef\u4ee5\u4e00\u4e2a\u4e00\u4e2a\u6570\u7684\u5c31\u53ef\u4ee5\u91c7\u7528\u590d\u6570\u5f62\u5f0f\uff0c\u5426\u5219\u4e0d\u91c7\u7528</p> </li> <li> <p>\u540d\u8bcd\u7684\u5b58\u5728\u5f62\u5f0f</p> </li> <li> <p>\u53ef\u6570\u540d\u8bcd\u7684\u5b58\u5728\u5f62\u5f0f\uff08<code>cn.</code>\u4e0d\u5355\u72ec\u88f8\u5954\uff0c\u9700\u8981\u52a0\u590d\u6570/\u51a0\u8bcd/\u7269\u4e3b\u4ee3\u8bcd\uff09</p> <p><code>cns</code>, <code>the cns</code>, <code>the/a/an cn</code>, <code>his/Tom's cn/cns</code></p> </li> <li> <p>\u4e0d\u53ef\u6570\u540d\u8bcd\u7684\u5b58\u5728\u5f62\u5f0f</p> <p><code>un</code>, <code>the un</code></p> </li> </ol>"},{"location":"Languages/English/IELTS_Notes/#_16","title":"\u51a0\u8bcd","text":"<p>Key points:</p> <ol> <li> <p>\u4ec0\u4e48\u65f6\u5019\u52a0<code>the</code>\uff1f</p> </li> <li> <p>\u524d\u6587\u6709\u63d0\u5230\u8fc7</p> <p>e.g. She bought a computer. I like the computer.</p> </li> <li> <p>\u8be5\u540d\u8bcd\u540e\u9762\u6709\u4fee\u9970</p> <p>e.g. The computer she bought was cheap.</p> <p>\u200b   The computers in the store are cheap.</p> </li> </ol> <p>==Tip==\u5b66\u4f1a\u7528\u4ee3\u8bcd\uff0c\u907f\u514d\u7ea0\u7ed3\u662f\u5426\u52a0<code>the</code></p> <p>\u200b    e.g. Neither students nor their teacher is tired.</p>"},{"location":"Languages/English/IELTS_Notes/#_17","title":"\u4ee3\u8bcd","text":"<p>Key points:</p> <ol> <li>\u4ee3\u8bcd\u662f\u6307\u4ee3\u540d\u8bcd\u7684\u8bcd\uff0c\u5c3d\u91cf\u4e0d\u6307\u4ee3\u53e5\u5b50\u3002</li> </ol> <p>e.g.<code>Original:</code> Students suffer from much pressure and it worries parents.</p> <p>\u200b    <code>Revision:</code> Students suffer from much pressure, worrying their parents.</p> <ol> <li>\u76f8\u540c\u7684\u4ee3\u8bcd\u6307\u4ee3\u76f8\u540c\u7684\u5bf9\u8c61</li> </ol> <p>e.g. <code>Original:</code> Teachers impart knowledge to students and ==they== are best their best friends.</p> <p>\u200b     <code>Revision:</code> Teachers, who impart knowledge to students, are their best friends.</p> <ol> <li>\u4ee3\u8bcd\u80fd\u8ba9\u6587\u7ae0\u8bed\u8a00\u548c\u903b\u8f91\u66f4\u7d27\u51d1</li> </ol> <p>e.g. <code>Original:</code> Education triggers heated discussions over the methods of improving the quality.</p> <p>\u200b     <code>Revision:</code> Ecucation triggers heated discussions over the methods of improving its quality.</p>"},{"location":"Languages/English/IELTS_Notes/#_18","title":"\u4ecb\u8bcd","text":"<p>Key points:</p> <p>\u4ecb\u8bcd\u80fd\u8868\u660e\u4e24\u4e2a\u4e8b\u7269\u4e4b\u95f4\u7684\u660e\u786e\u5173\u7cfb\uff0c\u5373<code>B prep. A</code> &gt; <code>AB</code></p> <p>e.g. the quality of education &gt; the education quality (\u4e0d\u591f\u597d\uff0c\u4e5f\u4e0d\u5bf9)"},{"location":"Languages/English/IELTS_Notes/#_19","title":"\u5f62\u5bb9\u8bcd\u3001\u526f\u8bcd","text":"<p><code>adj.</code>\u4fee\u9970<code>n.</code>\uff1b <code>adv.</code>\u4fee\u9970<code>adj./adv./verb./\u4e00\u4e2a\u53e5\u5b50</code></p> <p>Key points:</p> <ol> <li><code>adj.</code>\u548c<code>adv.</code>\u7684\u6bd4\u8f83\u7ea7</li> </ol> <p><code>more</code>\u65e2\u662f<code>adj.</code>\u53c8\u662f<code>adv.</code></p> <p>e.g. She does more exercise. (\u63d0\u524dthe more\u7684\u90e8\u5206)</p> <ul> <li> <p>more\u4f5c\u4e3a<code>adj.</code>\uff0cthe more\u7684\u90e8\u5206\u5373<code>the more + n.</code></p> <p>The more exercise she does,</p> </li> <li> <p>more\u4f5c\u4e3a<code>adv.</code>\u4fee\u9970<code>verb.</code>\uff0cthe more\u7684\u90e8\u5206\u5373<code>the more</code></p> <p>The more she does exercise,</p> </li> </ul>"},{"location":"Languages/English/IELTS_Notes/#_20","title":"\u7279\u6b8a\u53e5\u5f0f","text":"<p>==\u5168\u90e8\u5012\u88c5\uff1a==\u628a\u8c13\u8bed\uff08\u542b<code>be</code>\uff09\u5168\u90e8\u653e\u5728\u4e3b\u8bed\u4e4b\u524d</p> <p>==\u90e8\u5206\u5012\u88c5\uff1a==\u53ea\u628a\u52a9\u52a8\u8bcd\u6216\u60c5\u6001\u52a8\u8bcd\u653e\u5728\u4e3b\u8bed\u4e4b\u524d</p> <ul> <li>\u5e38\u89c1\u7684\u52a9\u52a8\u8bcd\uff1a<code>be, have, has, do, does, did</code></li> </ul> <p><code>be</code>\u88ab\u8ba4\u4e3a\u662f\u7cfb\u52a8\u8bcd\uff0c\u4e5f\u5c31\u662f\u8c13\u8bed\uff1b\u4e5f\u88ab\u8ba4\u4e3a\u662f\u52a9\u52a8\u8bcd\uff0c\u4e0d\u7ba1\u534a\u5012\u88c5\u8fd8\u662f\u5168\u90e8\u5012\u88c5\uff0c\u90fd\u628a<code>be</code>\u653e\u5728\u524d\u9762</p> <ul> <li>\u5e38\u89c1\u7684\u60c5\u6001\u52a8\u8bcd\uff1a <code>will, should, shall, must, would</code></li> </ul> <p>\u5199\u4f5c\u5e38\u7528\u5012\u88c5\uff1a</p> <ol> <li>\u5168\u5012\u88c5\uff1a</li> </ol> <p><code>Among Ns is xx.</code>\u4e00\u822c\u7528\u4e8e\u4e3e\u4f8b</p> <ol> <li> <p>\u534a\u5012\u88c5\uff1a</p> </li> <li> <p><code>\u5426\u5b9a\u8bcd\u653e\u5728\u53e5\u9996</code> + <code>\u534a\u5012\u88c5</code></p> <p>\u7279\u6b8a\uff1a <code>Not only</code> + <code>\u534a\u5012\u88c5</code>, <code>but also</code> + <code>\u6b63\u5e38\u5b8c\u6574</code></p> </li> <li> <p><code>Only</code> + <code>\u4ecb\u8bcd\u77ed\u8bed</code> + <code>\u534a\u5012\u88c5</code></p> </li> <li> <p><code>So/such</code> + <code>\u534a\u5012\u88c5</code> + <code>that</code> + <code>\u6b63\u5e38\u5b8c\u6574</code></p> </li> </ol>"},{"location":"Languages/English/IELTS_Notes/#_21","title":"\u5168\u90e8\u5012\u88c5","text":"<p>\u8868\u793a\u5730\u70b9\u7684\u72b6\u8bed\u7f6e\u4e8e\u53e5\u9996\uff0c\u53e5\u5b50\u53d1\u751f\u5168\u5012\u88c5</p> <p>e.g. In front of computers sits a student.</p> <p>\u200b    Among overwhelmed students is Tom</p>"},{"location":"Languages/English/IELTS_Notes/#_22","title":"\u90e8\u5206\u5012\u88c5","text":"<p>Key points:</p> <ol> <li>\u5426\u5b9a\u526f\u8bcd/\u4ecb\u8bcd\u77ed\u8bed\u653e\u5728\u53e5\u9996 + \u534a\u5012\u88c5</li> </ol> <p>==\u5e38\u89c1\u7684\u5426\u5b9a\u526f\u8bcd/\u4ecb\u8bcd\u77ed\u8bed\uff1a== never, seldom, rarely, barely, scarcely, hardly, little, neither, nor, under no cirtumstances, on no occasion, in no case</p> <p>e.g. Never should we ignore the significance of education.</p> <p>\u200b    Under no circumstances can people be excluded from junk food. <p>\u7279\u6b8a\u7684\u5426\u5b9a\u8bcd\uff1a <code>Not only</code> + <code>\u534a\u5012\u88c5</code>, <code>but also</code> + <code>\u6b63\u5e38\u5b8c\u6574</code></p> <p>e.g. Not only will they hone their skills, but (also) they can cultivate their personalities.</p> <p>:warning: <code>...not only...but also...</code>\u7ed3\u6784\u4e0d\u5012\u88c5\u65f6\uff0c\u540e\u534a\u90e8\u5206\u4e3b\u8bed\u53ef\u4ee5\u7701\u53bb\uff1b<code>Not only</code>\u653e\u5728\u53e5\u9996\u65f6\uff0c\u53e5\u5b50\u5fc5\u987b\u534a\u5012\u88c5\uff0c\u5012\u88c5\u65f6\uff0c<code>but also</code>\u540e\u9762\u4e3b\u8bed\u4e0d\u53ef\u7701\u53bb</p> <ol> <li>Only + \u72b6\u8bed + \u534a\u5012\u88c5</li> </ol> <p><code>Only</code> + <code>\u526f\u8bcd/\u4ecb\u8bcd\u77ed\u8bed/\u72b6\u8bed\u4ece\u53e5</code> + \u534a\u5012\u88c5</p> <ol> <li><code>so/such ... that</code>\u7ed3\u6784\u4e2d<code>So/Such</code>\u653e\u5728\u53e5\u9996\uff0c\u53d1\u751f\u534a\u5012\u88c5</li> </ol> <p>\u6b63\u5e38\u7684<code>so/such ... that</code>\u7ed3\u6784\u4e3a\uff1a</p> <ul> <li>\u4e3b \u8c13 <code>so</code> + <code>adj./adv.</code> + that\u7ed3\u679c\u72b6\u8bed\u4ece\u53e5</li> <li>\u4e3b \u8c13 <code>such</code> + <code>n.</code> + that\u7ed3\u679c\u72b6\u8bed\u4ece\u53e5</li> </ul> <p>e.g.1 <code>Original:</code> He is so honest that his subordinates admires him.</p> <p>\u200b      <code>Revision:</code> So honest is he that his subordinates admires him.</p> <p>e.g.2 <code>Original:</code> He works so hard that he wins the competition.</p> <p>\u200b      <code>Revision:</code> So hard does he works that he wins the competition.</p>"},{"location":"Languages/English/IELTS_Notes/#writing","title":"Writing","text":""},{"location":"Languages/English/IELTS_Notes/#_23","title":"\u5c0f\u4f5c\u6587","text":""},{"location":"Languages/English/IELTS_Notes/#_24","title":"\u5927\u4f5c\u6587","text":"<p>\u6309\u7167\u9898\u76ee\u7684\u63d0\u95ee\u65b9\u5f0f\u53ef\u4ee5\u5206\u4e3a<code>\u4f18\u7f3a\u70b9\u7c7b\u9898\u76ee\u3001\u89c2\u70b9\u7c7b\u9898\u76ee\u3001\u8bba\u8ff0\u7c7b\u9898\u76ee\u3001\u62a5\u544a\u7c7b\u9898\u76ee\u3001\u6df7\u5408\u7c7b\u9898\u76ee</code></p> <p>ABC\u6cd5\uff08\u5927\u4f5c\u6587\u9898\u76ee\u7684\u7c7b\u578b\u548c\u5ba1\u9898\uff09\uff1aA(\u52a8\u4f5c\u6216\u4e8b\u4ef6) B(\u52a8\u4f5c\u6216\u4e8b\u4ef6 or \u7ed3\u679c) C(\u7ed3\u679c)</p> <ul> <li>\u7528\u4e8e\u5ba1\u9898</li> </ul> <p>\u7c7b\u578b A-&gt;B-&gt;C, A&lt;-B&lt;-C, A-&gt;B&lt;-C</p> <ul> <li>\u7528\u4e8e\u5199\u4e3b\u9898\u6bb5\u4e2d\u5fc3\u53e5</li> </ul> <p>\u6b65\u9aa4\uff1a<code>\u5ba1\u9898\u2192\u60f3\u89c2\u70b9\u548c\u5199\u89c2\u70b9\u2192\u5f00\u5934\u6bb5\u2192\u4e3b\u4f53\u6bb5\u2192\u7ed3\u5c3e\u6bb5</code></p> <p>\u5ba1\u9898\uff1a\u5206\u6790\u9898\u76ee\u4e3b\u8981\u8ba8\u8bba\u8bae\u9898\uff0c\u786e\u5b9a\u9898\u76ee\u5173\u952e\u8bcd</p> <p>\u60f3\u89c2\u70b9\u548c\u5199\u89c2\u70b9\uff1a\u89c2\u70b9\u7d27\u6263\u9898\u76ee\u91cd\u70b9\uff0c\u5199\u6700\u719f\u6089\u548c\u6613\u4e8e\u62d3\u5c55\u7684\u89c2\u70b9</p> <p>\u5f00\u5934\u6bb5\uff1a\u7b2c\u4e00\u53e5<code>\u6539\u5199\u9898\u76ee</code>\uff0c\u7b2c\u4e8c\u53e5<code>\u9610\u660e\u7acb\u573a\uff08\u6263\u9898+\u51fa\u73b0\u5173\u952e\u8bcd\uff09</code></p> <p>\u4e3b\u4f53\u6bb5\uff1a<code>\u4e3b\u4f53\u7b2c\u4e00\u6bb5</code>+<code>\u6298\u4e2d\u6bb5</code></p> <ul> <li>\u4e3b\u4f53\u7b2c\u4e00\u6bb5\uff1a<code>\u7d27\u6263\u5173\u952e\u8bcd\u7684\u4e2d\u5fc3\u53e5</code>+<code>\u5177\u4f53\u89e3\u91ca</code>+<code>\u4e3e\u4f8b\uff08\u8be6\u7ec6\uff09</code>+<code>(Optional)\u6263\u9898\u53e5</code></li> <li>\u6298\u4e2d\u6bb5\uff1a<code>\u4e2d\u5fc3\u53e5\uff08\u4e09\u8981\u7d20\uff1a\u89c2\u70b9\u3001\u9898\u76ee\u5173\u952e\u8bcd\u3001\u8fde\u63a5\u8bcd\uff09</code>+<code>\u4e2d\u5fc3\u53e5\u80cc\u666f</code>+<code>\u89e3\u51b3\u65b9\u6848+(Optional)\u4e3e\u4f8b\u7ec6\u5316</code>+<code>\u5bf9\u6bd4\u8bba\u8bc1(\u4e0e\u4e3b\u4f53\u7b2c\u4e00\u6bb5\u4e3b\u9898\u5bf9\u6bd4\u5206\u6790)</code></li> <li>\u7ed3\u5c3e\u6bb5(Optional)\uff1a\u603b\u7ed3\u4e3b\u8981\u7acb\u573a\uff0c\u4e0d\u51fa\u73b0\u65b0\u7684\u5185\u5bb9\uff0c\u4e0d\u91cd\u590d\u7528\u8bcd\u3002</li> </ul>"},{"location":"Languages/English/IELTS_Notes/#_25","title":"\u4f18\u7f3a\u70b9\u7c7b\u9898\u76ee","text":"<p>\u6807\u5fd7\u53e5\uff1a<code>Advantages and disadvantages?</code>/<code>Positive or negative development?</code></p>"},{"location":"Languages/English/IELTS_Notes/#_26","title":"\u89c2\u70b9\u7c7b\u9898\u76ee","text":"<p>\u6807\u5fd7\u53e5\uff1a<code>To what extent do you agree or disagree?</code></p>"},{"location":"Languages/English/IELTS_Notes/#_27","title":"\u8bba\u8ff0\u7c7b\u9898\u76ee","text":"<p>\u6807\u5fd7\u53e5\uff1a<code>Discuss both views and give your own opinion.</code></p>"},{"location":"Languages/English/IELTS_Notes/#_28","title":"\u62a5\u544a\u7c7b\u9898\u76ee","text":"<p>\u6807\u5fd7\u53e5\uff1a<code>Why is it happened? How to solve it?</code></p>"},{"location":"Languages/English/IELTS_Notes/#_29","title":"\u6df7\u5408\u7c7b\u9898\u76ee","text":"<p>\u7c7b\u578b\uff1a<code>\u539f\u56e0+\u5f71\u54cd+\u65b9\u6cd5</code></p>"}]}