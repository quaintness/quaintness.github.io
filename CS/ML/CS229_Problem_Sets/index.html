
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Record and Share">
      
      
        <meta name="author" content="quaintness">
      
      
        <link rel="canonical" href="https://quaintness.github.io/CS/ML/CS229_Problem_Sets/">
      
      
      
      
      <link rel="icon" href="../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.0, mkdocs-material-9.5.27">
    
    
      
        <title>PS 1 - Welcome to Wonderland</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.6543a935.min.css">
      
        
        <link rel="stylesheet" href="../../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../../mkdocs/css/no-footer.css">
    
      <link rel="stylesheet" href="../../../mkdocs/css/unordered-list-symbols.css">
    
    <script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="orange">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#ps-1" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../.." title="Welcome to Wonderland" class="md-header__button md-logo" aria-label="Welcome to Wonderland" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Welcome to Wonderland
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              PS 1
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="orange"  aria-label="切换至夜间模式"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="切换至夜间模式" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3 3.19.09m3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95 2.06.05m-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31Z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="indigo" data-md-color-accent="orange"  aria-label="切换至日间模式"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="切换至日间模式" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 7a5 5 0 0 1 5 5 5 5 0 0 1-5 5 5 5 0 0 1-5-5 5 5 0 0 1 5-5m0 2a3 3 0 0 0-3 3 3 3 0 0 0 3 3 3 3 0 0 0 3-3 3 3 0 0 0-3-3m0-7 2.39 3.42C13.65 5.15 12.84 5 12 5c-.84 0-1.65.15-2.39.42L12 2M3.34 7l4.16-.35A7.2 7.2 0 0 0 5.94 8.5c-.44.74-.69 1.5-.83 2.29L3.34 7m.02 10 1.76-3.77a7.131 7.131 0 0 0 2.38 4.14L3.36 17M20.65 7l-1.77 3.79a7.023 7.023 0 0 0-2.38-4.15l4.15.36m-.01 10-4.14.36c.59-.51 1.12-1.14 1.54-1.86.42-.73.69-1.5.83-2.29L20.64 17M12 22l-2.41-3.44c.74.27 1.55.44 2.41.44.82 0 1.63-.17 2.37-.44L12 22Z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var media,input,key,value,palette=__md_get("__palette");if(palette&&palette.color){"(prefers-color-scheme)"===palette.color.media&&(media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']"),palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent"));for([key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/quaintness/quaintness.github.io" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.5.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
  </div>
  <div class="md-source__repository">
    quaintness.github.io
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../Intro_CS/" class="md-tabs__link">
          
  
    
  
  Computer Science

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../Languages/English/Intro_Languages.md" class="md-tabs__link">
          
  
    
  
  Languages

        </a>
      </li>
    
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../.." title="Welcome to Wonderland" class="md-nav__button md-logo" aria-label="Welcome to Wonderland" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    Welcome to Wonderland
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/quaintness/quaintness.github.io" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.5.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
  </div>
  <div class="md-source__repository">
    quaintness.github.io
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    
    
      
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_1" >
        
          
          <label class="md-nav__link" for="__nav_1" id="__nav_1_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Computer Science
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_1">
            <span class="md-nav__icon md-icon"></span>
            Computer Science
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Intro_CS/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Intro CS
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_1_2" >
        
          
          <label class="md-nav__link" for="__nav_1_2" id="__nav_1_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Basics
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_1_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_1_2">
            <span class="md-nav__icon md-icon"></span>
            Basics
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Basics/6_0001/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Intro to CS (MIT 6.0001)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Basics/6_006/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Algorithm (MIT 6.006)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Basics/Computer-Networking-Lecture-CS144-Stanford/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Computer Networking
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Basics/Linux-tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Linux Tutorial
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_1_3" >
        
          
          <label class="md-nav__link" for="__nav_1_3" id="__nav_1_3_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Machine Learning
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_1_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_1_3">
            <span class="md-nav__icon md-icon"></span>
            Machine Learning
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../CS229/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    CS229
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../CS229_Problem_Set.md" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    CS229bPS
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../CS231n/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    CS231n
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Languages
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Languages
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Languages/English/Intro_Languages.md" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    None
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2_2" >
        
          
          <label class="md-nav__link" for="__nav_2_2" id="__nav_2_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    English
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_2">
            <span class="md-nav__icon md-icon"></span>
            English
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Languages/English/English_Grammar_Notes/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Grammar
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Languages/English/IELTS_Notes/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    IELTS Tips
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#problem-1-linear-classifiers-logistic-regression-and-gda" class="md-nav__link">
    <span class="md-ellipsis">
      Problem 1 Linear Classifiers (logistic regression and GDA)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Problem 1 Linear Classifiers (logistic regression and GDA)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#b-newtons-method" class="md-nav__link">
    <span class="md-ellipsis">
      b) Newton's Method
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#e-gda" class="md-nav__link">
    <span class="md-ellipsis">
      e) GDA
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gb-vs-e" class="md-nav__link">
    <span class="md-ellipsis">
      g）(b) vs. (e)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#h" class="md-nav__link">
    <span class="md-ellipsis">
      h)
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#problem-2-incomplete-positive-only-labels" class="md-nav__link">
    <span class="md-ellipsis">
      Problem 2 Incomplete, Positive-Only Labels
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  

  
    <a href="https://github.com/quaintness/quaintness.github.io/edit/main/docs/CS/ML/CS229_Problem_Sets.md" title="Edit this page" class="md-content__button md-icon">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4v-2m10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1 2.1 2.1Z"/></svg>
    </a>
  
  


<h1 id="ps-1">PS 1<a class="headerlink" href="#ps-1" title="Permanent link">&para;</a></h1>
<h2 id="problem-1-linear-classifiers-logistic-regression-and-gda">Problem 1 Linear Classifiers (logistic regression and GDA)<a class="headerlink" href="#problem-1-linear-classifiers-logistic-regression-and-gda" title="Permanent link">&para;</a></h2>
<h3 id="b-newtons-method">b) Newton's Method<a class="headerlink" href="#b-newtons-method" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">util</span>

<span class="kn">from</span> <span class="nn">linear_model</span> <span class="kn">import</span> <span class="n">LinearModel</span>


<span class="k">def</span> <span class="nf">main</span><span class="p">(</span><span class="n">train_path</span><span class="p">,</span> <span class="n">eval_path</span><span class="p">,</span> <span class="n">pred_path</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Problem 1(b): Logistic regression with Newton&#39;s Method.</span>

<span class="sd">    Args:</span>
<span class="sd">        train_path: Path to CSV file containing dataset for training.</span>
<span class="sd">        eval_path: Path to CSV file containing dataset for evaluation.</span>
<span class="sd">        pred_path: Path to save predictions.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span> <span class="o">=</span> <span class="n">util</span><span class="o">.</span><span class="n">load_dataset</span><span class="p">(</span><span class="n">train_path</span><span class="p">,</span> <span class="n">add_intercept</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="c1"># *** START CODE HERE ***</span>
    <span class="c1"># Train Logistic Regression</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">eps</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

    <span class="c1"># Plot data and decision boundary</span>
    <span class="n">util</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">theta</span><span class="p">,</span> <span class="s1">&#39;output/p01b.png&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">pred_path</span><span class="p">[</span><span class="o">-</span><span class="mi">5</span><span class="p">]))</span> <span class="c1">#TODO: Why -5?</span>

    <span class="c1"># Save Predictions</span>
    <span class="n">x_eval</span><span class="p">,</span> <span class="n">y_eval</span> <span class="o">=</span> <span class="n">util</span><span class="o">.</span><span class="n">load_dataset</span><span class="p">(</span><span class="n">eval_path</span><span class="p">,</span> <span class="n">add_intercept</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_eval</span><span class="p">)</span>
    <span class="n">np</span><span class="o">.</span><span class="n">savetxt</span><span class="p">(</span><span class="n">pred_path</span><span class="p">,</span> <span class="n">y_pred</span> <span class="o">&gt;</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">fmt</span><span class="o">=</span><span class="s1">&#39;</span><span class="si">%d</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="c1"># *** END CODE HERE ***</span>


<span class="k">class</span> <span class="nc">LogisticRegression</span><span class="p">(</span><span class="n">LinearModel</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Logistic regression with Newton&#39;s Method as the solver.</span>

<span class="sd">    Example usage:</span>
<span class="sd">        &gt; clf = LogisticRegression()</span>
<span class="sd">        &gt; clf.fit(x_train, y_train)</span>
<span class="sd">        &gt; clf.predict(x_eval)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Run Newton&#39;s Method to minimize J(theta) for logistic regression.</span>

<span class="sd">        Args:</span>
<span class="sd">            x: Training example inputs. Shape (m, n).</span>
<span class="sd">            y: Training example labels. Shape (m,).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># *** START CODE HERE ***</span>
        <span class="c1"># Initialization</span>
        <span class="n">m</span><span class="p">,</span> <span class="n">n</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">theta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
        <span class="c1"># Newton&#39;s method</span>
        <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
            <span class="c1"># save precious theta</span>
            <span class="n">old_theta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">theta</span><span class="p">)</span>
            <span class="c1"># Sigmoid function</span>
            <span class="n">h_x</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">theta</span><span class="p">)))</span>
            <span class="c1"># Calculate Hessians</span>
            <span class="n">H</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">T</span> <span class="o">*</span> <span class="n">h_x</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">h_x</span><span class="p">))</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">/</span> <span class="n">m</span>
            <span class="n">gradient_J_theta</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">h_x</span><span class="o">-</span><span class="n">y</span><span class="p">)</span> <span class="o">/</span> <span class="n">m</span>

            <span class="c1"># Update theta</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">theta</span> <span class="o">-=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">H</span><span class="p">)</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">gradient_J_theta</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">theta</span> <span class="o">-</span> <span class="n">old_theta</span><span class="p">,</span><span class="nb">ord</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">eps</span><span class="p">:</span>
                <span class="k">break</span>
        <span class="c1"># *** END CODE HERE ***</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Make a prediction given new inputs x.</span>

<span class="sd">        Args:</span>
<span class="sd">            x: Inputs of shape (m, n).</span>

<span class="sd">        Returns:</span>
<span class="sd">            Outputs of shape (m,).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># *** START CODE HERE ***</span>
        <span class="k">return</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">theta</span><span class="p">)))</span>
        <span class="c1"># *** END CODE HERE ***</span>
</code></pre></div>
<p><img src="https://gitee.com/violets/typora--images/raw/main/imgs/202203271928427.png" alt="p01b_1" style="zoom: 50%;" /></p>
<p><img src="https://gitee.com/violets/typora--images/raw/main/imgs/202203271928860.png" alt="p01b_2" style="zoom:50%;" /></p>
<h3 id="e-gda">e) GDA<a class="headerlink" href="#e-gda" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">util</span>

<span class="kn">from</span> <span class="nn">linear_model</span> <span class="kn">import</span> <span class="n">LinearModel</span>


<span class="k">def</span> <span class="nf">main</span><span class="p">(</span><span class="n">train_path</span><span class="p">,</span> <span class="n">eval_path</span><span class="p">,</span> <span class="n">pred_path</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Problem 1(e): Gaussian discriminant analysis (GDA)</span>

<span class="sd">    Args:</span>
<span class="sd">        train_path: Path to CSV file containing dataset for training.</span>
<span class="sd">        eval_path: Path to CSV file containing dataset for evaluation.</span>
<span class="sd">        pred_path: Path to save predictions.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Load dataset</span>
    <span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span> <span class="o">=</span> <span class="n">util</span><span class="o">.</span><span class="n">load_dataset</span><span class="p">(</span><span class="n">train_path</span><span class="p">,</span> <span class="n">add_intercept</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="c1"># *** START CODE HERE ***</span>
    <span class="c1"># Train Logistic Regression</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">GDA</span><span class="p">()</span>
    <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

    <span class="c1"># Plot data and decision boundary</span>
    <span class="n">util</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">theta</span><span class="p">,</span> <span class="s1">&#39;output/p01e_</span><span class="si">{}</span><span class="s1">.png&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">pred_path</span><span class="p">[</span><span class="o">-</span><span class="mi">5</span><span class="p">]))</span>

    <span class="c1"># Save Predictions</span>
    <span class="n">x_eval</span><span class="p">,</span> <span class="n">y_eval</span> <span class="o">=</span> <span class="n">util</span><span class="o">.</span><span class="n">load_dataset</span><span class="p">(</span><span class="n">eval_path</span><span class="p">,</span> <span class="n">add_intercept</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_eval</span><span class="p">)</span>
    <span class="n">np</span><span class="o">.</span><span class="n">savetxt</span><span class="p">(</span><span class="n">pred_path</span><span class="p">,</span> <span class="n">y_pred</span> <span class="o">&gt;</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">fmt</span><span class="o">=</span><span class="s1">&#39;</span><span class="si">%d</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="c1"># *** END CODE HERE ***</span>


<span class="k">class</span> <span class="nc">GDA</span><span class="p">(</span><span class="n">LinearModel</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Gaussian Discriminant Analysis.</span>

<span class="sd">    Example usage:</span>
<span class="sd">        &gt; clf = GDA()</span>
<span class="sd">        &gt; clf.fit(x_train, y_train)</span>
<span class="sd">        &gt; clf.predict(x_eval)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Fit a GDA model to training set given by x and y.</span>

<span class="sd">        Args:</span>
<span class="sd">            x: Training example inputs. Shape (m, n).</span>
<span class="sd">            y: Training example labels. Shape (m,).</span>

<span class="sd">        Returns:</span>
<span class="sd">            theta: GDA model parameters.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># *** START CODE HERE ***</span>
        <span class="c1"># Initialization</span>
        <span class="n">m</span><span class="p">,</span> <span class="n">n</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">theta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">sum_y_indicator</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

        <span class="c1"># GDA Parameters</span>
        <span class="n">phi</span> <span class="o">=</span> <span class="n">sum_y_indicator</span><span class="o">/</span><span class="n">m</span>
        <span class="n">mu_0</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">m</span> <span class="o">-</span> <span class="n">sum_y_indicator</span><span class="p">)</span>
        <span class="n">mu_1</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">/</span> <span class="n">sum_y_indicator</span> <span class="o">/</span> <span class="n">sum_y_indicator</span>
        <span class="n">sigma</span> <span class="o">=</span> <span class="p">((</span><span class="n">x</span><span class="p">[</span><span class="n">y</span> <span class="o">==</span> <span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">mu_0</span><span class="p">)</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">y</span> <span class="o">==</span> <span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">mu_0</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">y</span> <span class="o">==</span> <span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">mu_1</span><span class="p">)</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">y</span> <span class="o">==</span> <span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">mu_1</span><span class="p">))</span> <span class="o">/</span> <span class="n">m</span>
        <span class="n">sigma_inv</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">sigma</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">theta</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.5</span><span class="o">*</span><span class="p">(</span><span class="n">mu_0</span><span class="o">+</span><span class="n">mu_1</span><span class="p">)</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">sigma_inv</span><span class="p">)</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">mu_0</span><span class="o">-</span><span class="n">mu_1</span><span class="p">)</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">((</span><span class="mi">1</span> <span class="o">-</span> <span class="n">phi</span><span class="p">)</span> <span class="o">/</span> <span class="n">phi</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">theta</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span> <span class="o">=</span> <span class="n">sigma_inv</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">mu_1</span><span class="o">-</span><span class="n">mu_0</span><span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">theta</span>
        <span class="c1"># *** END CODE HERE ***</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Make a prediction given new inputs x.</span>

<span class="sd">        Args:</span>
<span class="sd">            x: Inputs of shape (m, n).</span>

<span class="sd">        Returns:</span>
<span class="sd">            Outputs of shape (m,).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># *** START CODE HERE ***</span>
        <span class="k">return</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">theta</span><span class="p">)))</span>
        <span class="c1"># *** END CODE HERE</span>
</code></pre></div>
<p><img src="https://gitee.com/violets/typora--images/raw/main/imgs/202203281713662.png" alt="p01e_1" style="zoom:50%;" /></p>
<p><img src="https://gitee.com/violets/typora--images/raw/main/imgs/202203281713895.png" alt="p01e_2" style="zoom:50%;" /></p>
<h3 id="gb-vs-e">g）(b) vs. (e)<a class="headerlink" href="#gb-vs-e" title="Permanent link">&para;</a></h3>
<p><strong>Dataset 1</strong> is fitted worse by GDA compare with Newton. </p>
<p><strong>Possible reason :</strong> <span class="arithmatex">\(x|y\)</span> might not Gaussian distributed, so the fitted line performed worse.</p>
<h3 id="h">h)<a class="headerlink" href="#h" title="Permanent link">&para;</a></h3>
<p><strong>Box-Cox transformation.</strong></p>
<p>~~Because of the skewed distribution of the data. Try another distribution for dataset 1, (e.g. Poisson, parameter is <span class="arithmatex">\(\lambda\)</span>). <em><u>(img pasted from wikipedia)</u></em>~~</p>
<p><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/1/16/Poisson_pmf.svg/1280px-Poisson_pmf.svg.png" alt="Plot of the Poisson PMF" style="zoom:25%;" /></p>
<h2 id="problem-2-incomplete-positive-only-labels">Problem 2 Incomplete, Positive-Only Labels<a class="headerlink" href="#problem-2-incomplete-positive-only-labels" title="Permanent link">&para;</a></h2>
<h1 id="ps-2">PS 2<a class="headerlink" href="#ps-2" title="Permanent link">&para;</a></h1>
<h2 id="problem-1">Problem 1<a class="headerlink" href="#problem-1" title="Permanent link">&para;</a></h2>
<h3 id="a">a)<a class="headerlink" href="#a" title="Permanent link">&para;</a></h3>
<p>Dataset A can converge, while Dataset B cannot.</p>
<h3 id="b">b)<a class="headerlink" href="#b" title="Permanent link">&para;</a></h3>
<p><img src="https://gitee.com/violets/typora--images/raw/main/imgs/202204121631057.png" alt="image-20220412163139012" style="zoom: 67%;" /></p>
<p><img src="https://gitee.com/violets/typora--images/raw/main/imgs/202204121631765.png" alt="image-20220412163144730" style="zoom:67%;" /></p>
<p><em>Dataset B</em> is linearly separable, <em>Dataset A</em> isn't.</p>
<p>Because <span class="arithmatex">\(y \in \{1,-1\}\)</span>, we can see that the gradient of the cost function is</p>
<p>$$
\nabla_\theta J(\theta) = - \frac{1}{m} \displaystyle \sum_{i = 1}^{m} \frac{y^{(i)} x^{(i)}}{1 + \exp (y^{(i)} \theta^T x^{(i)})}
$$
which means that the gradient descent algorithm is trying to minimize</p>
<p>$$
\ell (\theta) = - \frac{1}{m}  \displaystyle  \sum_{i = 1}^{m} \log \frac{1}{1 + \exp (-y^{(i)} \theta^T x^{(i)})}
$$
If a dataset is completely linearly separable, i.e. <span class="arithmatex">\(\forall i \in \{1, \dots, m \}, \ y^{(i)} \theta^T x^{(i)} &gt; 0\)</span>, then, by multiplying a larger positive scalar, there will always be a new <span class="arithmatex">\(\theta\)</span> that makes <span class="arithmatex">\(\ell (\theta)\)</span> even smaller, which prevents the algorithm from converging. However, if the dataset is not linearly separable, <span class="arithmatex">\(\theta\)</span> cannot be generated in such way while minimizing <span class="arithmatex">\(\ell (\theta)\)</span>.</p>
<h3 id="c">c)<a class="headerlink" href="#c" title="Permanent link">&para;</a></h3>
<h2 id="problem-3">Problem 3<a class="headerlink" href="#problem-3" title="Permanent link">&para;</a></h2>
<h3 id="a_1">a)<a class="headerlink" href="#a_1" title="Permanent link">&para;</a></h3>
<p><em><u></em><em>Prob:</em><em></u></em> Show that <span class="arithmatex">\(<span class="arithmatex">\(\theta_{\mathrm{MAP}}=\operatorname{argmax}_{\theta} p(y \mid x, \theta) p(\theta)\)</span>\)</span>, while <span class="arithmatex">\(<span class="arithmatex">\(p(\theta) = p(\theta \mid x)\)</span>\)</span>
$$
\begin{aligned}
p(\theta \mid x,y) &amp; = \frac{p(x,y,\theta)}{p(x,y)} \
&amp; = \frac{p(y \mid x,\theta)p(x,\theta)}{p(x,y)} \
&amp; = \frac{p(y \mid x,\theta)p(\theta \mid x)p(x)}{p(x,y)} \
\end{aligned}
$$
Because <span class="arithmatex">\(p(\theta) = p(\theta \mid x)\)</span>, thus
$$
\begin{aligned}
p(\theta \mid x,y) &amp; = \frac{p(y \mid x,\theta)p(\theta \mid x)p(x)}{p(x,y)} \
&amp; = \frac{p(y \mid x,\theta)p(\theta)p(x)}{p(x,y)}
\end{aligned}
$$
then
$$
\begin{aligned}
\theta_{MAP} &amp; = \underset{\theta}{argmax} \space p(\theta \mid x,y) \
&amp; = \underset{\theta}{argmax} \space p(y \mid x,\theta)p(\theta)\frac{p(x)}{p(x,y)}\
&amp; = \underset{\theta}{argmax} \space p(y \mid x,\theta)p(\theta)
\end{aligned}
$$</p>
<h3 id="b_1">b)<a class="headerlink" href="#b_1" title="Permanent link">&para;</a></h3>
<p><em><u></em><em>Prob:</em><em></u></em> Show that MAP estimation with a zero-mean Gaussian prior over <span class="arithmatex">\(\theta\)</span> (i.e. <span class="arithmatex">\(\theta \sim \mathcal{N}\left(0, \eta^{2} I\right)\)</span>) is equivalent to applying L2 regularization with MLE estimation.</p>
<p><em><u>Solution:</u></em></p>
<p>From (a), we know that
$$
\begin{aligned}\theta_{MAP} &amp; = \underset{\theta}{argmax} \space p(\theta \mid x,y) \
&amp; = \underset{\theta}{argmax} \space p(y \mid x,\theta)p(\theta)
\end{aligned}
$$
Because <span class="arithmatex">\(\theta \sim \mathcal{N}\left(0, \eta^{2} I\right)\)</span>, we have <span class="arithmatex">\(p(\theta) = \frac{1}{\eta \sqrt{2 \pi}} e^{-\frac{1}{2}\left(\frac{\theta}{\eta}\right)^{2}}\)</span>
$$
\begin{aligned}
\theta_{MAP}&amp; = \underset{\theta}{argmax} \space p(y \mid x,\theta)p(\theta) \
&amp; = \underset{\theta}{argmax} \space p(y \mid x,\theta) \frac{1}{\eta \sqrt{2 \pi}} e^{-\frac{1}{2}\left(\frac{\theta}{\eta}\right)^{2}} \
&amp; =\underset{\theta}{argmax} \space log \space {p(y \mid x,\theta)} - \frac{1}{2}\left(\frac{\theta}{\eta}\right)^{2} 
\end{aligned}
$$
While <span class="arithmatex">\(\eta &gt; 0\)</span> and we do <span class="arithmatex">\(arg\)</span> operation, thus
$$
\begin{aligned}
\theta_{MAP}&amp; = arg \space \underset{\theta}{max} \space p(y \mid x,\theta)p(\theta) \
&amp; =arg \space \underset{\theta}{min} \space - log \space {p(y \mid x,\theta)} + \frac{1}{2{\eta}^2}{| \theta |} ^{2}_2 
\end{aligned}
$$
so we have <span class="arithmatex">\(\lambda =  \frac{1}{2{\eta}^2}\)</span></p>
<h3 id="c_1">c)<a class="headerlink" href="#c_1" title="Permanent link">&para;</a></h3>
<p><em><u></em><em>Prob:</em><em></u></em> For linear regression model. Come up with a closed form expression for <span class="arithmatex">\(\theta_{MAP}\)</span>
$$
\begin{array}{c}
\epsilon^{(i)} \sim \mathcal{N}\left(0, \sigma^{2}\right) \
y^{(i)}=\theta^{T} x^{(i)}+\epsilon^{(i)} \
y^{(i)} \mid x^{(i)}, \theta \sim \mathcal{N}\left(\theta^{T} x^{(i)}, \sigma^{2}\right) \
p\left(y^{(i)} \mid x^{(i)}, \theta\right)=\frac{1}{\sqrt{2 \pi} \sigma} \exp \left{-\frac{1}{2 \sigma^{2}}\left(y^{(i)}-\theta^{T} x^{(i)}\right)^{2}\right} \
p(\vec{y} \mid X, \theta)=\prod_{i=1}^{m} p\left(y^{(i)} \mid x^{(i)}, \theta\right) \
=\prod_{i=1}^{m} \frac{1}{\sqrt{2 \pi} \sigma} \exp \left{-\frac{1}{2 \sigma^{2}}\left(y^{(i)}-\theta^{T} x^{(i)}\right)^{2}\right} \
=\frac{1}{(2 \pi)^{m / 2} \sigma^{m}} \exp \left{-\frac{1}{2 \sigma^{2}} \sum_{i=1}^{m}\left(y^{(i)}-\theta^{T} x^{(i)}\right)^{2}\right} \
=\frac{1}{(2 \pi)^{m / 2} \sigma^{m}} \exp \left{-\frac{1}{2 \sigma^{2}}|X \theta-\vec{y}|<em 2="2">{2}^{2}\right} \
\log p(\vec{y} \mid X, \theta)=-\frac{m}{2} \log (2 \pi)-m \log \sigma-\frac{1}{2 \sigma^{2}}|X \theta-\vec{y}|</em> \
\theta_{\mathrm{MAP}}=\arg \min }^{2<em 2="2">{\theta}-\log p(y \mid x, \theta)+\frac{1}{2 \eta^{2}}|\theta|</em> \
=\arg \min }^{2<em 2="2">{\theta} \frac{1}{2 \sigma^{2}}|X \theta-\vec{y}|</em>|\theta|}^{2}+\frac{1}{2 \eta^{2}<em 2="2">{2}^{2} \
J(\theta)=\frac{1}{2 \sigma^{2}}|X \theta-\vec{y}|</em>|\theta|}^{2}+\frac{1}{2 \eta^{2}<em _theta="\theta">{2}^{2} \
\nabla</em> \theta=0 \
\theta_{\mathrm{MAP}}=\arg \min _{\theta} J(\theta)=\left(X^{T} X+\frac{\sigma^{2}}{\eta^{2}} I\right)^{-1} X^{T} \vec{y}
\end{array}
$$} J(\theta)=\frac{1}{\sigma^{2}}\left(X^{T} X \theta-X^{T} \vec{y}\right)+\frac{1}{\eta^{2}</p>
<h3 id="d">d)<a class="headerlink" href="#d" title="Permanent link">&para;</a></h3>
<p><em><u></em><em>Prob:</em><em></u></em> Show that <span class="arithmatex">\(\theta_{MAP}\)</span> in this case is equivalent to the solution of linear regression with <span class="arithmatex">\(L1\)</span> regularization,
$$
\begin{aligned}
\epsilon^{(i)} &amp;\sim \mathcal{N}\left(0, \sigma^{2}\right) \
\theta &amp;\sim \mathcal{L}(0, b I) \
y^{(i)}&amp;=\theta^{T} x^{(i)}+\epsilon^{(i)} \
f_{\mathcal{L}}(z \mid \mu, b) &amp;=\frac{1}{2 b} \exp \left(-\frac{|z-\mu|}{b}\right)
\end{aligned}
$$
Thus 
$$
\begin{aligned}
p(\theta) &amp; = \frac{1}{(2b)^n} exp(-\frac{|\theta|<em _mathrm_MAP="\mathrm{MAP">1}{b}) \
\log p(\theta) &amp;= -n\log(2b)-\frac{|\theta|_1}{b} \
\theta</em> &amp; = \arg \max_\theta p(y \ \vert \ x, \theta) \ p(\theta) \
&amp; = \arg \min_\theta - \sum_{i = 1}^{m} \log \frac{1}{\sqrt{2 \pi} \sigma} \exp \big( - \frac{(y^{(i)} - \theta^T x^{(i)})^2}{2 \sigma^2} \big)- \sum_{i = 1}^{n} \log \frac{1}{2 b} \exp \big( - \frac{\vert \theta_i - 0 \vert}{b} \big) \
                      &amp; = \arg \min_\theta \frac{1}{2 \sigma^2} \sum_{i = 1}^{m} (y^{(i)} - \theta^T x^{(i)})^2 + \sum_{i = 1}^{n} \frac{1}{b} \vert \theta_i \vert \
                      &amp; = \arg \min_\theta \frac{1}{2 \sigma^2} \Vert X \theta - \vec{y} \Vert_2^2 + \frac{1}{b} \Vert \theta \Vert_1 \
                      &amp; = \arg \min_\theta \Vert X \theta - \vec{y} \Vert_2^2 + \frac{2 \sigma^2}{b} \Vert \theta \Vert_1
\end{aligned}
$$}</p>
<h2 id="problem-4">Problem 4<a class="headerlink" href="#problem-4" title="Permanent link">&para;</a></h2>
<h3 id="a_2">a)<a class="headerlink" href="#a_2" title="Permanent link">&para;</a></h3>
<p>True. Because <span class="arithmatex">\(K_1\)</span> and <span class="arithmatex">\(K_2\)</span> are kernel in same dimension, thus they are symmetric and positive semidefinite matrix in n-th dimension, so <span class="arithmatex">\(K_1 + K_2\)</span> is also a (Mercer) kernel in that dimension.</p>
<h3 id="b_2">b)<a class="headerlink" href="#b_2" title="Permanent link">&para;</a></h3>
<p>Not necessarily. Because <span class="arithmatex">\(K\)</span> isn't necessarily  a positive semidefinite matrix, thus not necessarily a kernel in n-th dimension.</p>
<h3 id="c-d">c) &amp; d)<a class="headerlink" href="#c-d" title="Permanent link">&para;</a></h3>
<p><em>(c)</em> True <em>(d)</em> False.</p>
<p><span class="arithmatex">\(a\)</span> is a positive real number, thus <span class="arithmatex">\(aK\)</span> is a positive semidefinite matrix, while <span class="arithmatex">\(-aK\)</span> isn't.</p>
<h3 id="e">e)<a class="headerlink" href="#e" title="Permanent link">&para;</a></h3>
<p><span class="arithmatex">\(K(x, z) = K_1 (x, z) K_2 (x, z)\)</span> is a valid kernel.</p>
<p>Because for any <span class="arithmatex">\(z \in \mathbb{R}^n\)</span>,</p>
<div class="arithmatex">\[
\begin{align*}
z^T K z &amp; = \sum_i \sum_j z_i K_{ij} z_j \\
        &amp; = \sum_i \sum_j z_i K(x^{(i)}, x^{(j)}) z_j \\
        &amp; = \sum_i \sum_j z_i K_1 (x^{(i)}, x^{(j)}) K_2 (x^{(i)}, x^{(j)}) z_j \\
        &amp; = \sum_i \sum_j z_i \phi_1 (x^{(i)})^T \phi_1 (x^{(j)}) \phi_2 (x^{(i)})^T \phi_2 (x^{(j)}) z_j \\
        &amp; = \sum_i \sum_j z_i \sum_k \phi_{1k} (x^{(i)}) \phi_{1k} (x^{(j)}) \sum_l \phi_{2l} (x^{(i)}) \phi_{2l} (x^{(j)}) z_j \\
        &amp; = \sum_k \sum_l \sum_i \sum_j z_i \phi_{1k} (x^{(i)}) \phi_{2l} (x^{(i)}) z_j \phi_{1k} (x^{(j)}) \phi_{2l} (x^{(j)}) \\
        &amp; = \sum_k \sum_l \big( \sum_i z_i \phi_{1k} (x^{(i)}) \phi_{2l} (x^{(i)}) \big)^2 \\
        &amp; \geq 0
\end{align*}
\]</div>
<h3 id="f">f)<a class="headerlink" href="#f" title="Permanent link">&para;</a></h3>
<p><strong>Yes.</strong> <span class="arithmatex">\(K\)</span> is PSD </p>
<div class="arithmatex">\[
\begin{aligned}
z^T K z &amp; = \sum_i\sum_j z_i f(x_i)f(x_j) z_j \\
&amp; = \sum_i (z_i f(x_i))^2 \ge 0
\end{aligned}
\]</div>
<p>~~Not necessarily. When <span class="arithmatex">\(f(x)f(z)&lt;0\)</span>, then for <span class="arithmatex">\(z^T K z\)</span> we have a negative value multiply <span class="arithmatex">\(z^T z\)</span>,  so <span class="arithmatex">\(z^T K z \le 0\)</span>. <span class="arithmatex">\(K\)</span> isn't PSD.~~</p>
<h3 id="g">g)<a class="headerlink" href="#g" title="Permanent link">&para;</a></h3>
<p><span class="arithmatex">\(K\)</span> is PSD. Because <span class="arithmatex">\(K_3\)</span> is a kernel no matter what the inputs are.</p>
<h3 id="h_1">h)<a class="headerlink" href="#h_1" title="Permanent link">&para;</a></h3>
<p><span class="arithmatex">\(K\)</span> is a kernel. Because <span class="arithmatex">\(p(x) = \displaystyle \sum_k c_k x^k\)</span>, thus <span class="arithmatex">\(p(K_1 (x,z)) = \displaystyle \sum_k c_k (K_1 (x,z))^k\)</span>.</p>
<p>From <em>(e)</em>, we know <span class="arithmatex">\(K(x,z)=K_1(x,z) K_2(x,z)\)</span> is a kernel.</p>
<p>From <em>(c)</em>, we know <span class="arithmatex">\(K(x,z) = a K_1(x,z), a \in \mathbb R^+\)</span> is a kernel.</p>
<p>thus <span class="arithmatex">\(K\)</span> is a kernel. </p>
<h2 id="problem-5">Problem 5<a class="headerlink" href="#problem-5" title="Permanent link">&para;</a></h2>
<h3 id="a_3">a)<a class="headerlink" href="#a_3" title="Permanent link">&para;</a></h3>
<ol>
<li><strong><u>How to represent <span class="arithmatex">\(\theta^{(i)}\)</span></u></strong></li>
</ol>
<p>Because <span class="arithmatex">\(\theta^{(i+1)}:=\theta^{(i)}+\alpha\left(y^{(i+1)}-h_{\theta^{(i)}}\left(\phi (x^{(i+1))}\right)\right) \phi (x^{(i+1)})\)</span>, so <span class="arithmatex">\(\theta^{(i)}\)</span> is a linear function of <span class="arithmatex">\(\phi (x^{(0)}), \phi (x^{(1)})...\phi (x^{(i)})\)</span></p>
<p><em>i.e.</em>
   $$
   \begin{aligned}
   \theta^{(i)} &amp;= \sum_{j=1}^i \space \beta_i \phi (x^{(i)}) = \beta^T \Phi(x)\
   \theta^{(0)} &amp;= \sum_{j=1}^0 \space  \beta_i \phi (x^{(i)}) = 0
   \end{aligned}
   $$</p>
<ol>
<li>
<p><strong><u>How to represent <span class="arithmatex">\(h_{\theta^{(i)}}\left(x^{(i+1)}\right)\)</span></u></strong>
   $$
   \begin{aligned}
   h_{\theta^{(i)}}\left(x^{(i+1)}\right) &amp;= g\left(\theta^{(i)^{T}} \phi\left(x^{(i+1)}\right)\right) \
   &amp; = g\left(\sum_{j=1}^i \space \beta_i \phi \left(x^{(i)}\right) \phi\left(x^{(i+1)}\right)\right) \
   &amp; = g\left(\sum_{j=1}^i \space \beta_i K\left(x,z\right)\right) \
   &amp; = g\left(\beta^T K\right) \
   &amp; = \mathbb I{\beta^T K = 1}
   \end{aligned}
   $$</p>
</li>
<li>
<p><strong><u>How to modify the update rule?</u></strong>
   $$
   \begin{aligned}
   \theta^{(i+1)} &amp;= \theta^{(i)} + \alpha \left(\overset {\rightarrow}{y}X - \mathbb I \left{\beta^T K =1\right}^T X \right)
   \end{aligned}
   $$
   <span class="arithmatex">\(<span class="arithmatex">\(\begin{align*}
   \theta^{(i + 1)} : &amp; = \theta^{(i)} + \alpha \big( y^{(i + 1)} - h_{\theta^{(i)}} (\phi (x^{(i + 1)})) \big) \phi (x^{(i + 1)}) \\
                      &amp; = \sum_{j = 1}^{i} \beta_j \phi (x^{(j)}) + \underbrace{\alpha ( y^{(i + 1)} - \mathrm{sign} \big( \sum_{j = 1}^{i} \beta_j K(x^{(j)}, x^{(i + 1)}) \big) )}_{\beta_{i + 1}} \phi (x^{(i + 1)}) \\
                      &amp; = \sum_{j = 1}^{i + 1} \beta_j \phi (x^{(j)})
   \end{align*}\)</span>\)</span></p>
</li>
</ol>
<p>Therefore, the new update rule is:</p>
<p><span class="arithmatex">\(<span class="arithmatex">\(\beta_{i + 1} := \alpha ( y^{(i + 1)} - \mathrm{sign} \big( \sum_{j = 1}^{i} \beta_j K(x^{(j)}, x^{(i + 1)}) \big) )\)</span>\)</span></p>
<h3 id="b_3">b)<a class="headerlink" href="#b_3" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">initial_state</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Return the initial state for the perceptron.</span>

<span class="sd">    This function computes and then returns the initial state of the perceptron.</span>
<span class="sd">    Feel free to use any data type (dicts, lists, tuples, or custom classes) to</span>
<span class="sd">    contain the state of the perceptron.</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># *** START CODE HERE ***</span>
    <span class="k">return</span> <span class="p">[]</span>
    <span class="c1"># *** END CODE HERE ***</span>


<span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">kernel</span><span class="p">,</span> <span class="n">x_i</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Perform a prediction on a given instance x_i given the current state</span>
<span class="sd">    and the kernel.</span>

<span class="sd">    Args:</span>
<span class="sd">        state: The state returned from initial_state()</span>
<span class="sd">        kernel: A binary function that takes two vectors as input and returns</span>
<span class="sd">            the result of a kernel</span>
<span class="sd">        x_i: A vector containing the features for a single instance</span>

<span class="sd">    Returns:</span>
<span class="sd">        Returns the prediction (i.e 0 or 1)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># *** START CODE HERE ***</span>
    <span class="k">return</span> <span class="n">sign</span><span class="p">(</span><span class="nb">sum</span><span class="p">(</span><span class="n">beta</span> <span class="o">*</span> <span class="n">kernel</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x_i</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">beta</span> <span class="ow">in</span> <span class="n">state</span><span class="p">))</span>
    <span class="c1"># *** END CODE HERE ***</span>


<span class="k">def</span> <span class="nf">update_state</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">kernel</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">,</span> <span class="n">x_i</span><span class="p">,</span> <span class="n">y_i</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Updates the state of the perceptron.</span>

<span class="sd">    Args:</span>
<span class="sd">        state: The state returned from initial_state()</span>
<span class="sd">        kernel: A binary function that takes two vectors as input and returns the result of a kernel</span>
<span class="sd">        learning_rate: The learning rate for the update</span>
<span class="sd">        x_i: A vector containing the features for a single instance</span>
<span class="sd">        y_i: A 0 or 1 indicating the label for a single instance</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># *** START CODE HERE ***</span>
    <span class="n">beta_i</span> <span class="o">=</span> <span class="n">learning_rate</span><span class="o">*</span><span class="p">(</span><span class="n">y_i</span> <span class="o">-</span> <span class="n">sign</span><span class="p">(</span><span class="nb">sum</span><span class="p">(</span><span class="n">beta</span> <span class="o">*</span> <span class="n">kernel</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x_i</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">beta</span> <span class="ow">in</span> <span class="n">state</span><span class="p">)))</span>
    <span class="n">state</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">x_i</span><span class="p">,</span> <span class="n">beta_i</span><span class="p">))</span>
    <span class="c1"># *** END CODE HERE ***</span>
</code></pre></div>
<h3 id="c_2">c)<a class="headerlink" href="#c_2" title="Permanent link">&para;</a></h3>
<p><strong><u>Dot kernel</u></strong></p>
<p><img src="https://gitee.com/violets/typora--images/raw/main/imgs/202204181405364.png" alt="image-20220418140540184" style="zoom: 67%;" /></p>
<p><strong><u>Radial basis kernel</u></strong></p>
<p><img src="https://gitee.com/violets/typora--images/raw/main/imgs/202204181407778.png" alt="image-20220418140728685" style="zoom: 55%;" /></p>
<p>Dot kernel perform badly, because dataset isn't linearly separable. Because dot product kernel doesn't have feature mapping, thus the model is still linear after applying the product kernel.</p>
<h2 id="problem-6-spam-classification">Problem 6 Spam Classification<a class="headerlink" href="#problem-6-spam-classification" title="Permanent link">&para;</a></h2>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">collections</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="kn">import</span> <span class="nn">util</span>
<span class="kn">import</span> <span class="nn">svm</span>
</code></pre></div>
<h3 id="a-processing-the-the-spam-messages-into-numpy-arrays">a) processing the the spam messages into numpy arrays<a class="headerlink" href="#a-processing-the-the-spam-messages-into-numpy-arrays" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">get_words</span><span class="p">(</span><span class="n">message</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Get the normalized list of words from a message string.</span>

<span class="sd">    This function should split a message into words, normalize them, and return</span>
<span class="sd">    the resulting list. For splitting, you should split on spaces. For normalization,</span>
<span class="sd">    you should convert everything to lowercase.</span>

<span class="sd">    Args:</span>
<span class="sd">        message: A string containing an SMS message</span>

<span class="sd">    Returns:</span>
<span class="sd">       The list of normalized words from the message.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># *** START CODE HERE ***</span>
    <span class="k">return</span> <span class="n">message</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
    <span class="c1"># *** END CODE HERE ***</span>


<span class="k">def</span> <span class="nf">create_dictionary</span><span class="p">(</span><span class="n">messages</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Create a dictionary mapping words to integer indices.</span>

<span class="sd">    This function should create a dictionary of word to indices using the provided</span>
<span class="sd">    training messages. Use get_words to process each message. </span>

<span class="sd">    Rare words are often not useful for modeling. Please only add words to the dictionary</span>
<span class="sd">    if they occur in at least five messages.</span>

<span class="sd">    Args:</span>
<span class="sd">        messages: A list of strings containing SMS messages</span>

<span class="sd">    Returns:</span>
<span class="sd">        A python dict mapping words to integers.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># *** START CODE HERE ***</span>
    <span class="n">word_dict</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
    <span class="n">index</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">message</span> <span class="ow">in</span> <span class="n">messages</span><span class="p">:</span>
        <span class="n">word_list</span> <span class="o">=</span> <span class="n">get_words</span><span class="p">(</span><span class="n">message</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">word_list</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">word</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">word_dict</span><span class="p">:</span>
                <span class="n">word_dict</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">word_dict</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="c1"># Delete rare words</span>
    <span class="n">index</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="nb">list</span><span class="p">(</span><span class="n">word_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">()):</span>
        <span class="k">if</span> <span class="n">word_dict</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="mi">5</span><span class="p">:</span>
            <span class="n">word_dict</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">=</span> <span class="n">index</span>
            <span class="n">index</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">del</span> <span class="n">word_dict</span><span class="p">[</span><span class="n">word</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">word_dict</span>
    <span class="c1"># *** END CODE HERE ***</span>


<span class="k">def</span> <span class="nf">transform_text</span><span class="p">(</span><span class="n">messages</span><span class="p">,</span> <span class="n">word_dictionary</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Transform a list of text messages into a numpy array for further processing.</span>

<span class="sd">    This function should create a numpy array that contains the number of times each word</span>
<span class="sd">    appears in each message. Each row in the resulting array should correspond to each </span>
<span class="sd">    message and each column should correspond to a word.</span>

<span class="sd">    Use the provided word dictionary to map words to column indices. Ignore words that </span>
<span class="sd">    are not present in the dictionary. Use get_words to get the words for a message.</span>

<span class="sd">    Args:</span>
<span class="sd">        messages: A list of strings where each string is an SMS message.</span>
<span class="sd">        word_dictionary: A python dict mapping words to integers.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A numpy array marking the words present in each message.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># *** START CODE HERE ***</span>
    <span class="n">m</span><span class="p">,</span> <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">messages</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">word_dictionary</span><span class="p">)</span>
    <span class="c1"># Initialize word matrix</span>
    <span class="n">word_matrix</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>
    <span class="c1"># for word in word_dictionary:</span>
    <span class="c1">#     word_index = word_dictionary[word]</span>
    <span class="c1">#     word_matrix[0, word_index] = word_index</span>
    <span class="c1"># Record occurrence of words in messages</span>
    <span class="n">msg_index</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">message</span> <span class="ow">in</span> <span class="n">messages</span><span class="p">:</span>
        <span class="n">word_list</span> <span class="o">=</span> <span class="n">get_words</span><span class="p">(</span><span class="n">message</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">word_list</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">word_dictionary</span><span class="p">:</span>
                <span class="n">word_matrix</span><span class="p">[</span><span class="n">msg_index</span><span class="p">,</span> <span class="n">word_dictionary</span><span class="p">[</span><span class="n">word</span><span class="p">]]</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="n">msg_index</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">word_matrix</span>
    <span class="c1"># *** END CODE HERE ***</span>
</code></pre></div>
<h3 id="b-fit-predict">b) Fit &amp; Predict<a class="headerlink" href="#b-fit-predict" title="Permanent link">&para;</a></h3>
<p><strong><u>Prediction accuracy:</u></strong> <code>Naive Bayes had an accuracy of 0.8870967741935484 on the testing set</code></p>
<p>If possibility is <code>0.28</code> instead of <code>0.5</code>, the accuracy on the test set is <code>0.9301075268817204</code></p>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">fit_naive_bayes_model</span><span class="p">(</span><span class="n">matrix</span><span class="p">,</span> <span class="n">labels</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Fit a naive bayes model.</span>

<span class="sd">    This function should fit a Naive Bayes model given a training matrix and labels.</span>

<span class="sd">    The function should return the state of that model.</span>

<span class="sd">    Feel free to use whatever datatype you wish for the state of the model.</span>

<span class="sd">    Args:</span>
<span class="sd">        matrix: A numpy array containing word counts for the training data</span>
<span class="sd">        labels: The binary (0 or 1) labels for that training data</span>

<span class="sd">    Returns: The trained model</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># *** START CODE HERE ***</span>
    <span class="n">m</span><span class="p">,</span> <span class="n">n</span> <span class="o">=</span> <span class="n">matrix</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">num_spam</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span>
    <span class="n">phi_1</span> <span class="o">=</span> <span class="p">(</span><span class="n">matrix</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">num_spam</span> <span class="o">+</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">phi_0</span> <span class="o">=</span> <span class="p">(</span><span class="n">matrix</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">labels</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">m</span> <span class="o">-</span> <span class="n">num_spam</span> <span class="o">+</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">phi_y</span> <span class="o">=</span> <span class="n">num_spam</span> <span class="o">/</span> <span class="n">m</span>
    <span class="k">return</span> <span class="n">phi_1</span><span class="p">,</span> <span class="n">phi_0</span><span class="p">,</span> <span class="n">phi_y</span>
    <span class="c1"># *** END CODE HERE ***</span>


<span class="k">def</span> <span class="nf">predict_from_naive_bayes_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">matrix</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Use a Naive Bayes model to compute predictions for a target matrix.</span>

<span class="sd">    This function should be able to predict on the models that fit_naive_bayes_model</span>
<span class="sd">    outputs.</span>

<span class="sd">    Args:</span>
<span class="sd">        model: A trained model from fit_naive_bayes_model</span>
<span class="sd">        matrix: A numpy array containing word counts</span>

<span class="sd">    Returns: A numpy array contains the predictions from the model</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># *** START CODE HERE ***</span>
    <span class="n">phi_1</span><span class="p">,</span> <span class="n">phi_0</span><span class="p">,</span> <span class="n">phi_y</span> <span class="o">=</span> <span class="n">model</span>
    <span class="n">log_y1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">matrix</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">phi_1</span><span class="p">))</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">phi_y</span><span class="p">)</span>
    <span class="n">log_y0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">matrix</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">phi_0</span><span class="p">))</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">phi_y</span><span class="p">)</span>
    <span class="n">possibility_y1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">log_y1</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">log_y0</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">log_y1</span><span class="p">))</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">possibility_y1</span> <span class="o">&gt;</span> <span class="mf">0.5</span><span class="p">]</span>
    <span class="c1"># *** END CODE HERE ***</span>
</code></pre></div>
<h3 id="c-get-spam-indicators">c) Get Spam Indicators<a class="headerlink" href="#c-get-spam-indicators" title="Permanent link">&para;</a></h3>
<p><code>The top 5 indicative words for Naive Bayes are:  ['claim', 'won', 'prize', 'tone', 'urgent!']</code></p>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">get_top_five_naive_bayes_words</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">dictionary</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Compute the top five words that are most indicative of the spam (i.e positive) class.</span>

<span class="sd">    Ues the metric given in 6c as a measure of how indicative a word is.</span>
<span class="sd">    Return the words in sorted form, with the most indicative word first.</span>

<span class="sd">    Args:</span>
<span class="sd">        model: The Naive Bayes model returned from fit_naive_bayes_model</span>
<span class="sd">        dictionary: A mapping of word to integer ids</span>

<span class="sd">    Returns: The top five most indicative words in sorted order with the most indicative first</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># *** START CODE HERE ***</span>
    <span class="n">phi_1</span><span class="p">,</span> <span class="n">phi_0</span><span class="p">,</span> <span class="n">phi_y</span> <span class="o">=</span> <span class="n">model</span>
    <span class="n">spam_coeff</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">phi_1</span><span class="p">)</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">phi_0</span><span class="p">)</span>
    <span class="n">hot5word_index</span> <span class="o">=</span> <span class="n">spam_coeff</span><span class="o">.</span><span class="n">argsort</span><span class="p">()[</span><span class="o">-</span><span class="mi">5</span><span class="p">:]</span>
    <span class="n">hot5word</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">index</span> <span class="ow">in</span> <span class="n">hot5word_index</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">dictionary</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">dictionary</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">==</span> <span class="n">index</span><span class="p">:</span>
                <span class="n">hot5word</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
                <span class="k">break</span>
    <span class="n">hot5word</span><span class="o">.</span><span class="n">reverse</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">hot5word</span>
    <span class="c1"># *** END CODE HERE ***</span>
</code></pre></div>
<h3 id="d-compute-best-svm-radius">d) Compute Best SVM Radius<a class="headerlink" href="#d-compute-best-svm-radius" title="Permanent link">&para;</a></h3>
<p><code>The optimal SVM radius was 0.1
The SVM model had an accuracy of 0.9695340501792115 on the testing set</code></p>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">compute_best_svm_radius</span><span class="p">(</span><span class="n">train_matrix</span><span class="p">,</span> <span class="n">train_labels</span><span class="p">,</span> <span class="n">val_matrix</span><span class="p">,</span> <span class="n">val_labels</span><span class="p">,</span> <span class="n">radius_to_consider</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Compute the optimal SVM radius using the provided training and evaluation datasets.</span>

<span class="sd">    You should only consider radius values within the radius_to_consider list.</span>
<span class="sd">    You should use accuracy as a metric for comparing the different radius values.</span>

<span class="sd">    Args:</span>
<span class="sd">        train_matrix: The word counts for the training data</span>
<span class="sd">        train_labels: The spam or not spam labels for the training data</span>
<span class="sd">        val_matrix: The word counts for the validation data</span>
<span class="sd">        val_labels: The spam or not spam labels for the validation data</span>
<span class="sd">        radius_to_consider: The radius values to consider</span>

<span class="sd">    Returns:</span>
<span class="sd">        The best radius which maximizes SVM accuracy.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># *** START CODE HERE ***</span>
    <span class="n">rad_acc_dict</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">radius</span> <span class="ow">in</span> <span class="n">radius_to_consider</span><span class="p">:</span>
        <span class="n">predict_label</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">train_and_predict_svm</span><span class="p">(</span><span class="n">train_matrix</span><span class="p">,</span> <span class="n">train_labels</span><span class="p">,</span> <span class="n">val_matrix</span><span class="p">,</span> <span class="n">radius</span><span class="p">)</span>
        <span class="n">accuracy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">predict_label</span> <span class="o">==</span> <span class="n">val_labels</span><span class="p">)</span>
        <span class="n">rad_acc_dict</span><span class="p">[</span><span class="n">radius</span><span class="p">]</span> <span class="o">=</span> <span class="n">accuracy</span>
    <span class="n">maxaccuracy</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">rad_acc_dict</span><span class="o">.</span><span class="n">values</span><span class="p">())</span>
    <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">rad_acc_dict</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">rad_acc_dict</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">==</span> <span class="n">maxaccuracy</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">key</span>
    <span class="c1"># return rad_acc_dict.popitem()</span>
    <span class="c1"># *** END CODE HERE ***</span>
</code></pre></div>
<h1 id="ps-3">PS 3<a class="headerlink" href="#ps-3" title="Permanent link">&para;</a></h1>
<h2 id="problem-1-a-simple-neural-network">Problem 1 A Simple Neural Network<a class="headerlink" href="#problem-1-a-simple-neural-network" title="Permanent link">&para;</a></h2>
<h3 id="a-calculate-gradient-descent-update-to-w-_12i">a) Calculate gradient descent update to <span class="arithmatex">\(w _{1,2}^{[i]}\)</span><a class="headerlink" href="#a-calculate-gradient-descent-update-to-w-_12i" title="Permanent link">&para;</a></h3>
<div class="arithmatex">\[
\begin{aligned}
\frac{\part l}{\part w _{1,2}^{[i]}} &amp; = \frac{\part l}{\part o^{[i]}} \frac{\part o^{[i]}}{\part h_2^{[i]}} \frac{\part h_2^{[i]}}{\part w _{1,2}^{[i]}}\\
&amp; = \frac{2}{m} \sum _{i=1}^m \left(o^{[i]}-y^{[i]}\right) o^{[i]} \left(1-o^{[i]}\right) w_2^{[2]} h_2^{[i]} \left(1-h_2^{[i]}\right) x_1^{[i]} \\
&amp; = \frac{2}{m}  w_2^{[2]}  \sum _{i=1}^m \left(o^{[i]}-y^{[i]}\right) o^{[i]} \left(1-o^{[i]}\right) h_2^{[i]} \left(1-h_2^{[i]}\right) x_1^{[i]} \\
\text{Update rule for } w _{1,2}^{[i]} \text{ is:}\\
w _{1,2}^{[i]} &amp;= w _{1,2}^{[i]} - \alpha \frac{\part l}{\part w _{1,2}^{[i]}}\\
&amp; = w _{1,2}^{[i]} - \alpha \frac{2}{m}  w_2^{[2]}  \sum _{i=1}^m \left(o^{[i]}-y^{[i]}\right) o^{[i]} \left(1-o^{[i]}\right) h_2^{[i]} \left(1-h_2^{[i]}\right) x_1^{[i]} 
\end{aligned}
\]</div>
<h3 id="b-modify-activation-function-as-step-function-and-prove-its-accuracy">b) Modify activation function as step function and prove its accuracy<a class="headerlink" href="#b-modify-activation-function-as-step-function-and-prove-its-accuracy" title="Permanent link">&para;</a></h3>
<p>After using step function as activation function to achieve 100% accuracy.</p>
<p>Because we CAN use three line(which form a triangle area) to separate different category samples for the given dataset.</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">optimal_step_weights</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Return the optimal weights for the neural network with a step activation function.</span>

<span class="sd">    This function will not be graded if there are no optimal weights.</span>
<span class="sd">    See the PDF for instructions on what each weight represents.</span>

<span class="sd">    The hidden layer weights are notated by [1] on the problem set and </span>
<span class="sd">    the output layer weights are notated by [2].</span>

<span class="sd">    This function should return a dict with elements for each weight, see example_weights above.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">example_weights</span><span class="p">()</span>

    <span class="c1"># *** START CODE HERE ***</span>
    <span class="n">w</span><span class="p">[</span><span class="s1">&#39;hidden_layer_0_1&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
    <span class="n">w</span><span class="p">[</span><span class="s1">&#39;hidden_layer_1_1&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">2</span>
    <span class="n">w</span><span class="p">[</span><span class="s1">&#39;hidden_layer_2_1&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">w</span><span class="p">[</span><span class="s1">&#39;hidden_layer_0_2&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
    <span class="n">w</span><span class="p">[</span><span class="s1">&#39;hidden_layer_1_2&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">w</span><span class="p">[</span><span class="s1">&#39;hidden_layer_2_2&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">2</span>
    <span class="n">w</span><span class="p">[</span><span class="s1">&#39;hidden_layer_0_3&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">4</span>
    <span class="n">w</span><span class="p">[</span><span class="s1">&#39;hidden_layer_1_3&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">w</span><span class="p">[</span><span class="s1">&#39;hidden_layer_2_3&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>

    <span class="n">w</span><span class="p">[</span><span class="s1">&#39;output_layer_0&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
    <span class="n">w</span><span class="p">[</span><span class="s1">&#39;output_layer_1&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">2</span>
    <span class="n">w</span><span class="p">[</span><span class="s1">&#39;output_layer_2&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">2</span>
    <span class="n">w</span><span class="p">[</span><span class="s1">&#39;output_layer_3&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">2</span>
    <span class="c1"># *** END CODE HERE ***</span>

    <span class="k">return</span> <span class="n">w</span>
</code></pre></div>
<h3 id="c-modify-activation-function-as-step-function-and-prove-its-accuracy">c) Modify activation function as step function and prove its accuracy<a class="headerlink" href="#c-modify-activation-function-as-step-function-and-prove-its-accuracy" title="Permanent link">&para;</a></h3>
<p>It's not possible, when we use identity(linear) function as activation function and output layer activation is step function, the neural network can be viewed as a linear classifier. But given dataset is not linearly separable.</p>
<h2 id="problem-2-kl-divergence-and-maximum-likelihood">Problem 2 KL divergence and Maximum Likelihood<a class="headerlink" href="#problem-2-kl-divergence-and-maximum-likelihood" title="Permanent link">&para;</a></h2>
<h3 id="a-non-negativity">a) Non-negativity<a class="headerlink" href="#a-non-negativity" title="Permanent link">&para;</a></h3>
<div class="arithmatex">\[
\begin{aligned}
D_{\mathrm{KL}}(P \| Q) &amp;=\sum_{x \in \mathcal{X}} P(x) \log \frac{P(x)}{Q(x)} \\
&amp;=-\sum_{x \in \mathcal{X}} P(x) \log \frac{Q(x)}{P(x)} \\
&amp;=E\left[-\log \frac{Q(x)}{P(x)}\right]\\
&amp;\ge -\log E\left[ \frac{Q(x)}{P(x)}\right]\\
&amp;=- \log  \sum_{x \in \mathcal{X}}P(x) \frac{Q(x)}{P(x)}\\
&amp;=- \log  \sum_{x \in \mathcal{X}} Q(x) \\
&amp;=- \log 1 \\
&amp;=0
\end{aligned}
\]</div>
<p>Further,</p>
<ul>
<li>
<p>If <span class="arithmatex">\(P = Q\)</span>, then <span class="arithmatex">\(D_{\mathrm{KL}}(P \| Q) =\sum_{x \in \mathcal{X}} P(x) \log 1 = 0\)</span></p>
</li>
<li>
<p>Because <span class="arithmatex">\(-\log\)</span> is strictly convex, so if <span class="arithmatex">\(D_{\mathrm{KL}}(P \| Q) =0\)</span> while <span class="arithmatex">\(\frac{Q(x)}{P(x)}\)</span> is a constant for all <span class="arithmatex">\(x \in \mathcal{X}\)</span>, which is <span class="arithmatex">\(Q(x)=P(x)\)</span></p>
</li>
</ul>
<p>Thus, <span class="arithmatex">\(D_{\mathrm{KL}}(P \| Q) =0\)</span> if and only if <span class="arithmatex">\(P=Q\)</span></p>
<h3 id="b-chain-rule-for-kl-divergence">b) Chain rule for KL divergence<a class="headerlink" href="#b-chain-rule-for-kl-divergence" title="Permanent link">&para;</a></h3>
<div class="arithmatex">\[
\begin{aligned}
D_{\mathrm{KL}}(P(X) \| Q(X)) &amp;= \sum_{x \in \mathcal{X}} P(x) \log \frac{P(x)}{Q(x)} \\
D_{\mathrm{KL}}(P(Y \mid X) \| Q(Y \mid X)) &amp;= \sum_{x} P(x)\left(\sum_{y} P(y \mid x) \log \frac{P(y \mid x)}{Q(y \mid x)}\right)
\end{aligned}
\]</div>
<p>According to Bayes, we have
$$
P(X,Y) = P(Y \mid X) P(X)
$$</p>
<div class="arithmatex">\[
\begin{aligned}
D_{\mathrm{KL}}(P(X, Y) \| Q(X, Y)) &amp;= D_{\mathrm{KL}}(P(Y \mid X) P(X) \| Q(Y \mid X) Q(X)) \\
&amp;= \sum_{x}\sum_{y} P(y \mid x)P(x) \log {\frac {P(y \mid x)P(x)}{Q(y \mid x)Q(x)}} \\
&amp;= \sum_{x}\sum_{y} P(y \mid x)P(x)  \left(\log {\frac{P(y \mid x)}{Q(y \mid x)}}+\log \frac {P(x)}{Q(x)} \right) \\
&amp;= \sum_{y}P(y \mid x)\sum_{x}P(x)\log \frac {P(x)}{Q(x)} + \sum_{x} P(x)\left(\sum_{y} P(y \mid x) \log \frac{P(y \mid x)}{Q(y \mid x)}\right) \\
&amp;= 1* \sum_{x}P(x)\log \frac {P(x)}{Q(x)} + \sum_{x} P(x)\left(\sum_{y} P(y \mid x) \log \frac{P(y \mid x)}{Q(y \mid x)}\right) \\
&amp;=D_{\mathrm{KL}}(P(X) \| Q(X))+D_{\mathrm{KL}}(P(Y \mid X) \| Q(Y \mid X))
\end{aligned}
\]</div>
<h3 id="c-kl-and-maximum-likelihood">c) KL and maximum likelihood<a class="headerlink" href="#c-kl-and-maximum-likelihood" title="Permanent link">&para;</a></h3>
<div class="arithmatex">\[
D_{\mathrm{KL}}\left(\hat{P} \| P_{\theta}\right) = \sum_{x} \hat{P}(x) \log \frac{\hat{P}(x)}{P_{\theta}(x)}
\]</div>
<p>Thus,
$$
\begin{aligned}
\arg \min <em _mathrm_KL="\mathrm{KL">{\theta} D</em>\right)&amp;= \arg \min }}\left(\hat{P} | P_{\theta<em x="x">{\theta} \sum</em>\right) \
&amp;= \arg \max } \hat{P}(x) \log \left({\hat{P}(x)}-{P_{\theta}(x)<em x="x">{\theta}\sum</em>\
&amp;= \arg \max } \hat{P}(x) \log {P_{\theta}(x)<em x="x">{\theta}\sum</em>(x)\
&amp;= \arg \max } \left(\frac{1}{m} \sum_{i=1}^m 1\left{x^{(i)} = x \right}\right)P_{\theta<em i="1">{\theta} \sum</em>\right) \
\end{aligned}
$$}^{m} \log P_{\theta}\left(x^{(i)</p>
<h2 id="problem-3-kl-divergence-fisher-information-and-the-natural-gradient">Problem 3 KL Divergence, Fisher Information, and the Natural Gradient<a class="headerlink" href="#problem-3-kl-divergence-fisher-information-and-the-natural-gradient" title="Permanent link">&para;</a></h2>
<h3 id="a-score-function">a) Score Function<a class="headerlink" href="#a-score-function" title="Permanent link">&para;</a></h3>
<p><em>signifies the sensitivity of the likelihood function with respect to the parameters.</em>
$$
\begin{aligned}
\nabla_{\theta} \log p\left(y ; \theta\right) &amp;= \frac {\nabla_{\theta} p(y ; \theta)}{p(y ; \theta)} \
\
\mathbb{E}<em _theta_prime="\theta^{\prime">{y \sim p(y ; \theta)}\left[\left.\nabla</em>\right)\right|}} \log p\left(y ; \theta^{\prime<em -_infty="-\infty">{\theta^{\prime}=\theta}\right] &amp;= \int</em>\right)\right|}^{\infty} p(y) \left.\nabla_{\theta^{\prime}} \log p\left(y ; \theta^{\prime<em -_infty="-\infty">{\theta^{\prime}=\theta} d y\
&amp;= \int</em> d y \
&amp;= \int_{-\infty}^{\infty} \nabla_{\theta} p(y ; \theta) d y\
&amp;= 0 \
\end{aligned}
$$}^{\infty} p(y ; \theta)\frac {\nabla_{\theta} p(y ; \theta)}{p(y ; \theta)</p>
<h3 id="b-fisher-information">b) Fisher information<a class="headerlink" href="#b-fisher-information" title="Permanent link">&para;</a></h3>
<p><em>Fisher information represents the amount of information that a random variable Y carries about a parameter θ of interest.</em></p>
<p>From score function, we know that,
$$
\begin{aligned}
\mathbb{E}<em _theta_prime="\theta^{\prime">{y \sim p(y ; \theta)}\left[\left.\nabla</em>\right)\right|}} \log p\left(y ; \theta^{\prime<em _=";" _sim="\sim" _theta_="\theta)" p_y="p(y" y="y">{\theta^{\prime}=\theta}\right] = 0 
\end{aligned}
$$
So we can derive the following:
$$
\begin{aligned}
\mathcal{I}(\theta)&amp;= \operatorname{Cov}</em>\right)\right|}\left[\left.\nabla_{\theta^{\prime}} \log p\left(y ; \theta^{\prime<em _=";" _sim="\sim" _theta_="\theta)" p_y="p(y" y="y">{\theta^{\prime}=\theta}\right]\
&amp;= \mathbb{E}</em>\right|}\left[\left.\nabla_{\theta^{\prime}} \log p\left(y ; \theta^{\prime}\right) \nabla_{\theta^{\prime}} \log p\left(y ; \theta^{\prime}\right)^{T<em _=";" _sim="\sim" _theta_="\theta)" p_y="p(y" y="y">{\theta^{\prime} = \theta}\right] - 
\mathbb{E}</em>\right) \right|}\left[\left.\nabla_{\theta^{\prime}} \log p\left(y;\theta^{\prime<em _=";" _sim="\sim" _theta_="\theta)" p_y="p(y" y="y">{\theta^{\prime} = \theta}\right]
\mathbb{E}</em>\right|} \left[\left.\nabla_{\theta^{\prime}} \log p\left(y ; \theta^{\prime}\right)^{T<em _=";" _sim="\sim" _theta_="\theta)" p_y="p(y" y="y">{\theta^{\prime}=\theta}\right]\
&amp;=\mathbb{E}</em>\right|}\left[\left.\nabla_{\theta^{\prime}} \log p\left(y ; \theta^{\prime}\right) \nabla_{\theta^{\prime}} \log p\left(y ; \theta^{\prime}\right)^{T<em _=";" _sim="\sim" _theta_="\theta)" p_y="p(y" y="y">{\theta^{\prime}=\theta}\right]  -0 \
&amp;=\mathbb{E}</em>\right] \
\end{aligned}
$$}\left[\left.\nabla_{\theta^{\prime}} \log p\left(y ; \theta^{\prime}\right) \nabla_{\theta^{\prime}} \log p\left(y ; \theta^{\prime}\right)^{T}\right|_{\theta^{\prime}=\theta</p>
<h3 id="c-fisher-information-alternate-form">c)  ==Fisher Information (alternate form)==<a class="headerlink" href="#c-fisher-information-alternate-form" title="Permanent link">&para;</a></h3>
<div class="arithmatex">\[
\begin{aligned}
\mathbb{E}_{y \sim p(y ; \theta)}\left[-\left.\nabla_{\theta^{\prime}}^{2} \log p\left(y ; \theta^{\prime}\right)\right|_{\theta^{\prime}=\theta}\right]=\mathcal{I}(\theta)
\end{aligned}
\]</div>
<p><img alt="image-20220430105910930" src="https://gitee.com/violets/typora--images/raw/main/imgs/202204301059192.png" /></p>
<h3 id="d-approximating-d_kl-with-fisher-information">d) Approximating <span class="arithmatex">\(D_{KL}\)</span> with Fisher Information<a class="headerlink" href="#d-approximating-d_kl-with-fisher-information" title="Permanent link">&para;</a></h3>
<p>Make <span class="arithmatex">\(\overset \sim\theta = \theta +d\)</span>, Then Taylor expansion for <span class="arithmatex">\(\log p\left(\overset \sim\theta \right)\)</span> is 
$$
\log p\left(\overset \sim\theta \right) \approx \log p\left(\theta \right)+d^T \left.\nabla_{\theta ^\prime} \log p(\theta ^\prime) \right|<em _prime="^\prime" _theta="\theta">{\theta ^\prime = \theta}  +\frac{1}{2}d^T  \left.\nabla</em>d
$$}^2 \log p(\theta ^\prime) \right|_{\theta ^\prime = \theta</p>
<p>So we have
$$
\begin{align}
D_{\mathrm{KL}}\left(p_{\theta} | p_{\theta+d}\right) &amp;= \sum_y p_\theta \log p_\theta -\sum_y p_\theta \log p_{\theta+d}\
&amp;\approx \sum_y p_\theta \log p_\theta -\sum_y p_\theta \left(\log p\left(\theta \right)+d^T \left.\nabla_{\theta ^\prime} \log p(\theta ^\prime) \right|<em _prime="^\prime" _theta="\theta">{\theta ^\prime = \theta}  +\frac{1}{2}d^T  \left.\nabla</em>^2 \log p(\theta ^\prime) \right|<em>{\theta ^\prime = \theta}d\right) \
&amp;= -\sum_y p</em>\theta d^T \left.\nabla_{\theta ^\prime} \log p(\theta ^\prime) \right|<em>{\theta ^\prime = \theta} -\sum_y p</em>\theta  \frac{1}{2}d^T  \left.\nabla_{\theta ^\prime}^2 \log p(\theta ^\prime) \right|<em>{\theta ^\prime = \theta}d \
&amp;= - d^T \sum_y p</em>\theta \left.\nabla_{\theta ^\prime} \log p(\theta ^\prime) \right|<em>{\theta ^\prime = \theta} +  \frac{1}{2}d^T  \left(\sum_y p</em>\theta \left.\nabla_{\theta ^\prime}^2  -\log p(\theta ^\prime) \right|<em _=";" _sim="\sim" _theta_="\theta)" p_y="p(y" y="y">{\theta ^\prime = \theta} \right)d \
&amp;= 0 + \frac{1}{2}d^T \mathbb{E}</em>\right)\right|}\left[-\left.\nabla_{\theta^{\prime}}^{2} \log p\left(y ; \theta^{\prime<em _mathrm_KL="\mathrm{KL">{\theta^{\prime}=\theta}\right] d\
&amp;= \frac{1}{2} d^{T} \mathcal{I}(\theta) d \ \
D</em>(\theta) d 
\end{align}
$$}}\left(p_{\theta} | p_{\theta+d}\right) &amp;\approx  \frac{1}{2} d^{T} \mathcal{I</p>
<h3 id="e-natural-gradient">e) ==Natural Gradient==<a class="headerlink" href="#e-natural-gradient" title="Permanent link">&para;</a></h3>
<div class="arithmatex">\[
\begin{array}{l}
\ell(\theta+d) \approx \ell(\theta)+\left.d^{T} \nabla_{\theta^{\prime}} \ell\left(\theta^{\prime}\right)\right|_{\theta^{\prime}=\theta}\\
=\log p(y ; \theta)+\left.d^{T} \nabla_{\theta^{\prime}} \log p\left(y ; \theta^{\prime}\right)\right|_{\theta^{\prime}=\theta}\\
=\log p(y ; \theta)+d^{T} \frac{\left.\nabla_{\theta^{\prime}} p\left(y ; \theta^{\prime}\right)\right|_{\theta^{\prime}=\theta}}{p(y ; \theta)}\\
D_{\mathrm{KL}}\left(p_{\theta} \| p_{\theta+d}\right) \approx \frac{1}{2} d^{T} \mathcal{I}(\theta) d\\
\mathcal{L}(d, \lambda)=\ell(\theta+d)-\lambda\left[D_{\mathrm{KL}}\left(p_{\theta} \| p_{\theta+d}\right)-c\right]\\
\approx \log p(y ; \theta)+d^{T} \frac{\left.\nabla_{\theta^{\prime}} p\left(y ; \theta^{\prime}\right)\right|_{\theta^{\prime}=\theta}}{p(y ; \theta)}-\lambda\left[\frac{1}{2} d^{T} \mathcal{I}(\theta) d-c\right]\\
\nabla_{d} \mathcal{L}(d, \lambda) \approx \frac{\left.\nabla_{\theta^{\prime}} p\left(y ; \theta^{\prime}\right)\right|_{\theta^{\prime}=\theta}}{p(y ; \theta)}-\lambda \mathcal{I}(\theta) d=0\\
\tilde{d}=\frac{1}{\lambda} \mathcal{I}(\theta)^{-1} \frac{\left.\nabla_{\theta^{\prime}} p\left(y ; \theta^{\prime}\right)\right|_{\theta^{\prime}=\theta}}{p(y ; \theta)}\\
\nabla_{\lambda} \mathcal{L}(d, \lambda) \approx c-\frac{1}{2} d^{T} \mathcal{I}(\theta) d\\
=c-\frac{1}{2} \cdot \frac{1}{\lambda} \frac{\left.\nabla_{\theta^{\prime}} p\left(y ; \theta^{\prime}\right)\right|_{\theta^{\prime}=\theta}{ }^{T}}{p(y ; \theta)} \mathcal{I}(\theta)^{-1} \cdot \mathcal{I}(\theta) \cdot \frac{1}{\lambda} \mathcal{I}(\theta)^{-1} \frac{\left.\nabla_{\theta^{\prime}} p\left(y ; \theta^{\prime}\right)\right|_{\theta^{\prime}=\theta}}{p(y ; \theta)}\\
=c-\left.\left.\frac{1}{2 \lambda^{2}(p(y ; \theta))^{2}} \nabla_{\theta^{\prime}} p\left(y ; \theta^{\prime}\right)\right|_{\theta^{\prime}=\theta}{ }^{T} \mathcal{I}(\theta)^{-1} \nabla_{\theta^{\prime}} p\left(y ; \theta^{\prime}\right)\right|_{\theta^{\prime}=\theta}\\
=0\\
\lambda=\sqrt{\left.\left.\frac{1}{2 c(p(y ; \theta))^{2}} \nabla_{\theta^{\prime}} p\left(y ; \theta^{\prime}\right)\right|_{\theta^{\prime}=\theta} ^{T} \mathcal{I}(\theta)^{-1} \nabla_{\theta^{\prime}} p\left(y ; \theta^{\prime}\right)\right|_{\theta^{\prime}=\theta}}\\
d^{*}=\sqrt{\frac{2 c(p(y ; \theta))^{2}}{\left.\left.\nabla_{\theta^{\prime}} p\left(y ; \theta^{\prime}\right)\right|_{\theta^{\prime}=\theta} ^{T} \mathcal{I}(\theta)^{-1} \nabla_{\theta^{\prime}} p\left(y ; \theta^{\prime}\right)\right|_{\theta^{\prime}=\theta}}} \mathcal{I}(\theta)^{-1} \frac{\left.\nabla_{\theta^{\prime}} p\left(y ; \theta^{\prime}\right)\right|_{\theta^{\prime}=\theta}}{p(y ; \theta)}\\
=\left.\sqrt{\frac{2 c}{\left.\left.\nabla_{\theta^{\prime}} p\left(y ; \theta^{\prime}\right)\right|_{\theta^{\prime}=\theta} ^{T} \mathcal{I}(\theta)^{-1} \nabla_{\theta^{\prime}} p\left(y ; \theta^{\prime}\right)\right|_{\theta^{\prime}=\theta}}} \mathcal{I}(\theta)^{-1} \nabla_{\theta^{\prime}} p\left(y ; \theta^{\prime}\right)\right|_{\theta^{\prime}=\theta}
\end{array}
\]</div>
<h3 id="f-relation-to-newtons-method">f) ==Relation to Newton’s Method==<a class="headerlink" href="#f-relation-to-newtons-method" title="Permanent link">&para;</a></h3>
<p>For natural gradient
$$
\tilde{d}=\frac{1}{\lambda} \mathcal{I}(\theta)^{-1} \frac{\left.\nabla_{\theta^{\prime}} p\left(y ; \theta^{\prime}\right)\right|<em _theta="\theta">{\theta^{\prime}=\theta}}{p(y ; \theta)}
$$
For Newton’s Method
$$
\begin{align}
\theta: &amp; = \theta-H^{-1} \nabla</em> \ell(\theta)
\end{align}
$$
Then,
$$
\begin{aligned}
\mathcal{I}(\theta) &amp;=\mathbb{E}<em _theta_prime="\theta^{\prime">{y \sim p(y ; \theta)}\left[-\left.\nabla</em>\right)\right|}}^{2} \log p\left(y ; \theta^{\prime<em _=";" _sim="\sim" _theta_="\theta)" p_y="p(y" y="y">{\theta^{\prime}=\theta}\right] \
&amp;=\mathbb{E}</em> \ell(\theta)\right] \
&amp;=-\mathbb{E}}\left[-\nabla_{\theta}^{2<em _theta="\theta">{y \sim p(y ; \theta)}[H] \
\theta: &amp;=\theta+\tilde{d} \
&amp;=\theta+\frac{1}{\lambda} \mathcal{I}(\theta)^{-1} \nabla</em> \ell(\theta) \
&amp;=\theta-\frac{1}{\lambda} \mathbb{E}<em _theta="\theta">{y \sim p(y ; \theta)}[H]^{-1} \nabla</em> \ell(\theta)
\end{aligned}
$$</p>
<h2 id="problem-4-semi-supervised-em">Problem 4 Semi-supervised EM<a class="headerlink" href="#problem-4-semi-supervised-em" title="Permanent link">&para;</a></h2>
<h3 id="a_4">a)<a class="headerlink" href="#a_4" title="Permanent link">&para;</a></h3>
<div class="arithmatex">\[
\begin{aligned}
\ell_{\text {semi-sup }}\left(\theta^{(t+1)}\right) &amp;=\ell_{\text {unsup }}\left(\theta^{(t+1)}\right)+\alpha \ell_{\text {sup }}\left(\theta^{(t+1)}\right) \\
&amp; \geq \sum_{i=1}^{m}\left(\sum_{z^{(i)}} Q_{i}^{(t)}\left(z^{(i)}\right) \log \frac{p\left(x^{(i)}, z^{(i)} ; \theta^{(t+1)}\right)}{Q_{i}^{(t)}\left(z^{(i)}\right)}\right)+\alpha\left(\sum_{i=1}^{\tilde{m}} \log p\left(\tilde{x}^{(i)}, \tilde{z}^{(i)} ; \theta^{(t+1)}\right)\right) \\
&amp; \geq \sum_{i=1}^{m}\left(\sum_{z^{(i)}} Q_{i}^{(t)}\left(z^{(i)}\right) \log \frac{p\left(x^{(i)}, z^{(i)} ; \theta^{(t)}\right)}{Q_{i}^{(t)}\left(z^{(i)}\right)}\right)+\alpha\left(\sum_{i=1}^{\tilde{m}} \log p\left(\tilde{x}^{(i)}, \tilde{z}^{(i)} ; \theta^{(t)}\right)\right) \\
&amp;=\ell_{\text {unsup }}\left(\theta^{(t)}\right)+\alpha \ell_{\text {sup }}\left(\theta^{(t)}\right) \\
&amp;=\ell_{\text {semi-sup }}\left(\theta^{(t)}\right)
\end{aligned}
\]</div>
<h3 id="b-semi-supervised-e-step">b) Semi-supervised E-step<a class="headerlink" href="#b-semi-supervised-e-step" title="Permanent link">&para;</a></h3>
<div class="arithmatex">\[
\begin{aligned}
w_{j}^{(i)}&amp;=p\left(z^{(i)}=j \mid x^{(i)} ; \phi, \mu, \Sigma\right) \\
&amp;= \frac{p\left(x^{(i)} \mid z^{(i)}=j ; \mu, \Sigma\right) p\left(z^{(i)}=j ; \phi\right)}{\sum_{l=1}^{k} p\left(x^{(i)} \mid z^{(i)}=l ; \mu, \Sigma\right) p\left(z^{(i)}=l ; \phi\right)} \\
&amp;= \frac {\frac{1}{(2 \pi)^{n / 2}|\Sigma_j|^{1 / 2}} \exp \left(-\frac{1}{2}(x-\mu_j)^{T} \Sigma_j^{-1}(x-\mu_j)\right) \phi_j}
{\sum_{l=1}^{k} \frac {1}{(2 \pi)^{n / 2}|\Sigma_l|^{1 / 2}} \exp \left(-\frac{1}{2}(x-\mu_l)^{T} \Sigma_l^{-1}(x-\mu_l)\right) \phi_l} \\
\end{aligned}
\]</div>
<p>Appendix:
$$
\begin{aligned}
p\left(x^{(i)} \mid z^{(i)}=j ; \mu, \Sigma\right) &amp;= \frac{1}{(2 \pi)^{n / 2}|\Sigma|^{1 / 2}} \exp \left(-\frac{1}{2}(x-\mu)^{T} \Sigma^{-1}(x-\mu)\right)\
p\left(z^{(i)}=j ; \phi\right) &amp;= \phi _j
\end{aligned}
$$</p>
<h3 id="c-semi-supervised-m-step">c) ==Semi-supervised M-step==<a class="headerlink" href="#c-semi-supervised-m-step" title="Permanent link">&para;</a></h3>
<div class="arithmatex">\[
\begin{aligned}
\phi_{j} &amp; = \frac{\sum_{i = 1}^{m} w_{j}^{(i)}+\alpha \sum_{i = 1}^{\tilde{m}} \mathbb I\left\{\tilde{z}^{(i)}  = j\right\}}{m+\alpha \tilde{m}} \\
\mu_{j} &amp;=\frac{\sum_{i=1}^{m} w_{j}^{(i)} x^{(i)} + \alpha \sum_{i=1}^{\tilde m} \mathbb I\{\tilde z^{(i)} = j \} \tilde x^{(i)}}{\sum_{i=1}^{m} w_{j}^{(i)} + \alpha \sum_{i=1}^{\tilde m} \mathbb I\{\tilde z^{(i)} = j \}} \\
\Sigma_{j} &amp;=\frac{\sum_{i=1}^{m} w_{j}^{(i)}\left(x^{(i)}-\mu_{j}\right)\left(x^{(i)}-\mu_{j}\right)^{T} + \alpha \sum_{i=1}^{\tilde m} \mathbb I\{\tilde z^{(i)} = j \} \left(\tilde x^{(i)}-\mu_{j}\right)\left(\tilde x^{(i)}-\mu_{j}\right)^{T}}{\sum_{i=1}^{m} w_{j}^{(i)}+\alpha \sum_{i=1}^{\tilde m} \mathbb I\{\tilde z^{(i)} = j \}}\\ 
\end{aligned}
\]</div>
<h3 id="d-classical-unsupervised-em-implementation">d) Classical (Unsupervised) EM Implementation<a class="headerlink" href="#d-classical-unsupervised-em-implementation" title="Permanent link">&para;</a></h3>
<p><img src="https://gitee.com/violets/typora--images/raw/main/imgs/202205031506833.png" alt="image-20220503150636726" style="zoom: 50%;" /></p>
<p><img src="https://gitee.com/violets/typora--images/raw/main/imgs/202205031507505.png" alt="image-20220503150746391" style="zoom:50%;" /></p>
<p><img src="https://gitee.com/violets/typora--images/raw/main/imgs/202205031508378.png" alt="image-20220503150836263" style="zoom:50%;" /></p>
<p><strong>Initialize Dataset</strong></p>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">main</span><span class="p">(</span><span class="n">is_semi_supervised</span><span class="p">,</span> <span class="n">trial_num</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Problem 3: EM for Gaussian Mixture Models (unsupervised and semi-supervised)&quot;&quot;&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Running </span><span class="si">{}</span><span class="s1"> EM algorithm...&#39;</span>
          <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s1">&#39;semi-supervised&#39;</span> <span class="k">if</span> <span class="n">is_semi_supervised</span> <span class="k">else</span> <span class="s1">&#39;unsupervised&#39;</span><span class="p">))</span>

    <span class="c1"># Load dataset</span>
    <span class="n">train_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="s1">&#39;..&#39;</span><span class="p">,</span> <span class="s1">&#39;data&#39;</span><span class="p">,</span> <span class="s1">&#39;ds3_train.csv&#39;</span><span class="p">)</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">z</span> <span class="o">=</span> <span class="n">load_gmm_dataset</span><span class="p">(</span><span class="n">train_path</span><span class="p">)</span>
    <span class="n">x_tilde</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">if</span> <span class="n">is_semi_supervised</span><span class="p">:</span>
        <span class="c1"># Split into labeled and unlabeled examples</span>
        <span class="n">labeled_idxs</span> <span class="o">=</span> <span class="p">(</span><span class="n">z</span> <span class="o">!=</span> <span class="n">UNLABELED</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>
        <span class="n">x_tilde</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">labeled_idxs</span><span class="p">,</span> <span class="p">:]</span>   <span class="c1"># Labeled examples</span>
        <span class="n">z</span> <span class="o">=</span> <span class="n">z</span><span class="p">[</span><span class="n">labeled_idxs</span><span class="p">,</span> <span class="p">:]</span>         <span class="c1"># Corresponding labels</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="o">~</span><span class="n">labeled_idxs</span><span class="p">,</span> <span class="p">:]</span>        <span class="c1"># Unlabeled examples</span>

    <span class="c1"># *** START CODE HERE ***</span>
    <span class="c1"># (1) Initialize mu and sigma by splitting the m data points uniformly at random</span>
    <span class="c1"># into K groups, then calculating the sample mean and covariance for each group</span>
    <span class="c1"># Split data</span>
    <span class="n">m</span><span class="p">,</span> <span class="n">n</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">row_indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">permutation</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
    <span class="n">partitionLen</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">m</span> <span class="o">/</span> <span class="n">K</span><span class="p">)</span>
    <span class="c1"># initialize mu and sigma</span>
    <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">K</span><span class="p">,</span> <span class="n">n</span><span class="p">)),</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">K</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">n</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">K</span><span class="p">):</span>
        <span class="n">x_samp</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">row_indices</span><span class="p">[</span><span class="n">i</span> <span class="o">*</span> <span class="n">partitionLen</span><span class="p">:(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">partitionLen</span><span class="p">]]</span>
        <span class="n">mu</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x_samp</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">sigma</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cov</span><span class="p">(</span><span class="n">x_samp</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="c1"># (2) Initialize phi to place equal probability on each Gaussian</span>
    <span class="c1"># phi should be a numpy array of shape (K,)</span>
    <span class="n">phi</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">K</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">K</span><span class="p">)</span>
    <span class="c1"># (3) Initialize the w values to place equal probability on each Gaussian</span>
    <span class="c1"># w should be a numpy array of shape (m, K)</span>
    <span class="n">w</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">K</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">m</span><span class="p">,</span> <span class="n">K</span><span class="p">))</span>
    <span class="c1"># *** END CODE HERE ***</span>

    <span class="k">if</span> <span class="n">is_semi_supervised</span><span class="p">:</span>
        <span class="n">w</span> <span class="o">=</span> <span class="n">run_semi_supervised_em</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x_tilde</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">phi</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">w</span> <span class="o">=</span> <span class="n">run_em</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">phi</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">)</span>

    <span class="c1"># Plot your predictions</span>
    <span class="n">z_pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">w</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>  <span class="c1"># Just a placeholder for the starter code</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
            <span class="n">z_pred</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">w</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>

    <span class="n">plot_gmm_preds</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">z_pred</span><span class="p">,</span> <span class="n">is_semi_supervised</span><span class="p">,</span> <span class="n">plot_id</span><span class="o">=</span><span class="n">trial_num</span><span class="p">)</span>
</code></pre></div>
<p><strong>EM Algorithm</strong></p>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">run_em</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">phi</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Problem 3(d): EM Algorithm (unsupervised).</span>

<span class="sd">    See inline comments for instructions.</span>

<span class="sd">    Args:</span>
<span class="sd">        x: Design matrix of shape (m, n).</span>
<span class="sd">        w: Initial weight matrix of shape (m, k).</span>
<span class="sd">        phi: Initial mixture prior, of shape (k,).</span>
<span class="sd">        mu: Initial cluster means, list of k arrays of shape (n,).</span>
<span class="sd">        sigma: Initial cluster covariances, list of k arrays of shape (n, n).</span>

<span class="sd">    Returns:</span>
<span class="sd">        Updated weight matrix of shape (m, k) resulting from EM algorithm.</span>
<span class="sd">        More specifically, w[i, j] should contain the probability of</span>
<span class="sd">        example x^(i) belonging to the j-th Gaussian in the mixture.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">m</span><span class="p">,</span> <span class="n">n</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>
    <span class="c1"># No need to change any of these parameters</span>
    <span class="n">eps</span> <span class="o">=</span> <span class="mf">1e-3</span>  <span class="c1"># Convergence threshold</span>
    <span class="n">max_iter</span> <span class="o">=</span> <span class="mi">1000</span>

    <span class="c1"># Stop when the absolute change in log-likelihood is &lt; eps</span>
    <span class="c1"># See below for explanation of the convergence criterion</span>
    <span class="n">it</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">ll</span> <span class="o">=</span> <span class="n">prev_ll</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">while</span> <span class="n">it</span> <span class="o">&lt;</span> <span class="n">max_iter</span> <span class="ow">and</span> <span class="p">(</span><span class="n">prev_ll</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">ll</span> <span class="o">-</span> <span class="n">prev_ll</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="n">eps</span><span class="p">):</span>
        <span class="c1"># Just a placeholder for the starter code</span>
        <span class="c1"># *** START CODE HERE</span>
        <span class="c1"># (1) E-step: Update your estimates in w</span>
        <span class="c1"># (2) M-step: Update the model parameters phi, mu, and sigma</span>
        <span class="c1"># (3) Compute the log-likelihood of the data to check for convergence.</span>
        <span class="c1"># By log-likelihood, we mean `ll = sum_x[log(sum_z[p(x|z) * p(z)])]`.</span>
        <span class="c1"># We define convergence by the first iteration where abs(ll - prev_ll) &lt; eps.</span>
        <span class="c1"># Hint: For debugging, recall part (a). We showed that ll should be monotonically increasing.</span>

        <span class="c1"># (1) E-step:</span>
        <span class="c1"># it += 1</span>
        <span class="n">prev_ll</span> <span class="o">=</span> <span class="n">ll</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">K</span><span class="p">):</span>
            <span class="n">w</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="p">((</span><span class="n">x</span> <span class="o">-</span> <span class="n">mu</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">sigma</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span><span class="o">*</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">mu</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span><span class="o">*</span><span class="n">phi</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">det</span><span class="p">(</span><span class="n">sigma</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>
        <span class="n">ll</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">w</span><span class="p">))</span>
        <span class="n">w</span> <span class="o">/=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)[:,</span> <span class="kc">None</span><span class="p">]</span>
        <span class="c1"># (2) M-step</span>
        <span class="n">phi</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">/</span><span class="n">m</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">K</span><span class="p">):</span>
            <span class="n">mu</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w</span><span class="p">[:,</span> <span class="n">i</span><span class="p">])</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">w</span><span class="p">[:,</span> <span class="n">i</span><span class="p">])</span>
            <span class="n">sigma</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="p">((</span><span class="n">x</span><span class="o">-</span><span class="n">mu</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><span class="o">.</span><span class="n">T</span><span class="o">*</span><span class="n">w</span><span class="p">[:,</span> <span class="n">i</span><span class="p">])</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="o">-</span><span class="n">mu</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">w</span><span class="p">[:,</span> <span class="n">i</span><span class="p">])</span>
        <span class="n">it</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="c1"># *** END CODE HERE ***</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Number of iterations:</span><span class="si">{</span><span class="n">it</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">w</span>
</code></pre></div>
<h3 id="e-semi-supervised-em">e) Semi-supervised EM<a class="headerlink" href="#e-semi-supervised-em" title="Permanent link">&para;</a></h3>
<p><img src="https://gitee.com/violets/typora--images/raw/main/imgs/202205031511083.png" alt="image-20220503151118973" style="zoom:50%;" /></p>
<p><img src="https://gitee.com/violets/typora--images/raw/main/imgs/202205031512520.png" alt="image-20220503151227392" style="zoom:50%;" /></p>
<p><img src="https://gitee.com/violets/typora--images/raw/main/imgs/202205031512969.png" alt="image-20220503151245859" style="zoom:50%;" /></p>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">run_semi_supervised_em</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x_tilde</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">phi</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Problem 3(e): Semi-Supervised EM Algorithm.</span>

<span class="sd">    See inline comments for instructions.</span>

<span class="sd">    Args:</span>
<span class="sd">        x: Design matrix of unlabeled examples of shape (m, n).</span>
<span class="sd">        x_tilde: Design matrix of labeled examples of shape (m_tilde, n).</span>
<span class="sd">        z: Array of labels of shape (m_tilde, 1).</span>
<span class="sd">        w: Initial weight matrix of shape (m, k).</span>
<span class="sd">        phi: Initial mixture prior, of shape (k,).</span>
<span class="sd">        mu: Initial cluster means, list of k arrays of shape (n,).</span>
<span class="sd">        sigma: Initial cluster covariances, list of k arrays of shape (n, n).</span>

<span class="sd">    Returns:</span>
<span class="sd">        Updated weight matrix of shape (m, k) resulting from semi-supervised EM algorithm.</span>
<span class="sd">        More specifically, w[i, j] should contain the probability of</span>
<span class="sd">        example x^(i) belonging to the j-th Gaussian in the mixture.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># No need to change any of these parameters</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="mf">20.</span>  <span class="c1"># Weight for the labeled examples</span>
    <span class="n">eps</span> <span class="o">=</span> <span class="mf">1e-3</span>   <span class="c1"># Convergence threshold</span>
    <span class="n">max_iter</span> <span class="o">=</span> <span class="mi">1000</span>

    <span class="n">m</span><span class="p">,</span> <span class="n">n</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">m_tilde</span> <span class="o">=</span> <span class="n">x_tilde</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="c1"># Stop when the absolute change in log-likelihood is &lt; eps</span>
    <span class="c1"># See below for explanation of the convergence criterion</span>
    <span class="n">it</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">ll</span> <span class="o">=</span> <span class="n">prev_ll</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">while</span> <span class="n">it</span> <span class="o">&lt;</span> <span class="n">max_iter</span> <span class="ow">and</span> <span class="p">(</span><span class="n">prev_ll</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">ll</span> <span class="o">-</span> <span class="n">prev_ll</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="n">eps</span><span class="p">):</span>
        <span class="k">pass</span>  <span class="c1"># Just a placeholder for the starter code</span>
        <span class="c1"># *** START CODE HERE ***</span>
        <span class="c1"># (1) E-step: Update your estimates in w</span>
        <span class="c1"># (2) M-step: Update the model parameters phi, mu, and sigma</span>
        <span class="c1"># (3) Compute the log-likelihood of the data to check for convergence.</span>
        <span class="c1"># Hint: Make sure to include alpha in your calculation of ll.</span>
        <span class="c1"># Hint: For debugging, recall part (a). We showed that ll should be monotonically increasing.</span>
        <span class="c1"># (1) E-step:</span>
        <span class="c1"># it += 1</span>
        <span class="n">prev_ll</span> <span class="o">=</span> <span class="n">ll</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">K</span><span class="p">):</span>
            <span class="n">w</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="p">((</span><span class="n">x</span> <span class="o">-</span> <span class="n">mu</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">sigma</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">mu</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span> <span class="o">*</span> <span class="n">phi</span><span class="p">[</span>
                <span class="n">i</span><span class="p">]</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">det</span><span class="p">(</span><span class="n">sigma</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>
        <span class="n">ll</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">w</span><span class="p">))</span>
        <span class="n">w</span> <span class="o">/=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)[:,</span> <span class="kc">None</span><span class="p">]</span>
        <span class="c1"># (2) M-step</span>
        <span class="n">z_tilde_indi</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">m_tilde</span><span class="p">,</span> <span class="n">K</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">K</span><span class="p">):</span>
            <span class="n">z_tilde_indi</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mf">1.</span> <span class="k">if</span> <span class="n">z_ele</span> <span class="o">==</span> <span class="n">i</span> <span class="k">else</span> <span class="mi">0</span> <span class="k">for</span> <span class="n">z_ele</span> <span class="ow">in</span> <span class="n">z</span><span class="p">]</span>
        <span class="n">phi</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">+</span> <span class="n">alpha</span><span class="o">*</span><span class="n">z_tilde_indi</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span> <span class="o">/</span> <span class="p">(</span><span class="n">m</span><span class="o">+</span><span class="n">alpha</span><span class="o">*</span><span class="n">m_tilde</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">K</span><span class="p">):</span>
            <span class="n">mu</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w</span><span class="p">[:,</span> <span class="n">i</span><span class="p">])</span> <span class="o">+</span> <span class="n">alpha</span><span class="o">*</span><span class="n">x_tilde</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">z_tilde_indi</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]))</span> <span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">w</span><span class="p">[:,</span> <span class="n">i</span><span class="p">])</span><span class="o">+</span><span class="n">alpha</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">z_tilde_indi</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]))</span>
            <span class="n">sigma</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="p">(((</span><span class="n">x</span> <span class="o">-</span> <span class="n">mu</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><span class="o">.</span><span class="n">T</span> <span class="o">*</span> <span class="n">w</span><span class="p">[:,</span> <span class="n">i</span><span class="p">])</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">mu</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="o">+</span> <span class="n">alpha</span><span class="o">*</span><span class="p">((</span><span class="n">x_tilde</span> <span class="o">-</span> <span class="n">mu</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><span class="o">.</span><span class="n">T</span> <span class="o">*</span> <span class="n">z_tilde_indi</span><span class="p">[:,</span> <span class="n">i</span><span class="p">])</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x_tilde</span> <span class="o">-</span> <span class="n">mu</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span> \
                       <span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">w</span><span class="p">[:,</span> <span class="n">i</span><span class="p">])</span><span class="o">+</span><span class="n">alpha</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">z_tilde_indi</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]))</span>
        <span class="n">it</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Number of iterations:</span><span class="si">{</span><span class="n">it</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="c1"># *** END CODE HERE ***</span>

    <span class="k">return</span> <span class="n">w</span>
</code></pre></div>
<h3 id="f-comparison">f) Comparison<a class="headerlink" href="#f-comparison" title="Permanent link">&para;</a></h3>
<ol>
<li>
<h1 id="converge_iterations">converge_iterations<a class="headerlink" href="#converge_iterations" title="Permanent link">&para;</a></h1>
</li>
</ol>
<p><code>Running unsupervised EM algorithm...
   Number of iterations:1000
   Running semi-supervised EM algorithm...
   Number of iterations:52
   Running unsupervised EM algorithm...
   Number of iterations:1000
   Running semi-supervised EM algorithm...
   Number of iterations:58
   Running unsupervised EM algorithm...
   Number of iterations:1000
   Running semi-supervised EM algorithm...
   Number of iterations:53</code></p>
<ol>
<li>Stability</li>
</ol>
<p>~~Semi-supervised EM is perfectly stable, whereas classical EM isn't so satisfactory.~~</p>
<p>Semi-supervised EM are more stable than unsupervised EM. The assignments by unsupervised EM are random with different random initializations. But the assignments by semi-supervised EM are the same.</p>
<ol>
<li>Overall quality of assignments</li>
</ol>
<p>~~Classical EM performs poorly while high-variance Gaussian is mixed into dataset.~~</p>
<p>The overall quality of assignments by semi-supervised EM are higher than unsupervised EM. </p>
<p>In the pictures of semi-supervised EM, there are three nearly the same low-variance Gaussian distributions, and a high-variance Gaussian distribution.</p>
<p>In the pictures of unsupervised EM, there are four Gaussian distributions which variances are different.</p>
<p><strong><em>p03_gmm.py</em></strong></p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="n">PLOT_COLORS</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="s1">&#39;green&#39;</span><span class="p">,</span> <span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="s1">&#39;orange&#39;</span><span class="p">]</span>  <span class="c1"># Colors for your plots</span>
<span class="n">K</span> <span class="o">=</span> <span class="mi">4</span>           <span class="c1"># Number of Gaussians in the mixture model</span>
<span class="n">NUM_TRIALS</span> <span class="o">=</span> <span class="mi">3</span>  <span class="c1"># Number of trials to run (can be adjusted for debugging)</span>
<span class="n">UNLABELED</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>  <span class="c1"># Cluster label for unlabeled data points (do not change)</span>


<span class="k">def</span> <span class="nf">main</span><span class="p">(</span><span class="n">is_semi_supervised</span><span class="p">,</span> <span class="n">trial_num</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Problem 3: EM for Gaussian Mixture Models (unsupervised and semi-supervised)&quot;&quot;&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Running </span><span class="si">{}</span><span class="s1"> EM algorithm...&#39;</span>
          <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s1">&#39;semi-supervised&#39;</span> <span class="k">if</span> <span class="n">is_semi_supervised</span> <span class="k">else</span> <span class="s1">&#39;unsupervised&#39;</span><span class="p">))</span>

    <span class="c1"># Load dataset</span>
    <span class="n">train_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="s1">&#39;..&#39;</span><span class="p">,</span> <span class="s1">&#39;data&#39;</span><span class="p">,</span> <span class="s1">&#39;ds3_train.csv&#39;</span><span class="p">)</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">z</span> <span class="o">=</span> <span class="n">load_gmm_dataset</span><span class="p">(</span><span class="n">train_path</span><span class="p">)</span>
    <span class="n">x_tilde</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">if</span> <span class="n">is_semi_supervised</span><span class="p">:</span>
        <span class="c1"># Split into labeled and unlabeled examples</span>
        <span class="n">labeled_idxs</span> <span class="o">=</span> <span class="p">(</span><span class="n">z</span> <span class="o">!=</span> <span class="n">UNLABELED</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>
        <span class="n">x_tilde</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">labeled_idxs</span><span class="p">,</span> <span class="p">:]</span>   <span class="c1"># Labeled examples</span>
        <span class="n">z</span> <span class="o">=</span> <span class="n">z</span><span class="p">[</span><span class="n">labeled_idxs</span><span class="p">,</span> <span class="p">:]</span>         <span class="c1"># Corresponding labels</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="o">~</span><span class="n">labeled_idxs</span><span class="p">,</span> <span class="p">:]</span>        <span class="c1"># Unlabeled examples</span>

    <span class="c1"># *** START CODE HERE ***</span>
    <span class="c1"># (1) Initialize mu and sigma by splitting the m data points uniformly at random</span>
    <span class="c1"># into K groups, then calculating the sample mean and covariance for each group</span>
    <span class="c1"># Split data</span>
    <span class="n">m</span><span class="p">,</span> <span class="n">n</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">row_indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">permutation</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
    <span class="n">partitionLen</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">m</span> <span class="o">/</span> <span class="n">K</span><span class="p">)</span>
    <span class="c1"># initialize mu and sigma</span>
    <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">K</span><span class="p">,</span> <span class="n">n</span><span class="p">)),</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">K</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">n</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">K</span><span class="p">):</span>
        <span class="n">x_samp</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">row_indices</span><span class="p">[</span><span class="n">i</span> <span class="o">*</span> <span class="n">partitionLen</span><span class="p">:(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">partitionLen</span><span class="p">]]</span>
        <span class="n">mu</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x_samp</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">sigma</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cov</span><span class="p">(</span><span class="n">x_samp</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="c1"># (2) Initialize phi to place equal probability on each Gaussian</span>
    <span class="c1"># phi should be a numpy array of shape (K,)</span>
    <span class="n">phi</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">K</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">K</span><span class="p">)</span>
    <span class="c1"># (3) Initialize the w values to place equal probability on each Gaussian</span>
    <span class="c1"># w should be a numpy array of shape (m, K)</span>
    <span class="n">w</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">K</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">m</span><span class="p">,</span> <span class="n">K</span><span class="p">))</span>
    <span class="c1"># *** END CODE HERE ***</span>

    <span class="k">if</span> <span class="n">is_semi_supervised</span><span class="p">:</span>
        <span class="n">w</span> <span class="o">=</span> <span class="n">run_semi_supervised_em</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x_tilde</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">phi</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">w</span> <span class="o">=</span> <span class="n">run_em</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">phi</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">)</span>

    <span class="c1"># Plot your predictions</span>
    <span class="n">z_pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">w</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>  <span class="c1"># Just a placeholder for the starter code</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
            <span class="n">z_pred</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">w</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>

    <span class="n">plot_gmm_preds</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">z_pred</span><span class="p">,</span> <span class="n">is_semi_supervised</span><span class="p">,</span> <span class="n">plot_id</span><span class="o">=</span><span class="n">trial_num</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">run_em</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">phi</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Problem 3(d): EM Algorithm (unsupervised).</span>

<span class="sd">    See inline comments for instructions.</span>

<span class="sd">    Args:</span>
<span class="sd">        x: Design matrix of shape (m, n).</span>
<span class="sd">        w: Initial weight matrix of shape (m, k).</span>
<span class="sd">        phi: Initial mixture prior, of shape (k,).</span>
<span class="sd">        mu: Initial cluster means, list of k arrays of shape (n,).</span>
<span class="sd">        sigma: Initial cluster covariances, list of k arrays of shape (n, n).</span>

<span class="sd">    Returns:</span>
<span class="sd">        Updated weight matrix of shape (m, k) resulting from EM algorithm.</span>
<span class="sd">        More specifically, w[i, j] should contain the probability of</span>
<span class="sd">        example x^(i) belonging to the j-th Gaussian in the mixture.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">m</span><span class="p">,</span> <span class="n">n</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>
    <span class="c1"># No need to change any of these parameters</span>
    <span class="n">eps</span> <span class="o">=</span> <span class="mf">1e-3</span>  <span class="c1"># Convergence threshold</span>
    <span class="n">max_iter</span> <span class="o">=</span> <span class="mi">1000</span>

    <span class="c1"># Stop when the absolute change in log-likelihood is &lt; eps</span>
    <span class="c1"># See below for explanation of the convergence criterion</span>
    <span class="n">it</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">ll</span> <span class="o">=</span> <span class="n">prev_ll</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">while</span> <span class="n">it</span> <span class="o">&lt;</span> <span class="n">max_iter</span> <span class="ow">and</span> <span class="p">(</span><span class="n">prev_ll</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">ll</span> <span class="o">-</span> <span class="n">prev_ll</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="n">eps</span><span class="p">):</span>
        <span class="c1"># Just a placeholder for the starter code</span>
        <span class="c1"># *** START CODE HERE</span>
        <span class="c1"># (1) E-step: Update your estimates in w</span>
        <span class="c1"># (2) M-step: Update the model parameters phi, mu, and sigma</span>
        <span class="c1"># (3) Compute the log-likelihood of the data to check for convergence.</span>
        <span class="c1"># By log-likelihood, we mean `ll = sum_x[log(sum_z[p(x|z) * p(z)])]`.</span>
        <span class="c1"># We define convergence by the first iteration where abs(ll - prev_ll) &lt; eps.</span>
        <span class="c1"># Hint: For debugging, recall part (a). We showed that ll should be monotonically increasing.</span>

        <span class="c1"># (1) E-step:</span>
        <span class="c1"># it += 1</span>
        <span class="n">prev_ll</span> <span class="o">=</span> <span class="n">ll</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">K</span><span class="p">):</span>
            <span class="n">w</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="p">((</span><span class="n">x</span> <span class="o">-</span> <span class="n">mu</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">sigma</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span><span class="o">*</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">mu</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span><span class="o">*</span><span class="n">phi</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">det</span><span class="p">(</span><span class="n">sigma</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>
        <span class="n">ll</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">w</span><span class="p">))</span>
        <span class="n">w</span> <span class="o">/=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)[:,</span> <span class="kc">None</span><span class="p">]</span>
        <span class="c1"># (2) M-step</span>
        <span class="n">phi</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">/</span><span class="n">m</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">K</span><span class="p">):</span>
            <span class="n">mu</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w</span><span class="p">[:,</span> <span class="n">i</span><span class="p">])</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">w</span><span class="p">[:,</span> <span class="n">i</span><span class="p">])</span>
            <span class="n">sigma</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="p">((</span><span class="n">x</span><span class="o">-</span><span class="n">mu</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><span class="o">.</span><span class="n">T</span><span class="o">*</span><span class="n">w</span><span class="p">[:,</span> <span class="n">i</span><span class="p">])</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="o">-</span><span class="n">mu</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">w</span><span class="p">[:,</span> <span class="n">i</span><span class="p">])</span>
        <span class="n">it</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="c1"># *** END CODE HERE ***</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Number of iterations:</span><span class="si">{</span><span class="n">it</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">w</span>


<span class="k">def</span> <span class="nf">run_semi_supervised_em</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x_tilde</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">phi</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Problem 3(e): Semi-Supervised EM Algorithm.</span>

<span class="sd">    See inline comments for instructions.</span>

<span class="sd">    Args:</span>
<span class="sd">        x: Design matrix of unlabeled examples of shape (m, n).</span>
<span class="sd">        x_tilde: Design matrix of labeled examples of shape (m_tilde, n).</span>
<span class="sd">        z: Array of labels of shape (m_tilde, 1).</span>
<span class="sd">        w: Initial weight matrix of shape (m, k).</span>
<span class="sd">        phi: Initial mixture prior, of shape (k,).</span>
<span class="sd">        mu: Initial cluster means, list of k arrays of shape (n,).</span>
<span class="sd">        sigma: Initial cluster covariances, list of k arrays of shape (n, n).</span>

<span class="sd">    Returns:</span>
<span class="sd">        Updated weight matrix of shape (m, k) resulting from semi-supervised EM algorithm.</span>
<span class="sd">        More specifically, w[i, j] should contain the probability of</span>
<span class="sd">        example x^(i) belonging to the j-th Gaussian in the mixture.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># No need to change any of these parameters</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="mf">20.</span>  <span class="c1"># Weight for the labeled examples</span>
    <span class="n">eps</span> <span class="o">=</span> <span class="mf">1e-3</span>   <span class="c1"># Convergence threshold</span>
    <span class="n">max_iter</span> <span class="o">=</span> <span class="mi">1000</span>

    <span class="n">m</span><span class="p">,</span> <span class="n">n</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">m_tilde</span> <span class="o">=</span> <span class="n">x_tilde</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="c1"># Stop when the absolute change in log-likelihood is &lt; eps</span>
    <span class="c1"># See below for explanation of the convergence criterion</span>
    <span class="n">it</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">ll</span> <span class="o">=</span> <span class="n">prev_ll</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">while</span> <span class="n">it</span> <span class="o">&lt;</span> <span class="n">max_iter</span> <span class="ow">and</span> <span class="p">(</span><span class="n">prev_ll</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">ll</span> <span class="o">-</span> <span class="n">prev_ll</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="n">eps</span><span class="p">):</span>
        <span class="k">pass</span>  <span class="c1"># Just a placeholder for the starter code</span>
        <span class="c1"># *** START CODE HERE ***</span>
        <span class="c1"># (1) E-step: Update your estimates in w</span>
        <span class="c1"># (2) M-step: Update the model parameters phi, mu, and sigma</span>
        <span class="c1"># (3) Compute the log-likelihood of the data to check for convergence.</span>
        <span class="c1"># Hint: Make sure to include alpha in your calculation of ll.</span>
        <span class="c1"># Hint: For debugging, recall part (a). We showed that ll should be monotonically increasing.</span>
        <span class="c1"># (1) E-step:</span>
        <span class="c1"># it += 1</span>
        <span class="n">prev_ll</span> <span class="o">=</span> <span class="n">ll</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">K</span><span class="p">):</span>
            <span class="n">w</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="p">((</span><span class="n">x</span> <span class="o">-</span> <span class="n">mu</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">sigma</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">mu</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span> <span class="o">*</span> <span class="n">phi</span><span class="p">[</span>
                <span class="n">i</span><span class="p">]</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">det</span><span class="p">(</span><span class="n">sigma</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>
        <span class="n">ll</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">w</span><span class="p">))</span>
        <span class="n">w</span> <span class="o">/=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)[:,</span> <span class="kc">None</span><span class="p">]</span>
        <span class="c1"># (2) M-step</span>
        <span class="n">z_tilde_indi</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">m_tilde</span><span class="p">,</span> <span class="n">K</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">K</span><span class="p">):</span>
            <span class="n">z_tilde_indi</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mf">1.</span> <span class="k">if</span> <span class="n">z_ele</span> <span class="o">==</span> <span class="n">i</span> <span class="k">else</span> <span class="mi">0</span> <span class="k">for</span> <span class="n">z_ele</span> <span class="ow">in</span> <span class="n">z</span><span class="p">]</span>
        <span class="n">phi</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">+</span> <span class="n">alpha</span><span class="o">*</span><span class="n">z_tilde_indi</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span> <span class="o">/</span> <span class="p">(</span><span class="n">m</span><span class="o">+</span><span class="n">alpha</span><span class="o">*</span><span class="n">m_tilde</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">K</span><span class="p">):</span>
            <span class="n">mu</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w</span><span class="p">[:,</span> <span class="n">i</span><span class="p">])</span> <span class="o">+</span> <span class="n">alpha</span><span class="o">*</span><span class="n">x_tilde</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">z_tilde_indi</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]))</span> <span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">w</span><span class="p">[:,</span> <span class="n">i</span><span class="p">])</span><span class="o">+</span><span class="n">alpha</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">z_tilde_indi</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]))</span>
            <span class="n">sigma</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="p">(((</span><span class="n">x</span> <span class="o">-</span> <span class="n">mu</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><span class="o">.</span><span class="n">T</span> <span class="o">*</span> <span class="n">w</span><span class="p">[:,</span> <span class="n">i</span><span class="p">])</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">mu</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="o">+</span> <span class="n">alpha</span><span class="o">*</span><span class="p">((</span><span class="n">x_tilde</span> <span class="o">-</span> <span class="n">mu</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><span class="o">.</span><span class="n">T</span> <span class="o">*</span> <span class="n">z_tilde_indi</span><span class="p">[:,</span> <span class="n">i</span><span class="p">])</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x_tilde</span> <span class="o">-</span> <span class="n">mu</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span> \
                       <span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">w</span><span class="p">[:,</span> <span class="n">i</span><span class="p">])</span><span class="o">+</span><span class="n">alpha</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">z_tilde_indi</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]))</span>
        <span class="n">it</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Number of iterations:</span><span class="si">{</span><span class="n">it</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="c1"># *** END CODE HERE ***</span>

    <span class="k">return</span> <span class="n">w</span>


<span class="c1"># *** START CODE HERE ***</span>
<span class="c1"># Helper functions</span>
<span class="c1"># *** END CODE HERE ***</span>


<span class="k">def</span> <span class="nf">plot_gmm_preds</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">with_supervision</span><span class="p">,</span> <span class="n">plot_id</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Plot GMM predictions on a 2D dataset `x` with labels `z`.</span>

<span class="sd">    Write to the output directory, including `plot_id`</span>
<span class="sd">    in the name, and appending &#39;ss&#39; if the GMM had supervision.</span>

<span class="sd">    NOTE: You do not need to edit this function.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;</span><span class="si">{}</span><span class="s1"> GMM Predictions&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s1">&#39;Semi-supervised&#39;</span> <span class="k">if</span> <span class="n">with_supervision</span> <span class="k">else</span> <span class="s1">&#39;Unsupervised&#39;</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;x_1&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;x_2&#39;</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">x_1</span><span class="p">,</span> <span class="n">x_2</span><span class="p">,</span> <span class="n">z_</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">z</span><span class="p">):</span>
        <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;gray&#39;</span> <span class="k">if</span> <span class="n">z_</span> <span class="o">&lt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="n">PLOT_COLORS</span><span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">z_</span><span class="p">)]</span>
        <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.25</span> <span class="k">if</span> <span class="n">z_</span> <span class="o">&lt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="mf">0.75</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_1</span><span class="p">,</span> <span class="n">x_2</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;.&#39;</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">)</span>

    <span class="n">file_name</span> <span class="o">=</span> <span class="s1">&#39;p03_pred</span><span class="si">{}</span><span class="s1">_</span><span class="si">{}</span><span class="s1">.pdf&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s1">&#39;_ss&#39;</span> <span class="k">if</span> <span class="n">with_supervision</span> <span class="k">else</span> <span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">plot_id</span><span class="p">)</span>
    <span class="n">save_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="s1">&#39;output&#39;</span><span class="p">,</span> <span class="n">file_name</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="n">save_path</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">load_gmm_dataset</span><span class="p">(</span><span class="n">csv_path</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Load dataset for Gaussian Mixture Model (problem 3).</span>

<span class="sd">    Args:</span>
<span class="sd">         csv_path: Path to CSV file containing dataset.</span>

<span class="sd">    Returns:</span>
<span class="sd">        x: NumPy array shape (m, n)</span>
<span class="sd">        z: NumPy array shape (m, 1)</span>

<span class="sd">    NOTE: You do not need to edit this function.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># Load headers</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">csv_path</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">csv_fh</span><span class="p">:</span>
        <span class="n">headers</span> <span class="o">=</span> <span class="n">csv_fh</span><span class="o">.</span><span class="n">readline</span><span class="p">()</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;,&#39;</span><span class="p">)</span>

    <span class="c1"># Load features and labels</span>
    <span class="n">x_cols</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">headers</span><span class="p">))</span> <span class="k">if</span> <span class="n">headers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)]</span>
    <span class="n">z_cols</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">headers</span><span class="p">))</span> <span class="k">if</span> <span class="n">headers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">==</span> <span class="s1">&#39;z&#39;</span><span class="p">]</span>

    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">loadtxt</span><span class="p">(</span><span class="n">csv_path</span><span class="p">,</span> <span class="n">delimiter</span><span class="o">=</span><span class="s1">&#39;,&#39;</span><span class="p">,</span> <span class="n">skiprows</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">usecols</span><span class="o">=</span><span class="n">x_cols</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">loadtxt</span><span class="p">(</span><span class="n">csv_path</span><span class="p">,</span> <span class="n">delimiter</span><span class="o">=</span><span class="s1">&#39;,&#39;</span><span class="p">,</span> <span class="n">skiprows</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">usecols</span><span class="o">=</span><span class="n">z_cols</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">z</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="n">z</span>


<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">229</span><span class="p">)</span>
    <span class="c1"># Run NUM_TRIALS trials to see how different initializations</span>
    <span class="c1"># affect the final predictions with and without supervision</span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">NUM_TRIALS</span><span class="p">):</span>
        <span class="n">main</span><span class="p">(</span><span class="n">is_semi_supervised</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">trial_num</span><span class="o">=</span><span class="n">t</span><span class="p">)</span>

        <span class="c1"># *** START CODE HERE ***</span>
        <span class="c1"># Once you&#39;ve implemented the semi-supervised version,</span>
        <span class="c1"># uncomment the following line.</span>
        <span class="c1"># You do not need to add any other lines in this code block.</span>
        <span class="c1"># main(with_supervision=True, trial_num=t)</span>
        <span class="n">main</span><span class="p">(</span><span class="n">is_semi_supervised</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">trial_num</span><span class="o">=</span><span class="n">t</span><span class="p">)</span>
        <span class="c1"># *** END CODE HERE ***</span>
</code></pre></div>
<h2 id="problem-5-k-means-for-compression">Problem 5 K-means for compression<a class="headerlink" href="#problem-5-k-means-for-compression" title="Permanent link">&para;</a></h2>
<h3 id="a-k-means-compression-implementation">a) K-Means Compression Implementation<a class="headerlink" href="#a-k-means-compression-implementation" title="Permanent link">&para;</a></h3>
<p><img src="https://gitee.com/violets/typora--images/raw/main/imgs/202205032141519.png" alt="image-20220503214107405" style="zoom: 50%;" /></p>
<p><center>compressed image</center></p>
<h3 id="b-compression-factor">b) Compression factor<a class="headerlink" href="#b-compression-factor" title="Permanent link">&para;</a></h3>
<p>~~Compression factor is approximately <code>2</code>~~</p>
<p>In the original image, we need 3 8=24 bits to represent a pixel. </p>
<p>In the compressed image, we only need 4 bits (16 colors) to represent a pixel. </p>
<p>So the image are compressed by factor 6.</p>
<p><em>p05_kmeans.py</em></p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">matplotlib.image</span> <span class="kn">import</span> <span class="n">imread</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">K</span> <span class="o">=</span> <span class="mi">16</span>


<span class="c1"># Helper functions</span>
<span class="k">def</span> <span class="nf">rgb_img_vectorization</span><span class="p">(</span><span class="n">img_matrix</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>

<span class="sd">    Args:</span>
<span class="sd">        img_matrix: a list(size: [l,l,3]) represent a square image</span>

<span class="sd">    Returns: vectorized numpy array (size : [l**2,3] ) represent the image</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">img_matrix</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">img_matrix</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>


<span class="c1"># K-means</span>
<span class="k">def</span> <span class="nf">k_means_rgbimg</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mu</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compress an image (use K colors instead).</span>
<span class="sd">    Args:</span>
<span class="sd">        x: unlabelled dataset of size (m,3)</span>
<span class="sd">        mu: cluster centroids size (K, 3)</span>

<span class="sd">    Returns: updated mu</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">m</span><span class="p">,</span> <span class="n">n</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">k</span> <span class="o">=</span> <span class="n">mu</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">max_iter</span> <span class="o">=</span> <span class="mi">1000</span>

    <span class="n">prev_c</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">it</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">norm_mu_x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">m</span><span class="p">,</span> <span class="n">k</span><span class="p">))</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
    <span class="k">while</span> <span class="n">it</span> <span class="o">&lt;</span> <span class="n">max_iter</span> <span class="ow">and</span> <span class="p">(</span><span class="ow">not</span> <span class="p">(</span><span class="n">prev_c</span> <span class="o">==</span> <span class="n">c</span><span class="p">)</span><span class="o">.</span><span class="n">all</span><span class="p">()):</span>
        <span class="n">prev_c</span> <span class="o">=</span> <span class="n">c</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">K</span><span class="p">):</span>
            <span class="n">norm_mu_x</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">mu</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">c</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">norm_mu_x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">c_indi</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">m</span><span class="p">,</span> <span class="n">K</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">K</span><span class="p">):</span>
            <span class="n">c_indi</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mf">1.</span> <span class="k">if</span> <span class="n">c_ele</span> <span class="o">==</span> <span class="n">i</span> <span class="k">else</span> <span class="mi">0</span> <span class="k">for</span> <span class="n">c_ele</span> <span class="ow">in</span> <span class="n">c</span><span class="p">]</span>
            <span class="n">mu</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">c_indi</span><span class="p">[:,</span> <span class="n">i</span><span class="p">])</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">c_indi</span><span class="p">[:,</span> <span class="n">i</span><span class="p">])</span>
        <span class="n">it</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Number of iterations:</span><span class="si">{</span><span class="n">it</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="n">new_img</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
        <span class="n">new_img</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">mu</span><span class="p">[</span><span class="n">c</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span>
    <span class="n">img_array</span> <span class="o">=</span> <span class="n">new_img</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">m</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)),</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">m</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)),</span> <span class="mi">3</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img_array</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
    <span class="k">return</span> <span class="kc">None</span>


<span class="c1"># Initialize mu to randomly chosen pixel in the image</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">imread</span><span class="p">(</span><span class="s1">&#39;../data/peppers-large.tiff&#39;</span><span class="p">)</span>
<span class="n">mu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">K</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="n">x_t</span> <span class="o">=</span> <span class="n">rgb_img_vectorization</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>

<span class="n">idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">A</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">K</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">K</span><span class="p">):</span>
    <span class="n">mu</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">x_t</span><span class="p">[</span><span class="n">idx</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="p">:]</span>

<span class="n">k_means_rgbimg</span><span class="p">(</span><span class="n">x_t</span><span class="p">,</span> <span class="n">mu</span><span class="p">)</span>
</code></pre></div>
<h1 id="ps-4">PS 4<a class="headerlink" href="#ps-4" title="Permanent link">&para;</a></h1>
<h2 id="problem-1-cnn-for-mnist">Problem 1 CNN for MNIST<a class="headerlink" href="#problem-1-cnn-for-mnist" title="Permanent link">&para;</a></h2>
<h3 id="a-backward-functions">a) Backward functions<a class="headerlink" href="#a-backward-functions" title="Permanent link">&para;</a></h3>
<hr />
<p><strong>FYI :</strong></p>
<p><a href="https://e2eml.school/softmax.html">Softmax in neural network</a></p>
<ul>
<li>Softmax is responsible for properly backpropagating the loss gradient so that upstream layers can learn from it.</li>
</ul>
<hr />
<p><strong>backward softmax</strong></p>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">backward_softmax</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">grad_outputs</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute the gradient of the loss with respect to x.</span>

<span class="sd">    grad_outputs is the gradient of the loss with respect to the outputs of the softmax.</span>

<span class="sd">    Args:</span>
<span class="sd">        x: A 1d numpy float array of shape number_of_classes</span>
<span class="sd">        grad_outputs: A 1d numpy float array of shape number_of_classes</span>

<span class="sd">    Returns:</span>
<span class="sd">        A 1d numpy float array of the same shape as x with the derivative of the loss with respect to x</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># *** START CODE HERE ***</span>
    <span class="c1"># FYI: the input gradient is the output gradient multiplied by the softmax derivative</span>
    <span class="c1"># Calculate Softmax derivative</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">forward_softmax</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">soft_deri</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">outer</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">s</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">soft_deri</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">grad_outputs</span><span class="p">)</span>
    <span class="c1"># *** END CODE HERE ***</span>
</code></pre></div>
<p><strong>backward ReLU</strong></p>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">backward_relu</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">grad_outputs</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute the gradient of the loss with respect to x</span>

<span class="sd">    Args:</span>
<span class="sd">        x: A numpy array of arbitrary shape containing the input.</span>
<span class="sd">        grad_outputs: A numpy array of the same shape of x containing the gradient of the loss with respect</span>
<span class="sd">            to the output of relu</span>

<span class="sd">    Returns:</span>
<span class="sd">        A numpy array of the same shape as x containing the gradients with respect to x.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># *** START CODE HERE ***</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">x</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span> <span class="o">*</span> <span class="n">grad_outputs</span>
    <span class="c1"># *** END CODE HERE ***</span>
</code></pre></div>
<p><strong>backward cross entropy loss</strong></p>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">backward_cross_entropy_loss</span><span class="p">(</span><span class="n">probabilities</span><span class="p">,</span> <span class="n">labels</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute the gradient of the cross entropy loss with respect to the probabilities.</span>

<span class="sd">    probabilities is of the shape (# classes)</span>
<span class="sd">    labels is of the shape (# classes)</span>

<span class="sd">    The output should be the gradient with respect to the probabilities.</span>

<span class="sd">    Returns:</span>
<span class="sd">        The gradient of the loss with respect to the probabilities.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># *** START CODE HERE ***</span>
    <span class="c1"># loss_grad = np.zeros(probabilities.shape)</span>
    <span class="c1"># for i, label in enumerate(labels):</span>
    <span class="c1">#     if label == 1:</span>
    <span class="c1">#         loss_grad[i] = -1/probabilities[i]</span>
    <span class="c1"># return loss_grad</span>
    <span class="k">return</span> <span class="o">-</span> <span class="n">labels</span><span class="o">/</span><span class="n">probabilities</span>
    <span class="c1"># *** END CODE HERE ***</span>
</code></pre></div>
<p><strong>backward linear</strong></p>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">backward_linear</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">output_grad</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute the gradients of the loss with respect to the parameters of a linear layer.</span>

<span class="sd">    See forward_linear for information about the shapes of the variables.</span>

<span class="sd">    output_grad is the gradient of the loss with respect to the output of this layer.</span>

<span class="sd">    This should return a tuple with three elements:</span>
<span class="sd">    - The gradient of the loss with respect to the weights</span>
<span class="sd">    - The gradient of the loss with respect to the bias</span>
<span class="sd">    - The gradient of the loss with respect to the data</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># *** START CODE HERE ***</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">outer</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">output_grad</span><span class="p">),</span> <span class="n">output_grad</span><span class="p">,</span> <span class="n">weights</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">output_grad</span><span class="p">)</span>
    <span class="c1"># *** END CODE HERE ***</span>
</code></pre></div>
<p><strong>backward convolution</strong></p>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">backward_convolution</span><span class="p">(</span><span class="n">conv_W</span><span class="p">,</span> <span class="n">conv_b</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">output_grad</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute the gradient of the loss with respect to the parameters of the convolution.</span>

<span class="sd">    See forward_convolution for the sizes of the arguments.</span>
<span class="sd">    output_grad is the gradient of the loss with respect to the output of the convolution.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A tuple containing 3 gradients.</span>
<span class="sd">        The first element is the gradient of the loss with respect to the convolution weights</span>
<span class="sd">        The second element is the gradient of the loss with respect to the convolution bias</span>
<span class="sd">        The third element is the gradient of the loss with respect to the input data</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># *** START CODE HERE ***</span>
    <span class="n">conv_channels</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">conv_width</span><span class="p">,</span> <span class="n">conv_height</span> <span class="o">=</span> <span class="n">conv_W</span><span class="o">.</span><span class="n">shape</span>

    <span class="n">input_channels</span><span class="p">,</span> <span class="n">input_width</span><span class="p">,</span> <span class="n">input_height</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">shape</span>

    <span class="n">grad_bias</span> <span class="o">=</span> <span class="n">output_grad</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
    <span class="n">grad_weight</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">conv_W</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">grad_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">input_width</span> <span class="o">-</span> <span class="n">conv_width</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">input_height</span> <span class="o">-</span> <span class="n">conv_height</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">output_channel</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">conv_channels</span><span class="p">):</span>
                <span class="n">grad_weight</span><span class="p">[</span><span class="n">output_channel</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="p">:]</span> <span class="o">+=</span> <span class="n">data</span><span class="p">[:,</span> <span class="n">x</span><span class="p">:(</span><span class="n">x</span> <span class="o">+</span> <span class="n">conv_width</span><span class="p">),</span> <span class="n">y</span><span class="p">:(</span><span class="n">y</span> <span class="o">+</span> <span class="n">conv_height</span><span class="p">)]</span> <span class="o">*</span> <span class="n">output_grad</span><span class="p">[</span><span class="n">output_channel</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">]</span>
                <span class="n">grad_data</span><span class="p">[:,</span> <span class="n">x</span><span class="p">:(</span><span class="n">x</span> <span class="o">+</span> <span class="n">conv_width</span><span class="p">),</span> <span class="n">y</span><span class="p">:(</span><span class="n">y</span> <span class="o">+</span> <span class="n">conv_height</span><span class="p">)]</span> <span class="o">+=</span> <span class="n">conv_W</span><span class="p">[</span><span class="n">output_channel</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="p">:]</span> <span class="o">*</span> <span class="n">output_grad</span><span class="p">[</span><span class="n">output_channel</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">]</span>

    <span class="k">return</span> <span class="n">grad_weight</span><span class="p">,</span> <span class="n">grad_bias</span><span class="p">,</span> <span class="n">grad_data</span>
    <span class="c1"># *** END CODE HERE ***</span>
</code></pre></div>
<p><strong>backward max pool</strong></p>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">backward_max_pool</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">pool_width</span><span class="p">,</span> <span class="n">pool_height</span><span class="p">,</span> <span class="n">output_grad</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute the gradient of the loss with respect to the data in the max pooling layer.</span>

<span class="sd">    data is of the shape (# channels, width, height)</span>
<span class="sd">    output_grad is of shape (# channels, width // pool_width, height // pool_height)</span>

<span class="sd">    output_grad is the gradient of the loss with respect to the output of the backward max</span>
<span class="sd">    pool layer.</span>

<span class="sd">    Returns:</span>
<span class="sd">        The gradient of the loss with respect to the data (of same shape as data)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># *** START CODE HERE ***</span>
    <span class="n">input_channels</span><span class="p">,</span> <span class="n">input_width</span><span class="p">,</span> <span class="n">input_height</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">grad_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">input_channels</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">input_width</span><span class="p">,</span> <span class="n">pool_width</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">input_height</span><span class="p">,</span> <span class="n">pool_height</span><span class="p">):</span>
                <span class="c1"># Need the index of the max entry</span>
                <span class="c1"># Solution 1</span>
                <span class="n">temp_index</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">x</span><span class="p">:(</span><span class="n">x</span> <span class="o">+</span> <span class="n">pool_width</span><span class="p">),</span> <span class="n">y</span><span class="p">:(</span><span class="n">y</span> <span class="o">+</span> <span class="n">pool_height</span><span class="p">)]</span><span class="o">.</span><span class="n">argmax</span><span class="p">()</span>
                <span class="n">grad_data</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">x</span><span class="p">:(</span><span class="n">x</span> <span class="o">+</span> <span class="n">pool_width</span><span class="p">),</span> <span class="n">y</span><span class="p">:(</span><span class="n">y</span> <span class="o">+</span> <span class="n">pool_height</span><span class="p">)]</span><span class="o">.</span><span class="n">flat</span><span class="p">[</span><span class="n">temp_index</span><span class="p">]</span> <span class="o">+=</span> <span class="n">output_grad</span><span class="p">[</span>
                    <span class="n">i</span><span class="p">,</span> <span class="n">x</span> <span class="o">//</span> <span class="n">pool_width</span><span class="p">,</span> <span class="n">y</span> <span class="o">//</span> <span class="n">pool_height</span><span class="p">]</span>
                <span class="c1"># Solution 2</span>
                <span class="c1"># temp_matrix = data[i, x:(x + pool_width), y:(y + pool_height)]</span>
                <span class="c1"># grad_data[i, x:(x + pool_width), y:(y + pool_height)][i, np.unravel_index(temp_matrix.argmax(), temp_matrix.shape)] += output_grad[</span>
                <span class="c1">#     i, x // pool_width, y // pool_height]</span>

    <span class="k">return</span> <span class="n">grad_data</span>
    <span class="c1"># *** END CODE HERE ***</span>
</code></pre></div>
<h3 id="b-backward-propagation">b) Backward Propagation<a class="headerlink" href="#b-backward-propagation" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">backward_prop</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Implement the backward propagation gradient computation step for a neural network</span>

<span class="sd">    Args:</span>
<span class="sd">        data: A numpy array containing the input for a single example</span>
<span class="sd">        labels: A 1d numpy array containing the labels for a single example</span>
<span class="sd">        params: A dictionary mapping parameter names to numpy arrays with the parameters.</span>
<span class="sd">            This numpy array will contain W1, b1, W2, and b2</span>
<span class="sd">            W1 and b1 represent the weights and bias for the convolutional layer</span>
<span class="sd">            W2 and b2 represent the weights and bias for the output layer of the network</span>

<span class="sd">    Returns:</span>
<span class="sd">        A dictionary of strings to numpy arrays where each key represents the name of a weight</span>
<span class="sd">        and the values represent the gradient of the loss with respect to that weight.</span>

<span class="sd">        In particular, it should have 4 elements:</span>
<span class="sd">            W1, W2, b1, and b2</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># *** START CODE HERE ***</span>
    <span class="n">W1</span> <span class="o">=</span> <span class="n">params</span><span class="p">[</span><span class="s1">&#39;W1&#39;</span><span class="p">]</span>
    <span class="n">b1</span> <span class="o">=</span> <span class="n">params</span><span class="p">[</span><span class="s1">&#39;b1&#39;</span><span class="p">]</span>
    <span class="n">W2</span> <span class="o">=</span> <span class="n">params</span><span class="p">[</span><span class="s1">&#39;W2&#39;</span><span class="p">]</span>
    <span class="n">b2</span> <span class="o">=</span> <span class="n">params</span><span class="p">[</span><span class="s1">&#39;b2&#39;</span><span class="p">]</span>

    <span class="n">first_convolution</span> <span class="o">=</span> <span class="n">forward_convolution</span><span class="p">(</span><span class="n">W1</span><span class="p">,</span> <span class="n">b1</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span>
    <span class="n">first_max_pool</span> <span class="o">=</span> <span class="n">forward_max_pool</span><span class="p">(</span><span class="n">first_convolution</span><span class="p">,</span> <span class="n">MAX_POOL_SIZE</span><span class="p">,</span> <span class="n">MAX_POOL_SIZE</span><span class="p">)</span>
    <span class="n">first_after_relu</span> <span class="o">=</span> <span class="n">forward_relu</span><span class="p">(</span><span class="n">first_max_pool</span><span class="p">)</span>
    <span class="n">flattened</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">first_after_relu</span><span class="p">,</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">forward_linear</span><span class="p">(</span><span class="n">W2</span><span class="p">,</span> <span class="n">b2</span><span class="p">,</span> <span class="n">flattened</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">forward_softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>

    <span class="n">dc_dp</span> <span class="o">=</span> <span class="n">backward_cross_entropy_loss</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
    <span class="n">d_softmax</span> <span class="o">=</span> <span class="n">backward_softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dc_dp</span><span class="p">)</span>
    <span class="n">d_W2</span><span class="p">,</span> <span class="n">d_b2</span><span class="p">,</span> <span class="n">dlin_dlinear</span> <span class="o">=</span> <span class="n">backward_linear</span><span class="p">(</span><span class="n">W2</span><span class="p">,</span> <span class="n">b2</span><span class="p">,</span> <span class="n">flattened</span><span class="p">,</span> <span class="n">d_softmax</span><span class="p">)</span>
    <span class="n">d_relu</span> <span class="o">=</span> <span class="n">backward_relu</span><span class="p">(</span><span class="n">first_max_pool</span><span class="p">,</span> <span class="n">dlin_dlinear</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">first_max_pool</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
    <span class="n">d_maxpool</span> <span class="o">=</span> <span class="n">backward_max_pool</span><span class="p">(</span><span class="n">first_convolution</span><span class="p">,</span> <span class="n">MAX_POOL_SIZE</span><span class="p">,</span> <span class="n">MAX_POOL_SIZE</span><span class="p">,</span> <span class="n">d_relu</span><span class="p">)</span>
    <span class="n">d_W1</span><span class="p">,</span> <span class="n">d_b1</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">backward_convolution</span><span class="p">(</span><span class="n">W1</span><span class="p">,</span> <span class="n">b1</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">d_maxpool</span><span class="p">)</span>

    <span class="n">grad_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;W1&#39;</span><span class="p">:</span> <span class="n">d_W1</span><span class="p">,</span>
                 <span class="s1">&#39;b1&#39;</span><span class="p">:</span> <span class="n">d_b1</span><span class="p">,</span>
                 <span class="s1">&#39;W2&#39;</span><span class="p">:</span> <span class="n">d_W2</span><span class="p">,</span>
                 <span class="s1">&#39;b2&#39;</span><span class="p">:</span> <span class="n">d_b2</span><span class="p">}</span>

    <span class="k">return</span> <span class="n">grad_dict</span>
    <span class="c1"># *** END CODE HERE ***</span>
</code></pre></div>
<h2 id="problem-2-off-policy-evaluation-for-mdps">Problem 2 Off-policy evaluation for MDPs<a class="headerlink" href="#problem-2-off-policy-evaluation-for-mdps" title="Permanent link">&para;</a></h2>
<h3 id="a-importance-sampling-estimator">a) Importance Sampling Estimator<a class="headerlink" href="#a-importance-sampling-estimator" title="Permanent link">&para;</a></h3>
<p>Wanted equation is equal to</p>
<div class="arithmatex">\[
\begin{aligned}
\mathbb{E}_{\substack{s \sim p(s) \\ a \sim \pi_{0}(s, a)}} \frac{\pi_{1}(s, a)}{\hat{\pi}_{0}(s, a)} R(s, a) &amp;= \sum_{(s,a)} p(s,a) \frac{\pi_{1}(s, a)}{\hat{\pi}_{0}(s, a)} R(s, a)\\
&amp;= \sum_{(s,a)} p(s) \pi_{0}(s, a) \frac{\pi_{1}(s, a)}{\hat{\pi}_{0}(s, a)} R(s, a)
\end{aligned}
\]</div>
<p>Besides, we have <span class="arithmatex">\(\hat \pi _0 = \pi _0\)</span>, thus
$$
\begin{aligned}
\mathbb{E}<em 0="0">{\substack{s \sim p(s) \ a \sim \pi</em>}(s, a)}} \frac{\pi_{1}(s, a)}{\hat{\pi<em _s_a_="(s,a)">{0}(s, a)} R(s, a) &amp;= \sum</em>} p(s) \pi_{0}(s, a) \frac{\pi_{1}(s, a)}{{\pi<em _s_a_="(s,a)">{0}(s, a)} R(s, a) \
&amp;= \sum</em> R(s, a)\
&amp;= \mathbb{E}} p(s)  {\pi_{1}(s, a)<em 1="1">{\substack{s \sim p(s) \ a \sim \pi</em> R(s, a)
\end{aligned}
$$}(s, a)}</p>
<h3 id="b-weighted-importance-sampling">b) Weighted Importance Sampling<a class="headerlink" href="#b-weighted-importance-sampling" title="Permanent link">&para;</a></h3>
<p>When <span class="arithmatex">\(\hat \pi _0 = \pi _0\)</span>, we have
$$
\begin{aligned}
\frac{\mathbb{E}<em 0="0">{\substack{s \sim p(s) \ a \sim \pi</em>}(s, a)}} \frac{\pi_{1}(s, a)}{\hat{\pi<em _92_="\" _pi__0="\pi_{0" _sim="\sim" _substack_s="\substack{s" a="a" p_s_="p(s)">{0}(s, a)} R(s, a)}{\mathbb{E}</em>}(s, a)}} \frac{\pi_{1}(s, a)}{\hat{\pi<em _s_a_="(s,a)">{0}(s, a)}} &amp;= \frac{\sum</em> \
&amp;= \frac{\sum_{(s,a)} p(s)  {\pi_{1}(s, a)} R(s, a)}{\sum_{(s,a)} p(s,a)} \
&amp;= \sum_{(s,a)} p(s)  {\pi_{1}(s, a)} R(s, a) \
&amp;= \mathbb{E}} p(s)  {\pi_{1}(s, a)} R(s, a)}{\sum_{(s,a)} p(s)  {\pi_{1}(s, a)}<em 1="1">{\substack{s \sim p(s) \ a \sim \pi</em> R(s, a)
\end{aligned}
$$}(s, a)}</p>
<h3 id="c-weighted-importance-sampling-estimator-is-biased">c) Weighted importance sampling estimator is biased<a class="headerlink" href="#c-weighted-importance-sampling-estimator-is-biased" title="Permanent link">&para;</a></h3>
<p>In our assumption, we have
$$
\begin{aligned}
\frac{\mathbb{E}<em 0="0">{\substack{s \sim p(s) \ a \sim \pi</em>}(s, a)}} \frac{\pi_{1}(s, a)}{\hat{\pi<em _92_="\" _pi__0="\pi_{0" _sim="\sim" _substack_s="\substack{s" a="a" p_s_="p(s)">{0}(s, a)} R(s, a)}{\mathbb{E}</em>}(s, a)}} \frac{\pi_{1}(s, a)}{\hat{\pi<em _92_="\" _pi__1="\pi_{1" _sim="\sim" _substack_s="\substack{s" a="a" p_s_="p(s)">{0}(s, a)}} &amp;= \mathbb{E}</em> R(s, a)
\end{aligned}
$$
If there's only one data element in observational dataset, we have
$$
\begin{aligned}
\frac{\mathbb{E}}(s, a)}<em 0="0">{\substack{s \sim p(s) \ a \sim \pi</em>}(s, a)}} \frac{\pi_{1}(s, a)}{\hat{\pi<em _92_="\" _pi__0="\pi_{0" _sim="\sim" _substack_s="\substack{s" a="a" p_s_="p(s)">{0}(s, a)} R(s, a)}{\mathbb{E}</em>}(s, a)}} \frac{\pi_{1}(s, a)}{\hat{\pi<em _s_a_="(s,a)">{0}(s, a)}} &amp;= \frac{\sum</em>} p(s,a) \frac{\pi_{1}(s, a)}{\hat{\pi<em _s_a_="(s,a)">{0}(s, a)} R(s, a)}{\sum</em>} p(s,a) \frac{\pi_{1}(s, a)}{\hat{\pi<em 1="1">{0}(s, a)}} \
&amp;= \frac{p(s,a) \frac{\pi</em>}(s, a)}{\hat{\pi<em 1="1">{0}(s, a)} R(s, a)}{p(s,a) \frac{\pi</em>}(s, a)}{\hat{\pi<em _92_="\" _pi__0="\pi_{0" _sim="\sim" _substack_s="\substack{s" a="a" p_s_="p(s)">{0}(s, a)}} \
&amp;= R(s, a) \
&amp;= \mathbb{E}</em> R(s, a)
\end{aligned}
$$
If }(s, a)}<span class="arithmatex">\(\pi_1 \ne \pi_0\)</span>,
$$
\mathbb{E}<em 0="0">{\substack{s \sim p(s) \ a \sim \pi</em>}(s, a)}} R(s, a) \ne \mathbb{E<em 1="1">{\substack{s \sim p(s) \ a \sim \pi</em> R(s, a)
$$}(s, a)}</p>
<h3 id="d-doubly-robust">d) Doubly Robust<a class="headerlink" href="#d-doubly-robust" title="Permanent link">&para;</a></h3>
<p><strong>1)</strong> When <span class="arithmatex">\(\hat \pi _0 = \pi _0\)</span>, we have <span class="arithmatex">\(\hat R (s,a) = R (s,a)\)</span>, thus
$$
\begin{aligned}
\mathbb{E}<em 0="0">{\substack{s \sim p(s) \ a \sim \pi</em>}(s, a)}}\left(\left(\mathbb{E<em 1="1">{a \sim \pi</em>}(s, a)} \hat{R}(s, a)\right)+\frac{\pi_{1}(s, a)}{\hat{\pi<em _92_="\" _pi__0="\pi_{0" _sim="\sim" _substack_s="\substack{s" a="a" p_s_="p(s)">{0}(s, a)}(R(s, a)-\hat{R}(s, a))\right) 
&amp;= {\mathbb{E}</em>}(s, a)}} \left(\mathbb{E<em 1="1">{a \sim \pi</em>}(s, a)} \hat R(s, a)\right)}+{\mathbb{E<em 0="0">{\substack{s \sim p(s) \ a \sim \pi</em>}(s, a)}} \frac{\pi_{1}(s, a)}{{\pi<em _92_="\" _pi__0="\pi_{0" _sim="\sim" _substack_s="\substack{s" a="a" p_s_="p(s)">{0}(s, a)}R(s, a)}\
&amp;-{\mathbb{E}</em>}(s, a)}} \frac{\pi_{1}(s, a)}{{\pi<em _92_="\" _pi__1="\pi_{1" _sim="\sim" _substack_s="\substack{s" a="a" p_s_="p(s)">{0}(s, a)}\hat R(s, a)} \
&amp;= {\mathbb{E}</em>}(s, a)}} \hat R(s, a)} + {\mathbb{E<em 0="0">{\substack{s \sim p(s) \ a \sim \pi</em>}(s, a)}} \frac{\pi_{1}(s, a)}{{\pi<em _92_="\" _pi__1="\pi_{1" _sim="\sim" _substack_s="\substack{s" a="a" p_s_="p(s)">{0}(s, a)}R(s, a)} \
&amp;- {\mathbb{E}</em> \
&amp;= {\mathbb{E}}(s, a)}} \hat R(s, a)<em 0="0">{\substack{s \sim p(s) \ a \sim \pi</em>}(s, a)}} \frac{\pi_{1}(s, a)}{{\pi<em _92_="\" _pi__1="\pi_{1" _sim="\sim" _substack_s="\substack{s" a="a" p_s_="p(s)">{0}(s, a)}R(s, a)}\
&amp;= \mathbb{E}</em> R(s, a)
\end{aligned}
$$
}(s, a)}<strong>2)</strong> When <span class="arithmatex">\(\hat R (s,a) = R (s,a)\)</span>, thus
$$
\begin{aligned}
\mathbb{E}<em 0="0">{\substack{s \sim p(s) \ a \sim \pi</em>}(s, a)}}\left(\left(\mathbb{E<em 1="1">{a \sim \pi</em>}(s, a)} \hat{R}(s, a)\right)+\frac{\pi_{1}(s, a)}{\hat{\pi<em _92_="\" _pi__0="\pi_{0" _sim="\sim" _substack_s="\substack{s" a="a" p_s_="p(s)">{0}(s, a)}(R(s, a)-\hat{R}(s, a))\right) 
&amp;= \mathbb{E}</em>}(s, a)}} \left(\mathbb{E<em 1="1">{a \sim \pi</em> R(s, a)\right)\
&amp;= \frac{\mathbb{E}}(s, a)<em 0="0">{\substack{s \sim p(s) \ a \sim \pi</em>}(s, a)}} \frac{\pi_{1}(s, a)}{\hat{\pi<em _92_="\" _pi__0="\pi_{0" _sim="\sim" _substack_s="\substack{s" a="a" p_s_="p(s)">{0}(s, a)} R(s, a)}{\mathbb{E}</em>}(s, a)}} \frac{\pi_{1}(s, a)}{\hat{\pi<em _92_="\" _pi__1="\pi_{1" _sim="\sim" _substack_s="\substack{s" a="a" p_s_="p(s)">{0}(s, a)}}\
&amp;= \mathbb{E}</em> R(s, a)\
\end{aligned}
$$}(s, a)}</p>
<h3 id="e-choice-between-estimator">e) Choice between estimator<a class="headerlink" href="#e-choice-between-estimator" title="Permanent link">&para;</a></h3>
<p><strong>i.</strong>  Use importance sampling estimator. Because the interaction <span class="arithmatex">\(R(s,a)\)</span> is very complicated, and importance sampling estimator only need to model drug assignment policy <span class="arithmatex">\(\pi _0\)</span> using <span class="arithmatex">\(R(s,a)\)</span> as observation data. </p>
<p><strong>ii.</strong> Use regression estimator. Because interaction <span class="arithmatex">\(R(s,a)\)</span> is very simple. And for regression estimator, if <span class="arithmatex">\(\hat R(s,a) = R(s,a)\)</span>, the estimator is trivially correct.</p>
<h2 id="problem-3-pca">Problem 3 ==PCA==<a class="headerlink" href="#problem-3-pca" title="Permanent link">&para;</a></h2>
<div class="arithmatex">\[
\begin{aligned}
f_{u}(x) &amp; = \arg \min _{v \in \mathcal{V}}\|x-v\|^{2} =\frac{uu^T x}{u^T u} = uu^T x \\ \\
\arg \min _{u: u^{T} u=1} \sum_{i=1}^{m}\left\|x^{(i)}-f_{u}\left(x^{(i)}\right)\right\|_{2}^{2} &amp;=\arg \min _{u: u^{T} u=1} \sum_{i=1}^{m}\left\|x^{(i)}-u u^{T} x^{(i)}\right\|_{2}^{2} \\
&amp;=\arg \min _{u: u^{T} u=1} \sum_{i=1}^{m}\left(x^{(i)}-u u^{T} x^{(i)}\right)^{T}\left(x^{(i)}-u u^{T} x^{(i)}\right) \\
&amp;=\arg \min _{u: u^{T} u=1} \sum_{i=1}^{m} x^{(i)^{T}} x^{(i)}-x^{(i)^{T}} u u^{T} x^{(i)} \\
&amp;=\arg \max _{u: u^{T} u=1} \sum_{i=1}^{m} x^{(i)^{T}} u u^{T} x^{(i)} \\
&amp;=\arg \max _{u: u^{T} u=1} \sum_{i=1}^{m} u^{T} x^{(i)} x^{(i)^{T}} u \\
&amp;=\arg \max _{u: u^{T} u=1} u^{T}\left(\sum_{i=1}^{m} x^{(i)} x^{(i)^{T}}\right) u
\end{aligned}
\]</div>
<h2 id="problem-4-independent-components-analysis">Problem 4  Independent Components Analysis<a class="headerlink" href="#problem-4-independent-components-analysis" title="Permanent link">&para;</a></h2>
<h3 id="a-gaussian-source">a) Gaussian source<a class="headerlink" href="#a-gaussian-source" title="Permanent link">&para;</a></h3>
<p>When <span class="arithmatex">\(g\prime\)</span> is standard normal distribution, we have
$$
\begin{aligned}
\ell (W) &amp;= \sum_{i=1}^n \left(\log |W| + \sum_{j=1}^d \log {\frac{1}{\sqrt{2\pi}} \exp \frac{- \left(w_j^T x^{(i)} \right)^2}{2}} \right) \
\nabla_W \ell &amp;= n{(W^{-1})^T} + \left(\sum_{i=1}^n \nabla_W\sum_{j=1}^d \frac{- \left(w_j^T x^{(i)} \right)^2}{2} \right) \
&amp;=  n{(W^{-1})^T} + \sum_{i=1}^n W x^{(i)} x^{(i)T} \
&amp;=  n{(W^{-1})^T} - W X X^T \
&amp;= 0 \ \</p>
<p>n{(W^{-1})^T} &amp;= W X X^T \
W^T W &amp;= n X^T X
\end{aligned}
$$
Let <span class="arithmatex">\(u\)</span> be an arbitrary orthogonal matrix, and let <span class="arithmatex">\(W ^\prime = RW\)</span>. Then
$$
{W ^\prime}^T W ^\prime =  W^T R^T R W = W^T W
$$
i.e. if <span class="arithmatex">\(W\)</span> is a solution, any <span class="arithmatex">\(W^ \prime\)</span> is also a solution.</p>
<h3 id="b-laplace-source">b) ==Laplace source==<a class="headerlink" href="#b-laplace-source" title="Permanent link">&para;</a></h3>
<p>Derive update for <span class="arithmatex">\(W\)</span> when <span class="arithmatex">\(s_i \sim \mathcal L (0,1)\)</span>
$$
\begin{aligned}
\nabla_{W} \ell(W) &amp;=\nabla_{W}\left(\log |W|+\sum_{j=1}^{d} \log \frac{1}{2} \exp \left(-\left|w_{j}^{T} x^{(i)}\right|\right)\right) \
&amp;=\left(W^{-1}\right)^{T}-\nabla_{W} \sum_{j=1}^{d}\left|w_{j}^{T} x^{(i)}\right| \
&amp;=\left(W^{T}\right)^{-1}-\operatorname{sign}\left(W x^{(i)}\right) x^{(i)^{T}} \
W &amp;:=W+\alpha\left(\left(W^{T}\right)^{-1}-\operatorname{sign}\left(W x^{(i)}\right) x^{(i)^{T}}\right)
\end{aligned}
$$</p>
<p><em><u>Wrong Solution</u></em>
$$
\begin{aligned}
\ell (W) &amp;= \sum_{i=1}^n \left(\log |W| + \sum_{j=1}^d \log {\frac{1}{2} \exp \left({- w_j^T x^{(i)}} \right) }\right) \
\nabla_W \ell &amp;= n{(W^{-1})^T} + \left(\sum_{i=1}^n \nabla_W\sum_{j=1}^d {- w_j^T x^{(i)}} \right) \
&amp;= n{(W^{-1})^T} - nX \
\
W &amp;:= W + \alpha n \left({(W^{-1})^T} - X \right)
\end{aligned}
$$</p>
<h3 id="c-cocktail-party-problem">c) Cocktail Party Problem<a class="headerlink" href="#c-cocktail-party-problem" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">update_W</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Perform a gradient ascent update on W using data element x and the provided learning rate.</span>

<span class="sd">    This function should return the updated W.</span>

<span class="sd">    Use the laplace distribiution in this problem.</span>

<span class="sd">    Args:</span>
<span class="sd">        W: The W matrix for ICA</span>
<span class="sd">        x: A single data element</span>
<span class="sd">        learning_rate: The learning rate to use</span>

<span class="sd">    Returns:</span>
<span class="sd">        The updated W</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># *** START CODE HERE ***</span>
    <span class="n">updated_W</span> <span class="o">=</span> <span class="n">W</span> <span class="o">+</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">W</span><span class="o">.</span><span class="n">T</span><span class="p">)</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">outer</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sign</span><span class="p">(</span><span class="n">W</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">)),</span> <span class="n">x</span><span class="o">.</span><span class="n">T</span><span class="p">))</span>
    <span class="c1"># *** END CODE HERE ***</span>

    <span class="k">return</span> <span class="n">updated_W</span>


<span class="k">def</span> <span class="nf">unmix</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">W</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Unmix an X matrix according to W using ICA.</span>

<span class="sd">    Args:</span>
<span class="sd">        X: The data matrix</span>
<span class="sd">        W: The W for ICA</span>

<span class="sd">    Returns:</span>
<span class="sd">        A numpy array S containing the split data</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">S</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>


    <span class="c1"># *** START CODE HERE ***</span>
    <span class="n">S</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="c1"># *** END CODE HERE ***</span>

    <span class="k">return</span> <span class="n">S</span>
</code></pre></div>
<h2 id="problem-5-markov-decision-processes">Problem 5 Markov decision processes<a class="headerlink" href="#problem-5-markov-decision-processes" title="Permanent link">&para;</a></h2>
<h3 id="a_5">a)<a class="headerlink" href="#a_5" title="Permanent link">&para;</a></h3>
<div class="arithmatex">\[
\begin{aligned}
\left\|B\left(V_{1}\right)-B\left(V_{2}\right)\right\|_{\infty} &amp;=\gamma\left\|\max _{a \in A} \sum_{s^{\prime} \in S} P_{s a}\left(s^{\prime}\right)\left[V_{1}\left(s^{\prime}\right)-V_{2}\left(s^{\prime}\right)\right]\right\|_{\infty} \\
&amp;=\gamma \max _{s^{\prime} \in S}\left|\max _{a \in A} \sum_{s^{\prime} \in S} P_{s a}\left(s^{\prime}\right)\left[V_{1}\left(s^{\prime}\right)-V_{2}\left(s^{\prime}\right)\right]\right| \\
&amp; \leq \gamma\left\|V_{1}-V_{2}\right\|_{\infty}
\end{aligned}
\]</div>
<p>The inequality holds because for any <span class="arithmatex">\(\alpha,x \in \mathbb R ^n\)</span>, if <span class="arithmatex">\(\sum_i \alpha_i = 1\)</span> and <span class="arithmatex">\(\alpha_i \ge 0\)</span>, then <span class="arithmatex">\(\sum_i \alpha_i x_i \le \max_i x_i\)</span>   </p>
<h3 id="b_4">b)<a class="headerlink" href="#b_4" title="Permanent link">&para;</a></h3>
<p>Assume there are two fixed points <span class="arithmatex">\(V_1,V_2\)</span>, i.e. <span class="arithmatex">\(B\left(V_{1}\right)=V_{1}, B\left(V_{2}\right)=V_{2}\)</span>
$$
\begin{array}{c}
\left|V_{1}-V_{2}\right|<em 1="1">{\infty} = \left|B\left(V</em>\right)\right|}\right)-B\left(V_{2<em 1="1">{\infty} \leq \gamma\left|V</em>\right|}-V_{2<em 1="1">{\infty} \
\left|V</em>\right|}-V_{2<em 1="1">{\infty} = 0 \
V</em>
\end{array}
$$
So } = V_{2<span class="arithmatex">\(B\)</span> have at most one fixed point.</p>
<h2 id="problem-6-reinforcement-learning-the-inverted-pendulum">Problem 6 Reinforcement Learning: The inverted pendulum<a class="headerlink" href="#problem-6-reinforcement-learning-the-inverted-pendulum" title="Permanent link">&para;</a></h2>
<ul>
<li><code>simulate()</code> function for simulating the pole dynamics</li>
<li><code>get_state()</code> for discretizing</li>
<li><code>show_cart()</code> for display</li>
<li><code>NUM_STATES</code> </li>
<li><code>time_steps_to_failure</code>  records the time for which the pole was balanced before each failure is in memory</li>
<li><code>num_failures</code>  stores the number of failures (pole drops / cart out of bounds) till now.</li>
</ul>
<p>:warning: </p>
<ul>
<li>Update the transition counts and rewards observed after each simulation cycle</li>
</ul>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">choose_action</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">mdp_data</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Choose the next action (0 or 1) that is optimal according to your current</span>
<span class="sd">    mdp_data. When there is no optimal action, return a random action.</span>

<span class="sd">    Args:</span>
<span class="sd">        state: The current state in the MDP</span>
<span class="sd">        mdp_data: The parameters for your MDP. See initialize_mdp_data.</span>

<span class="sd">    Returns:</span>
<span class="sd">        0 or 1 that is optimal according to your current MDP</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># *** START CODE HERE ***</span>
    <span class="n">expected_value</span> <span class="o">=</span> <span class="n">mdp_data</span><span class="p">[</span><span class="s1">&#39;value&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">mdp_data</span><span class="p">[</span><span class="s1">&#39;transition_probs&#39;</span><span class="p">][</span><span class="n">state</span><span class="p">])</span>
    <span class="k">if</span> <span class="n">expected_value</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">expected_value</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">expected_value</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">expected_value</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
        <span class="k">return</span> <span class="mi">0</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="mi">1</span>

    <span class="c1"># Plan B</span>

    <span class="c1"># else:</span>
    <span class="c1">#     return np.argmax(expect_value)</span>

    <span class="c1"># *** END CODE HERE ***</span>

<span class="k">def</span> <span class="nf">update_mdp_transition_counts_reward_counts</span><span class="p">(</span><span class="n">mdp_data</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">new_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Update the transition count and reward count information in your mdp_data.</span>
<span class="sd">    Do not change the other MDP parameters (those get changed later).</span>

<span class="sd">    Record the number of times `state, action, new_state` occurs.</span>
<span class="sd">    Record the rewards for every `new_state`</span>
<span class="sd">    (since rewards are -1 or 0, you just need to record number of times reward -1 is seen in &#39;reward_counts&#39; index new_state,0)</span>
<span class="sd">    Record the number of time `new_state` was reached (in &#39;reward_counts&#39; index new_state,1)</span>

<span class="sd">    Args:</span>
<span class="sd">        mdp_data: The parameters of your MDP. See initialize_mdp_data.</span>
<span class="sd">        state: The state that was observed at the start.</span>
<span class="sd">        action: The action you performed.</span>
<span class="sd">        new_state: The state after your action.</span>
<span class="sd">        reward: The reward after your action (i.e. reward corresponding to new_state).</span>

<span class="sd">    Returns:</span>
<span class="sd">        Nothing</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># *** START CODE HERE ***</span>
    <span class="n">mdp_data</span><span class="p">[</span><span class="s1">&#39;transition_counts&#39;</span><span class="p">][</span><span class="n">state</span><span class="p">,</span> <span class="n">new_state</span><span class="p">,</span> <span class="n">action</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="n">mdp_data</span><span class="p">[</span><span class="s1">&#39;reward_counts&#39;</span><span class="p">][</span><span class="n">new_state</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="k">if</span> <span class="n">reward</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">mdp_data</span><span class="p">[</span><span class="s1">&#39;reward_counts&#39;</span><span class="p">][</span><span class="n">new_state</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="c1"># *** END CODE HERE ***</span>

    <span class="c1"># This function does not return anything</span>
    <span class="k">return</span>


<span class="k">def</span> <span class="nf">update_mdp_transition_probs_reward</span><span class="p">(</span><span class="n">mdp_data</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Update the estimated transition probabilities and reward values in your MDP.</span>

<span class="sd">    Make sure you account for the case when a state-action pair has never</span>
<span class="sd">    been tried before, or the state has never been visited before. In that</span>
<span class="sd">    case, you must not change that component (and thus keep it at the</span>
<span class="sd">    initialized uniform distribution).</span>

<span class="sd">    Args:</span>
<span class="sd">        mdp_data: The data for your MDP. See initialize_mdp_data.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Nothing</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># *** START CODE HERE ***</span>
    <span class="n">transition_counts</span> <span class="o">=</span> <span class="n">mdp_data</span><span class="p">[</span><span class="s1">&#39;transition_counts&#39;</span><span class="p">]</span>
    <span class="n">num_counts</span> <span class="o">=</span> <span class="n">transition_counts</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">mdp_data</span><span class="p">[</span><span class="s1">&#39;num_states&#39;</span><span class="p">]):</span>
        <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">num_counts</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">a</span><span class="p">]</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">mdp_data</span><span class="p">[</span><span class="s1">&#39;transition_probs&#39;</span><span class="p">][</span><span class="n">i</span><span class="p">,</span> <span class="p">:,</span> <span class="n">a</span><span class="p">]</span> <span class="o">=</span> <span class="n">transition_counts</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:,</span> <span class="n">a</span><span class="p">]</span> <span class="o">/</span> <span class="n">num_counts</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">a</span><span class="p">]</span>

    <span class="k">for</span> <span class="n">state</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">mdp_data</span><span class="p">[</span><span class="s1">&#39;num_states&#39;</span><span class="p">]):</span>
        <span class="k">if</span> <span class="n">mdp_data</span><span class="p">[</span><span class="s1">&#39;reward_counts&#39;</span><span class="p">][</span><span class="n">state</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">mdp_data</span><span class="p">[</span><span class="s1">&#39;reward&#39;</span><span class="p">][</span><span class="n">state</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span> <span class="n">mdp_data</span><span class="p">[</span><span class="s1">&#39;reward_counts&#39;</span><span class="p">][</span><span class="n">state</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="n">mdp_data</span><span class="p">[</span><span class="s1">&#39;reward_counts&#39;</span><span class="p">][</span><span class="n">state</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span>
    <span class="c1"># *** END CODE HERE ***</span>

    <span class="c1"># This function does not return anything</span>
    <span class="k">return</span>

<span class="k">def</span> <span class="nf">update_mdp_value</span><span class="p">(</span><span class="n">mdp_data</span><span class="p">,</span> <span class="n">tolerance</span><span class="p">,</span> <span class="n">gamma</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Update the estimated values in your MDP.</span>

<span class="sd">    Perform value iteration using the new estimated model for the MDP.</span>
<span class="sd">    The convergence criterion should be based on `TOLERANCE` as described</span>
<span class="sd">    at the top of the file.</span>

<span class="sd">    Return true if it converges within one iteration.</span>

<span class="sd">    Args:</span>
<span class="sd">        mdp_data: The data for your MDP. See initialize_mdp_data.</span>
<span class="sd">        tolerance: The tolerance to use for the convergence criterion.</span>
<span class="sd">        gamma: Your discount factor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        True if the value iteration converged in one iteration</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># *** START CODE HERE ***</span>
    <span class="n">iters</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
        <span class="n">iters</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="n">value</span> <span class="o">=</span> <span class="n">mdp_data</span><span class="p">[</span><span class="s1">&#39;value&#39;</span><span class="p">]</span>
        <span class="n">new_value</span> <span class="o">=</span> <span class="n">mdp_data</span><span class="p">[</span><span class="s1">&#39;reward&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">value</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">mdp_data</span><span class="p">[</span><span class="s1">&#39;transition_probs&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">mdp_data</span><span class="p">[</span><span class="s1">&#39;value&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_value</span>

        <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">value</span> <span class="o">-</span> <span class="n">new_value</span><span class="p">))</span> <span class="o">&lt;</span> <span class="n">tolerance</span><span class="p">:</span>
            <span class="k">break</span>

    <span class="k">return</span> <span class="n">iters</span> <span class="o">==</span> <span class="mi">1</span>
    <span class="c1"># *** END CODE HERE ***</span>
</code></pre></div>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12Z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      quaintness
    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        <div class="md-social">
  
    
    
    
    
    <a href="https://github.com/quaintness" target="_blank" rel="noopener" title="GitHub | quaintness" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.5.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../../..", "features": ["navigation.tabs", "navigation.top", "navigation.indexes", "navigation.expand", "search.suggest", "search.highlight", "content.code.copy", "content.action.edit"], "search": "../../../assets/javascripts/workers/search.b8dbb3d2.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../../assets/javascripts/bundle.ad660dcc.min.js"></script>
      
        <script src="../../../mkdocs/javascripts/mathjax.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://unpkg.com/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>